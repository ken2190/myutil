rank_biased_overlap_mean: 0.14476181560428572 
rank_topk_kendall_mean: 0.4945454545454546

## example from cocount
{
	'fun': {
	  'white': 0.48153474517398526,
	  'index': 0.48106646888061716,
	  'testing': 0.4804071456474755,
	  'due': 0.47953235564571334,
	  'held': 0.4776860975855776,
	  'vehicle': 0.47626142925543735,
	  'specie': 0.475699764779269,
	  'form': 0.4756814352283156,
	  'goal': 0.47515676343217517,
	  'example': 0.4748241063726463
	}
}

## example from model 
{
	'fun':{'social': 0.2607463002204895,
 'testing': 0.22950029373168945,
 'goal': 0.19418969750404358,
 'vehicle': 0.16583408415317535,
 'held': 0.15749740600585938,
 'form': 0.1519775092601776,
 'including': 0.14990702271461487,
 'specie': 0.1435031294822693,
 'hotel': 0.14171545207500458,
 'earth': 0.12495720386505127}
}

###########
## from cocount
{
	'meaning': {'white': 0.48874858294413687,
  'vehicle': 0.488647609706989,
  'testing': 0.4878704238151364,
  'rule': 0.48069319165648067,
  'major': 0.47993589003781983,
  'specie': 0.47887000868050206,
  'area': 0.47826580581400463,
  'france': 0.4779735951588427,
  'goal': 0.47646189160049895,
  'form': 0.47599706674577447})
}

## from model 
{
	'meaning':{'example': 0.21299901604652405,
 'several': 0.19398052990436554,
 'area': 0.1809905767440796,
 'rule': 0.16790534555912018,
 'vehicle': 0.16451159119606018,
 'testing': 0.16310158371925354,
 'specie': 0.1185464859008789,
 'largest': 0.11156687885522842,
 'one': 0.1102587878704071,
 'form': 0.09800101071596146}
}


uri,name,type,n_variable,n_words,n_words_unique,n_characters,avg_char_per_word,n_loop,n_ifthen,arg_name,arg_type,arg_value,line,docs,list_functions,n_functions
main.py:get_obj_from_str,get_obj_from_str,function,9,13,13,177,13.62,0,1,"['string', 'reload']","[None, None]","[None, 'False']",14,[],"['string.rsplit', 'importlib.import_module', 'importlib.reload', 'getattr']",4
main.py:get_parser,get_parser,function,9,149,103,1356,9.1,0,2,['**parser_kwargs'],[None],[None],22,[],"['str2bool', 'isinstance', 'v.lower', 'argparse.ArgumentTypeError', 'argparse.ArgumentParser', 'parser.add_argument', 'default=list']",7
main.py:nondefault_trainer_args,nondefault_trainer_args,function,7,17,16,165,9.71,0,0,['opt'],[None],[None],106,[],"['argparse.ArgumentParser', 'Trainer.add_argparse_args', 'parser.parse_args', 'sorted', 'vars', 'getattr']",6
main.py:instantiate_from_config,instantiate_from_config,function,3,14,14,145,10.36,0,1,['config'],[None],[None],113,[],"['KeyError', 'get_obj_from_str', 'dict']",3
main.py:WrappedDataset,WrappedDataset,class,7,14,11,130,9.29,0,0,[],[],[],119,[],[],0
main.py:DataModuleFromConfig,DataModuleFromConfig,class,35,96,69,1260,13.12,3,5,[],[],[],131,[],[],0
main.py:SetupCallback,SetupCallback,class,30,71,63,1020,14.37,0,2,[],[],[],175,[],[],0
main.py:ImageLogger,ImageLogger,class,68,245,162,2783,11.36,5,7,[],[],[],215,[],[],0
main.py:WrappedDataset:__init__,WrappedDataset:__init__,method,2,2,2,17,8.5,0,0,"['self', 'dataset']","[None, None]","[None, None]",121,[],[],0
main.py:WrappedDataset:__len__,WrappedDataset:__len__,method,1,2,2,20,10.0,0,0,['self'],[None],[None],124,[],['len'],1
main.py:WrappedDataset:__getitem__,WrappedDataset:__getitem__,method,2,2,2,20,10.0,0,0,"['self', 'idx']","[None, None]","[None, None]",127,[],[],0
main.py:DataModuleFromConfig:__init__,DataModuleFromConfig:__init__,method,19,43,31,458,10.65,0,4,"['self', 'batch_size', 'train', 'validation', 'test', 'wrap', 'num_workers']","[None, None, None, None, None, None, None]","[None, None, 'None', 'None', 'None', 'False', 'None']",132,[],"['super', 'dict']",2
main.py:DataModuleFromConfig:prepare_data,DataModuleFromConfig:prepare_data,method,3,5,5,77,15.4,1,0,['self'],[None],[None],149,[],['instantiate_from_config'],1
main.py:DataModuleFromConfig:setup,DataModuleFromConfig:setup,method,6,16,13,184,11.5,2,1,"['self', 'stage']","[None, None]","[None, 'None']",153,[],"['dict', 'instantiate_from_config', 'WrappedDataset']",3
main.py:DataModuleFromConfig:_train_dataloader,DataModuleFromConfig:_train_dataloader,method,2,5,5,110,22.0,0,0,['self'],[None],[None],161,[],['DataLoader'],1
main.py:DataModuleFromConfig:_val_dataloader,DataModuleFromConfig:_val_dataloader,method,2,4,4,103,25.75,0,0,['self'],[None],[None],165,[],['DataLoader'],1
main.py:DataModuleFromConfig:_test_dataloader,DataModuleFromConfig:_test_dataloader,method,2,4,4,96,24.0,0,0,['self'],[None],[None],170,[],['DataLoader'],1
main.py:SetupCallback:__init__,SetupCallback:__init__,method,15,15,15,167,11.13,0,0,"['self', 'resume', 'now', 'logdir', 'ckptdir', 'cfgdir', 'config', 'lightning_config']","[None, None, None, None, None, None, None, None]","[None, None, None, None, None, None, None, None]",176,[],['super'],1
main.py:SetupCallback:on_pretrain_routine_start,SetupCallback:on_pretrain_routine_start,method,14,43,37,722,16.79,0,2,"['self', 'trainer', 'pl_module']","[None, None, None]","[None, None, None]",186,[],"['os.makedirs', 'print', 'OmegaConf.save', 'os.rename']",4
main.py:ImageLogger:__init__,ImageLogger:__init__,method,16,28,26,323,11.54,1,1,"['self', 'batch_frequency', 'max_images', 'clamp', 'increase_log_steps']","[None, None, None, None, None]","[None, None, None, 'True', 'True']",216,[],"['super', 'range']",2
main.py:ImageLogger:_wandb,ImageLogger:_wandb,method,9,15,15,179,11.93,1,0,"['self', 'pl_module', 'images', 'batch_idx', 'split']","[None, None, None, None, None]","[None, None, None, None, None]",230,[],"['ValueError', 'dict', 'wandb.Image']",3
main.py:ImageLogger:_testtube,ImageLogger:_testtube,method,6,19,18,196,10.32,1,0,"['self', 'pl_module', 'images', 'batch_idx', 'split']","[None, None, None, None, None]","[None, None, None, None, None]",239,[],[],0
main.py:ImageLogger:log_local,ImageLogger:log_local,method,12,36,32,455,12.64,1,0,"['self', 'save_dir', 'split', 'images', 'global_step', 'current_epoch', 'batch_idx']","[None, None, None, None, None, None, None]","[None, None, None, None, None, None, None]",250,[],"['grid.transpose', 'grid.numpy', 'os.makedirs', 'Image.fromarray']",4
main.py:ImageLogger:log_img,ImageLogger:log_img,method,25,67,58,802,11.97,1,5,"['self', 'pl_module', 'batch', 'batch_idx', 'split']","[None, None, None, None, None]","[None, None, None, None, '""train""']",269,[],"['hasattr', 'callable', 'type', 'pl_module.eval', 'torch.no_grad', 'pl_module.log_images', 'min', 'isinstance', 'torch.clamp', 'self.log_local', 'logger_log_images', 'pl_module.train']",12
main.py:ImageLogger:check_frequency,ImageLogger:check_frequency,method,3,18,16,135,7.5,0,1,"['self', 'batch_idx']","[None, None]","[None, None]",300,[],[],0
main.py:ImageLogger:on_train_batch_end,ImageLogger:on_train_batch_end,method,1,4,4,53,13.25,0,0,"['self', 'trainer', 'pl_module', 'outputs', 'batch', 'batch_idx', 'dataloader_idx']","[None, None, None, None, None, None, None]","[None, None, None, None, None, None, None]",309,[],['self.log_img'],1
main.py:ImageLogger:on_validation_batch_end,ImageLogger:on_validation_batch_end,method,1,4,4,51,12.75,0,0,"['self', 'trainer', 'pl_module', 'outputs', 'batch', 'batch_idx', 'dataloader_idx']","[None, None, None, None, None, None, None]","[None, None, None, None, None, None, None]",312,[],['self.log_img'],1
scripts/extract_depth.py:get_state,get_state,function,12,21,19,258,12.29,0,1,['gpu'],[None],[None],8,[],"['midas.cuda', 'midas.eval']",2
scripts/extract_depth.py:depth_to_rgba,depth_to_rgba,function,9,14,12,131,9.36,0,0,['x'],[None],[None],23,[],"['len', 'x.copy', 'y.reshape', 'np.ascontiguousarray']",4
scripts/extract_depth.py:rgba_to_depth,rgba_to_depth,function,9,17,15,146,8.59,0,0,['x'],[None],[None],32,[],"['len', 'x.copy', 'y.reshape', 'np.ascontiguousarray']",4
scripts/extract_depth.py:run,run,function,15,23,21,307,13.35,0,0,"['x', 'state']","[None, None]","[None, None]",41,[],"['torch.no_grad', 'model', 'prediction.unsqueeze', 'prediction.cpu']",4
scripts/extract_depth.py:get_filename,get_filename,function,8,9,8,90,10.0,0,0,"['relpath', 'level']","[None, None]","[None, '-2']",57,[],['relpath.split'],1
scripts/extract_depth.py:save_depth,save_depth,function,23,37,36,416,11.24,1,1,"['dataset', 'path', 'debug']","[None, None, None]","[None, None, 'False']",65,[],"['os.makedirs', 'len', 'get_state', 'trange', 'get_filename', 'run', 'depth_to_rgba', 'Image.fromarray']",8
scripts/extract_segmentation.py:rescale_bgr,rescale_bgr,function,3,7,5,46,6.57,0,0,['x'],[None],[None],20,[],['torch.flip'],1
scripts/extract_segmentation.py:run_model,run_model,function,8,14,13,193,13.79,0,0,"['img', 'model']","[None, None]","[None, None]",67,[],"['model.eval', 'torch.no_grad', 'model', 'torch.argmax', 'segmentation.detach']",5
scripts/extract_segmentation.py:get_input,get_input,function,7,15,13,122,8.13,0,1,"['batch', 'k']","[None, None]","[None, None]",75,[],"['len', 'x.permute', 'x.float']",3
scripts/extract_segmentation.py:save_segmentation,save_segmentation,function,7,15,12,232,15.47,1,0,"['segmentation', 'path']","[None, None]","[None, None]",83,[],"['os.makedirs', 'len', 'seg.permute', 'Image.fromarray', 'seg.save']",5
scripts/extract_segmentation.py:iterate_dataset,iterate_dataset,function,16,43,38,434,10.09,1,0,"['dataloader', 'destpath', 'model']","[None, None, None]","[None, None, None]",94,[],"['os.makedirs', 'tqdm', 'get_input', 'img.cuda', 'run_model', 'save_segmentation', 'print']",7
scripts/extract_segmentation.py:COCOStuffSegmenter,COCOStuffSegmenter,class,25,77,61,853,11.08,0,1,[],[],[],26,[],[],0
scripts/extract_segmentation.py:COCOStuffSegmenter:__init__,COCOStuffSegmenter:__init__,method,14,28,28,459,16.39,0,0,"['self', 'config']","[None, None]","[None, None]",27,[],"['super', 'model.load_state_dict', 'torch.stack']",3
scripts/extract_segmentation.py:COCOStuffSegmenter:forward,COCOStuffSegmenter:forward,method,6,14,11,124,8.86,0,1,"['self', 'x', 'upsample']","[None, None, None]","[None, None, 'None']",42,[],"['self._pre_process', 'self.model']",2
scripts/extract_segmentation.py:COCOStuffSegmenter:_pre_process,COCOStuffSegmenter:_pre_process,method,3,4,3,33,8.25,0,0,"['self', 'x']","[None, None]","[None, None]",49,[],['self.image_transform'],1
scripts/extract_segmentation.py:COCOStuffSegmenter:mean,COCOStuffSegmenter:mean,method,1,4,4,31,7.75,0,0,['self'],[None],[None],54,[],[],0
scripts/extract_segmentation.py:COCOStuffSegmenter:std,COCOStuffSegmenter:std,method,1,4,4,19,4.75,0,0,['self'],[None],[None],59,[],[],0
scripts/extract_segmentation.py:COCOStuffSegmenter:input_size,COCOStuffSegmenter:input_size,method,1,4,4,17,4.25,0,0,['self'],[None],[None],63,[],[],0
scripts/make_samples.py:save_image,save_image,function,5,7,7,141,20.14,0,0,"['x', 'path']","[None, None]","[None, None]",12,[],['Image.fromarray'],1
scripts/make_samples.py:run_conditional,run_conditional,function,88,214,152,2433,11.37,6,7,"['model', 'dsets', 'outdir', 'top_k', 'temperature', 'batch_size']","[None, None, None, None, None, None]","[None, None, None, None, None, '1']",20,[],"['len', 'sorted', 'next', 'print', 'trange', 'list', 'default_collate', 'model.get_input', 'range', 'save_image', 'model.encode_to_z', 'model.encode_to_c', 'torch.argmax', 'c.squeeze', 'idx.reshape', 'cidx.reshape', 'patch.reshape', 'cpatch.reshape', 'torch.cat', 'model.transformer', 'logits.reshape', 'model.top_k_logits', 'torch.multinomial', 'torch.topk', 'model.decode_to_img']",25
scripts/make_samples.py:get_parser,get_parser,function,6,125,95,1045,8.36,0,0,[],[],[],124,[],"['argparse.ArgumentParser', 'parser.add_argument', 'default=list']",3
scripts/make_samples.py:load_model_from_config,load_model_from_config,function,17,93,56,951,10.23,0,7,"['config', 'sd', 'gpu', 'eval_mode']","[None, None, None, None]","[None, None, 'True', 'True']",179,[],"['print', 'instantiate_from_config', 'model.load_state_dict', 'model.cuda', 'model.eval']",5
scripts/make_samples.py:get_data,get_data,function,5,6,5,85,14.17,0,0,['config'],[None],[None],209,[],"['instantiate_from_config', 'data.prepare_data', 'data.setup']",3
scripts/make_samples.py:load_model_and_dset,load_model_and_dset,function,12,28,25,307,10.96,0,1,"['config', 'ckpt', 'gpu', 'eval_mode']","[None, None, None, None]","[None, None, None, None]",217,[],"['get_data', 'torch.load', 'load_model_from_config']",3
scripts/sample_conditional.py:bchw_to_st,bchw_to_st,function,2,2,2,58,29.0,0,0,['x'],[None],[None],16,[],['rescale'],1
scripts/sample_conditional.py:save_img,save_img,function,2,3,3,75,25.0,0,0,"['xstart', 'fname']","[None, None]","[None, None]",19,[],['Image.fromarray'],1
scripts/sample_conditional.py:get_interactive_image,get_interactive_image,function,12,35,24,333,9.51,0,3,['resize'],[None],['False'],25,[],"['st.file_uploader', 'Image.open', 'image.convert', 'np.array', 'print', 'Image.fromarray', 'img.resize']",7
scripts/sample_conditional.py:single_image_to_torch,single_image_to_torch,function,7,27,23,161,5.96,0,1,"['x', 'permute']","[None, None]","[None, 'True']",40,[],"['np.array', 'torch.FloatTensor', 'x.permute']",3
scripts/sample_conditional.py:pad_to_M,pad_to_M,function,6,9,8,136,15.11,0,0,"['x', 'M']","[None, None]","[None, None]",49,[],['math.ceil'],1
scripts/sample_conditional.py:run_conditional,run_conditional,function,123,320,232,4016,12.55,2,14,"['model', 'dsets']","[None, None]","[None, None]",56,[],"['len', 'sorted', 'next', 'max_value=len', 'list', 'default_collate', 'model.get_input', 'model.encode_to_z', 'model.encode_to_c', 'st.write', 'st.image', 'torch.argmax', 'c.squeeze', 'idx.reshape', 'st.info', 'c_indices.clone', 'cidx.reshape', 'model.decode_to_img', 'st.number_input', 'st.checkbox', 'st.text', 'imageio.get_writer', 'st.empty', 'st.button', 'time.time', 'range', 'elapsed_t.text', 'info.text', 'patch.reshape', 'cpatch.reshape', 'torch.cat', 'model.transformer', 'logits.reshape', 'model.top_k_logits', 'torch.multinomial', 'torch.topk', 'bchw_to_st', 'output.image', 'writer.append_data', 'writer.close', 'st.video']",41
scripts/sample_conditional.py:get_parser,get_parser,function,6,98,77,762,7.78,0,0,[],[],[],205,[],"['argparse.ArgumentParser', 'parser.add_argument', 'default=list']",3
scripts/sample_conditional.py:load_model_from_config,load_model_from_config,function,18,93,56,975,10.48,0,7,"['config', 'sd', 'gpu', 'eval_mode']","[None, None, None, None]","[None, None, 'True', 'True']",242,[],"['st.warning', 'instantiate_from_config', 'model.load_state_dict', 'st.info', 'model.cuda', 'model.eval']",6
scripts/sample_conditional.py:get_data,get_data,function,5,6,5,85,14.17,0,0,['config'],[None],[None],272,[],"['instantiate_from_config', 'data.prepare_data', 'data.setup']",3
scripts/sample_conditional.py:load_model_and_dset,load_model_and_dset,function,12,28,25,307,10.96,0,1,"['config', 'ckpt', 'gpu', 'eval_mode']","[None, None, None, None]","[None, None, None, None]",281,[],"['get_data', 'torch.load', 'load_model_from_config']",3
scripts/sample_fast.py:chw_to_pillow,chw_to_pillow,function,2,2,2,108,54.0,0,0,['x'],[None],[None],17,[],['Image.fromarray'],1
scripts/sample_fast.py:sample_classconditional,sample_classconditional,function,20,65,62,686,10.55,0,1,"['model', 'batch_size', 'class_label', 'steps', 'temperature', 'top_k', 'callback', 'dim_z', 'h', 'w', 'verbose_time', 'top_p']","[None, None, None, None, None, None, None, None, None, None, None, None]","[None, None, None, '256', 'None', 'None', 'None', '256', '16', '16', 'False', 'None']",22,[],"['dict', 'type', 'repeat', 'time.time', 'sample_with_past', 'print', 'model.decode_to_img']",7
scripts/sample_fast.py:sample_unconditional,sample_unconditional,function,19,52,51,563,10.83,0,1,"['model', 'batch_size', 'steps', 'temperature', 'top_k', 'top_p', 'callback', 'dim_z', 'h', 'w', 'verbose_time']","[None, None, None, None, None, None, None, None, None, None, None]","[None, None, '256', 'None', 'None', 'None', 'None', '256', '16', '16', 'False']",43,[],"['dict', 'repeat', 'time.time', 'sample_with_past', 'print', 'model.decode_to_img']",6
scripts/sample_fast.py:run,run,function,16,97,67,927,9.56,4,3,"['logdir', 'model', 'batch_size', 'temperature', 'top_k', 'unconditional', 'num_samples', 'given_classes', 'top_p']","[None, None, None, None, None, None, None, None, None]","[None, None, None, None, None, 'True', '50000', 'None', 'None']",62,[],"['range', 'print', 'tqdm', 'sample_classconditional', 'save_from_logs', 'sample_unconditional']",6
scripts/sample_fast.py:save_from_logs,save_from_logs,function,15,32,28,350,10.94,1,2,"['logs', 'logdir', 'base_count', 'key', 'cond_key']","[None, None, None, None, None]","[None, None, None, '""samples""', 'None']",84,[],"['enumerate', 'chw_to_pillow', 'x.save', 'type', 'condlabel.item', 'os.makedirs', 'str']",7
scripts/sample_fast.py:get_parser,get_parser,function,9,165,110,1390,8.42,0,2,[],[],[],98,[],"['str2bool', 'isinstance', 'v.lower', 'argparse.ArgumentTypeError', 'argparse.ArgumentParser', 'parser.add_argument', 'default=list']",7
scripts/sample_fast.py:load_model_from_config,load_model_from_config,function,9,17,15,146,8.59,0,3,"['config', 'sd', 'gpu', 'eval_mode']","[None, None, None, None]","[None, None, 'True', 'True']",183,[],"['instantiate_from_config', 'model.load_state_dict', 'model.cuda', 'model.eval']",4
scripts/sample_fast.py:load_model,load_model,function,18,48,39,494,10.29,0,4,"['config', 'sd', 'gpu', 'eval_mode']","[None, None, None, None]","[None, None, 'True', 'True']",194,[],"['instantiate_from_config', 'model.load_state_dict', 'model.cuda', 'model.eval', 'load_model', 'torch.load', 'print', 'load_model_from_config']",8
taming/lr_scheduler.py:LambdaWarmUpCosineScheduler,LambdaWarmUpCosineScheduler,class,24,81,60,781,9.64,0,3,[],[],[],4,[],[],0
taming/lr_scheduler.py:LambdaWarmUpCosineScheduler:__init__,LambdaWarmUpCosineScheduler:__init__,method,13,14,14,195,13.93,0,0,"['self', 'warm_up_steps', 'lr_min', 'lr_max', 'lr_start', 'max_decay_steps', 'verbosity_interval']","[None, None, None, None, None, None, None]","[None, None, None, None, None, None, '0']",8,[],[],0
taming/lr_scheduler.py:LambdaWarmUpCosineScheduler:schedule,LambdaWarmUpCosineScheduler:schedule,method,11,51,38,427,8.37,0,3,"['self', 'n']","[None, None]","[None, None]",17,[],"['print', 'min', 'np.cos']",3
taming/lr_scheduler.py:LambdaWarmUpCosineScheduler:__call__,LambdaWarmUpCosineScheduler:__call__,method,2,2,2,22,11.0,0,0,"['self', 'n']","[None, None]","[None, None]",32,[],['self.schedule'],1
taming/util.py:download,download,function,10,29,25,326,11.24,1,1,"['url', 'local_path', 'chunk_size']","[None, None, None]","[None, None, '1024']",18,[],"['os.makedirs', 'requests.get', 'int', 'tqdm', 'open', 'r.iter_content', 'f.write', 'pbar.update']",8
taming/util.py:md5_hash,md5_hash,function,6,9,9,79,8.78,0,0,['path'],[None],[None],30,[],"['open', 'f.read', 'hashlib.md5']",3
taming/util.py:get_ckpt_path,get_ckpt_path,function,11,35,30,287,8.2,0,1,"['name', 'root', 'check']","[None, None, None]","[None, None, 'False']",36,[],"['md5_hash', 'print', 'download']",3
taming/util.py:retrieve,retrieve,function,22,97,66,814,8.39,1,6,"['list_or_dict', 'key', 'splitval', 'default', 'expand', 'pass_success']","[None, None, None, None, None, None]","[None, None, '""/""', 'None', 'True', 'False']",62,"['    """"""Given a nested list or dict return the desired value at key expanding\n', '    callable nodes if necessary and :attr:`expand` is ``True``. The expansion\n', '    is done in-place.\n', '\n', '    Parameters\n', '    ----------\n', '        list_or_dict : list or dict\n', '            Possibly nested list or dictionary.\n', '        key : str\n', '            key/to/value, path like string describing all keys necessary to\n', '            consider to get to the desired value. List indices can also be\n', '            passed here.\n', '        splitval : str\n', '            String that defines the delimiter between keys of the\n', '            different depth levels in `key`.\n', '        default : obj\n', '            Value returned if :attr:`key` is not found.\n', '        expand : bool\n', '            Whether to expand callable nodes on the path or not.\n', '\n', '    Returns\n', '    -------\n', '        The desired value or if :attr:`default` is not ``None`` and the\n', '        :attr:`key` is not found returns ``default``.\n', '\n', '    Raises\n', '    ------\n', '        Exception if ``key`` not in ``list_or_dict`` and :attr:`default` is\n', '        ``None``.\n', '    """"""\n']","['key.split', 'callable', 'KeyNotFoundError', 'ValueError', 'list_or_dict', 'isinstance', 'list_or_dict[int']",7
taming/util.py:KeyNotFoundError,KeyNotFoundError,class,11,33,28,344,10.42,0,2,[],[],[],47,[],[],0
taming/util.py:KeyNotFoundError:__init__,KeyNotFoundError:__init__,method,10,28,23,296,10.57,0,2,"['self', 'cause', 'keys', 'visited']","[None, None, None, None]","[None, None, 'None', 'None']",48,[],"['list', 'messages.append', 'super']",3
taming/data/ade20k.py:Examples,Examples,class,9,13,13,305,23.46,0,0,[],[],[],11,[],[],0
taming/data/ade20k.py:ADE20kBase,ADE20kBase,class,49,224,137,2728,12.18,1,10,[],[],[],22,[],[],0
taming/data/ade20k.py:ADE20kTrain,ADE20kTrain,class,6,16,15,245,15.31,0,0,[],[],[],101,[],[],0
taming/data/ade20k.py:ADE20kValidation,ADE20kValidation,class,2,4,4,38,9.5,0,0,[],[],[],111,[],[],0
taming/data/ade20k.py:Examples:__init__,Examples:__init__,method,8,8,8,235,29.38,0,0,"['self', 'size', 'random_crop', 'interpolation']","[None, None, None, None]","[None, '256', 'False', '""bicubic""']",12,[],['super'],1
taming/data/ade20k.py:ADE20kBase:__init__,ADE20kBase:__init__,method,32,150,93,1798,11.99,1,6,"['self', 'config', 'size', 'random_crop', 'interpolation', 'crop_size']","[None, None, None, None, None, None]","[None, 'None', 'None', 'False', '""bicubic""', 'None']",23,[],"['self.get_split', 'open', 'f.read', 'dict', 'len', 'l.replace', 'albumentations.SmallestMaxSize', 'albumentations.CenterCrop', 'albumentations.RandomCrop']",9
taming/data/ade20k.py:ADE20kBase:__len__,ADE20kBase:__len__,method,2,2,2,18,9.0,0,0,['self'],[None],[None],75,[],[],0
taming/data/ade20k.py:ADE20kBase:__getitem__,ADE20kBase:__getitem__,method,18,60,40,770,12.83,0,4,"['self', 'i']","[None, None]","[None, None]",78,[],"['dict', 'Image.open', 'image.convert', 'np.array', 'self.image_rescaler', 'self.segmentation_rescaler', 'self.preprocessor', 'np.eye']",8
taming/data/ade20k.py:ADE20kTrain:__init__,ADE20kTrain:__init__,method,3,5,5,114,22.8,0,0,"['self', 'config', 'size', 'random_crop', 'interpolation', 'crop_size']","[None, None, None, None, None, None]","[None, 'None', 'None', 'True', '""bicubic""', 'None']",103,[],['super'],1
taming/data/ade20k.py:ADE20kTrain:get_split,ADE20kTrain:get_split,method,1,2,2,13,6.5,0,0,['self'],[None],[None],107,[],[],0
taming/data/ade20k.py:ADE20kValidation:get_split,ADE20kValidation:get_split,method,1,2,2,18,9.0,0,0,['self'],[None],[None],107,[],[],0
taming/data/base.py:ConcatDatasetWithIndex,ConcatDatasetWithIndex,class,9,40,31,345,8.62,0,3,[],[],[],8,[],[],0
taming/data/base.py:ImagePaths,ImagePaths,class,33,89,67,1065,11.97,1,3,[],[],[],23,[],[],0
taming/data/base.py:NumpyPaths,NumpyPaths,class,8,26,19,300,11.54,0,0,[],[],[],62,[],[],0
taming/data/base.py:ConcatDatasetWithIndex:__getitem__,ConcatDatasetWithIndex:__getitem__,method,8,37,28,319,8.62,0,3,"['self', 'idx']","[None, None]","[None, None]",10,[],"['len', 'ValueError', 'bisect.bisect_right']",3
taming/data/base.py:ImagePaths:__init__,ImagePaths:__init__,method,17,44,34,535,12.16,0,2,"['self', 'paths', 'size', 'random_crop', 'labels']","[None, None, None, None, None]","[None, None, 'None', 'False', 'None']",24,[],"['dict', 'len', 'albumentations.SmallestMaxSize', 'albumentations.CenterCrop', 'albumentations.RandomCrop', 'albumentations.Compose']",6
taming/data/base.py:ImagePaths:__len__,ImagePaths:__len__,method,2,2,2,18,9.0,0,0,['self'],[None],[None],42,[],[],0
taming/data/base.py:ImagePaths:preprocess_image,ImagePaths:preprocess_image,method,7,17,12,219,12.88,0,1,"['self', 'image_path']","[None, None]","[None, None]",45,[],"['Image.open', 'image.convert', 'np.array', 'self.preprocessor']",4
taming/data/base.py:ImagePaths:__getitem__,ImagePaths:__getitem__,method,8,12,11,145,12.08,1,0,"['self', 'i']","[None, None]","[None, None]",54,[],"['dict', 'self.preprocess_image']",2
taming/data/base.py:NumpyPaths:preprocess_image,NumpyPaths:preprocess_image,method,7,23,16,262,11.39,0,0,"['self', 'image_path']","[None, None]","[None, None]",45,[],"['np.load', 'np.transpose', 'Image.fromarray', 'np.array', 'self.preprocessor']",5
taming/data/coco.py:Examples,Examples,class,9,13,13,298,22.92,0,0,[],[],[],12,[],[],0
taming/data/coco.py:CocoBase,CocoBase,class,91,283,199,4015,14.19,2,11,[],[],[],22,[],[],0
taming/data/coco.py:CocoImagesAndCaptionsTrain,CocoImagesAndCaptionsTrain,class,10,18,17,377,20.94,0,0,[],[],[],151,[],[],0
taming/data/coco.py:CocoImagesAndCaptionsValidation,CocoImagesAndCaptionsValidation,class,11,20,19,421,21.05,0,0,[],[],[],164,[],[],0
taming/data/coco.py:Examples:__init__,Examples:__init__,method,8,8,8,228,28.5,0,0,"['self', 'size', 'random_crop', 'interpolation']","[None, None, None, None]","[None, '256', 'False', '""bicubic""']",13,[],['super'],1
taming/data/coco.py:CocoBase:__init__,CocoBase:__init__,method,57,160,119,2291,14.32,2,7,"['self', 'size', 'dataroot', 'datajson', 'onehot_segmentation', 'use_stuffthing', 'crop_size', 'force_no_crop', 'given_files']","[None, None, None, None, None, None, None, None, None]","[None, 'None', '""""', '""""', 'False', 'False', 'None', 'False', 'None']",24,[],"['self.get_split', 'NotImplemented', 'open', 'json.load', 'dict', 'data_json.split', 'data_json.endswith', 'list', 'tqdm', 'albumentations.SmallestMaxSize', 'albumentations.CenterCrop', 'albumentations.RandomCrop', 'albumentations.Compose', 'albumentations.Resize']",14
taming/data/coco.py:CocoBase:__len__,CocoBase:__len__,method,1,2,2,35,17.5,0,0,['self'],[None],[None],93,[],['len'],1
taming/data/coco.py:CocoBase:preprocess_image,CocoBase:preprocess_image,method,25,73,47,945,12.95,0,4,"['self', 'image_path', 'segmentation_path']","[None, None, None]","[None, None, None]",96,[],"['Image.open', 'image.convert', 'np.array', 'segmentation.convert', 'self.preprocessor', 'np.ravel', 'np.zeros', 'onehot.reshape']",8
taming/data/coco.py:CocoBase:__getitem__,CocoBase:__getitem__,method,14,29,28,490,16.9,0,0,"['self', 'i']","[None, None]","[None, None]",134,[],"['self.preprocess_image', 'len', 'img_path.split']",3
taming/data/coco.py:CocoImagesAndCaptionsTrain:__init__,CocoImagesAndCaptionsTrain:__init__,method,7,7,7,237,33.86,0,0,"['self', 'size', 'onehot_segmentation', 'use_stuffthing', 'crop_size', 'force_no_crop']","[None, None, None, None, None, None]","[None, None, 'False', 'False', 'None', 'False']",153,[],['super'],1
taming/data/coco.py:CocoImagesAndCaptionsTrain:get_split,CocoImagesAndCaptionsTrain:get_split,method,1,2,2,13,6.5,0,0,['self'],[None],[None],160,[],[],0
taming/data/coco.py:CocoImagesAndCaptionsValidation:__init__,CocoImagesAndCaptionsValidation:__init__,method,8,8,8,258,32.25,0,0,"['self', 'size', 'onehot_segmentation', 'use_stuffthing', 'crop_size', 'force_no_crop', 'given_files']","[None, None, None, None, None, None, None]","[None, None, 'False', 'False', 'None', 'False', 'None']",166,[],['super'],1
taming/data/coco.py:CocoImagesAndCaptionsValidation:get_split,CocoImagesAndCaptionsValidation:get_split,method,1,2,2,18,9.0,0,0,['self'],[None],[None],160,"['    """"""returns a pair of (image, caption)""""""\n']",[],0
taming/data/custom.py:CustomBase,CustomBase,class,8,18,14,165,9.17,0,0,[],[],[],9,[],[],0
taming/data/custom.py:CustomTrain,CustomTrain,class,8,16,16,202,12.62,0,0,[],[],[],23,[],[],0
taming/data/custom.py:CustomTest,CustomTest,class,8,16,16,194,12.12,0,0,[],[],[],31,[],[],0
taming/data/custom.py:CustomBase:__init__,CustomBase:__init__,method,2,3,3,33,11.0,0,0,"['self', '*args', '**kwargs']","[None, None, None]","[None, None, None]",10,[],['super'],1
taming/data/custom.py:CustomBase:__len__,CustomBase:__len__,method,1,2,2,20,10.0,0,0,['self'],[None],[None],14,[],['len'],1
taming/data/custom.py:CustomBase:__getitem__,CustomBase:__getitem__,method,3,4,3,34,8.5,0,0,"['self', 'i']","[None, None]","[None, None]",17,[],[],0
taming/data/custom.py:CustomTrain:__init__,CustomTrain:__init__,method,7,12,12,152,12.67,0,0,"['self', 'size', 'training_images_list_file']","[None, None, None]","[None, None, None]",24,[],"['super', 'open', 'f.read', 'ImagePaths']",4
taming/data/custom.py:CustomTest:__init__,CustomTest:__init__,method,7,12,12,148,12.33,0,0,"['self', 'size', 'test_images_list_file']","[None, None, None]","[None, None, None]",32,[],"['super', 'open', 'f.read', 'ImagePaths']",4
taming/data/faceshq.py:FacesBase,FacesBase,class,13,36,29,254,7.06,1,1,[],[],[],9,[],[],0
taming/data/faceshq.py:CelebAHQTrain,CelebAHQTrain,class,12,27,27,279,10.33,0,0,[],[],[],29,[],[],0
taming/data/faceshq.py:CelebAHQValidation,CelebAHQValidation,class,12,27,27,284,10.52,0,0,[],[],[],40,[],[],0
taming/data/faceshq.py:FFHQTrain,FFHQTrain,class,12,27,27,271,10.04,0,0,[],[],[],51,[],[],0
taming/data/faceshq.py:FFHQValidation,FFHQValidation,class,12,27,27,276,10.22,0,0,[],[],[],62,[],[],0
taming/data/faceshq.py:FacesHQTrain,FacesHQTrain,class,25,66,53,769,11.65,0,4,[],[],[],73,[],[],0
taming/data/faceshq.py:FacesHQValidation,FacesHQValidation,class,25,66,53,779,11.8,0,4,[],[],[],105,[],[],0
taming/data/faceshq.py:FacesBase:__init__,FacesBase:__init__,method,3,5,5,48,9.6,0,0,"['self', '*args', '**kwargs']","[None, None, None]","[None, None, None]",10,[],['super'],1
taming/data/faceshq.py:FacesBase:__len__,FacesBase:__len__,method,1,2,2,20,10.0,0,0,['self'],[None],[None],15,[],['len'],1
taming/data/faceshq.py:FacesBase:__getitem__,FacesBase:__getitem__,method,8,20,18,108,5.4,1,1,"['self', 'i']","[None, None]","[None, None]",18,[],[],0
taming/data/faceshq.py:CelebAHQTrain:__init__,CelebAHQTrain:__init__,method,11,23,23,245,10.65,0,0,"['self', 'size', 'keys']","[None, None, None]","[None, None, 'None']",30,[],"['super', 'open', 'f.read', 'NumpyPaths']",4
taming/data/faceshq.py:CelebAHQValidation:__init__,CelebAHQValidation:__init__,method,11,23,23,250,10.87,0,0,"['self', 'size', 'keys']","[None, None, None]","[None, None, 'None']",30,[],"['super', 'open', 'f.read', 'NumpyPaths']",4
taming/data/faceshq.py:FFHQTrain:__init__,FFHQTrain:__init__,method,11,23,23,237,10.3,0,0,"['self', 'size', 'keys']","[None, None, None]","[None, None, 'None']",30,[],"['super', 'open', 'f.read', 'ImagePaths']",4
taming/data/faceshq.py:FFHQValidation:__init__,FFHQValidation:__init__,method,11,23,23,242,10.52,0,0,"['self', 'size', 'keys']","[None, None, None]","[None, None, 'None']",30,[],"['super', 'open', 'f.read', 'ImagePaths']",4
taming/data/faceshq.py:FacesHQTrain:__init__,FacesHQTrain:__init__,method,12,24,21,329,13.71,0,2,"['self', 'size', 'keys', 'crop_size', 'coord']","[None, None, None, None, None]","[None, None, 'None', 'None', 'False']",75,[],"['CelebAHQTrain', 'FFHQTrain', 'ConcatDatasetWithIndex', 'albumentations.RandomCrop', 'albumentations.Compose']",5
taming/data/faceshq.py:FacesHQTrain:__len__,FacesHQTrain:__len__,method,1,2,2,20,10.0,0,0,['self'],[None],[None],15,[],['len'],1
taming/data/faceshq.py:FacesHQTrain:__getitem__,FacesHQTrain:__getitem__,method,13,29,25,315,10.86,0,2,"['self', 'i']","[None, None]","[None, None]",18,[],"['hasattr', 'self.cropper', 'np.arange']",3
taming/data/faceshq.py:FacesHQValidation:__init__,FacesHQValidation:__init__,method,12,24,21,339,14.12,0,2,"['self', 'size', 'keys', 'crop_size', 'coord']","[None, None, None, None, None]","[None, None, 'None', 'None', 'False']",75,[],"['CelebAHQValidation', 'FFHQValidation', 'ConcatDatasetWithIndex', 'albumentations.CenterCrop', 'albumentations.Compose']",5
taming/data/faceshq.py:FacesHQValidation:__len__,FacesHQValidation:__len__,method,1,2,2,20,10.0,0,0,['self'],[None],[None],15,[],['len'],1
taming/data/faceshq.py:FacesHQValidation:__getitem__,FacesHQValidation:__getitem__,method,13,29,25,315,10.86,0,2,"['self', 'i']","[None, None]","[None, None]",18,[],"['hasattr', 'self.cropper', 'np.arange']",3
taming/data/imagenet.py:give_synsets_from_indices,give_synsets_from_indices,function,10,24,21,206,8.58,1,0,"['indices', 'path_to_yaml']","[None, None]","[None, '""data/imagenet_idx_to_synset.yaml""']",15,[],"['open', 'yaml.load', 'synsets.append', 'print']",4
taming/data/imagenet.py:str_to_indices,str_to_indices,function,14,42,38,336,8.0,2,1,['string'],[None],[None],25,"['    """"""Expects a string in the format \'32-123, 256, 280-321\'""""""\n']","['string.endswith', 'string.split', 'sub.split', 'len', 'indices.append', 'range', 'int', 'indices.extend', 'sorted']",9
taming/data/imagenet.py:get_preprocessor,get_preprocessor,function,18,60,35,840,14.0,0,3,"['size', 'random_crop', 'additional_targets', 'crop_size']","[None, None, None, None]","['None', 'False', 'None', 'None']",244,[],"['list', 'albumentations.SmallestMaxSize', 'transforms.append', 'albumentations.CenterCrop', 'albumentations.RandomCrop', 'albumentations.HorizontalFlip', 'albumentations.Compose']",7
taming/data/imagenet.py:rgba_to_depth,rgba_to_depth,function,9,17,15,146,8.59,0,0,['x'],[None],[None],273,[],"['len', 'x.copy', 'y.reshape', 'np.ascontiguousarray']",4
taming/data/imagenet.py:imscale,imscale,function,28,66,50,491,7.44,0,2,"['x', 'factor', 'keepshapes', 'keepmode']","[None, None, None, None]","[None, None, 'False', '""bicubic""']",416,[],"['x.min', 'x.max', 'lr.clip', 'Image.fromarray', 'lr.resize', 'np.array', 'lr.astype']",7
taming/data/imagenet.py:ImageNetBase,ImageNetBase,class,61,196,142,2298,11.72,3,5,[],[],[],41,[],[],0
taming/data/imagenet.py:ImageNetTrain,ImageNetTrain,class,40,122,96,1551,12.71,1,3,[],[],[],123,[],[],0
taming/data/imagenet.py:ImageNetValidation,ImageNetValidation,class,49,157,122,1930,12.29,2,4,[],[],[],178,[],[],0
taming/data/imagenet.py:BaseWithDepth,BaseWithDepth,class,32,89,68,1176,13.21,0,3,[],[],[],282,[],[],0
taming/data/imagenet.py:ImageNetTrainWithDepth,ImageNetTrainWithDepth,class,11,32,25,409,12.78,0,1,[],[],[],328,[],[],0
taming/data/imagenet.py:ImageNetValidationWithDepth,ImageNetValidationWithDepth,class,11,30,23,376,12.53,0,1,[],[],[],346,[],[],0
taming/data/imagenet.py:RINTrainWithDepth,RINTrainWithDepth,class,5,21,21,264,12.57,0,0,[],[],[],363,[],[],0
taming/data/imagenet.py:RINValidationWithDepth,RINValidationWithDepth,class,5,21,21,265,12.62,0,0,[],[],[],370,[],[],0
taming/data/imagenet.py:DRINExamples,DRINExamples,class,22,85,65,1123,13.21,0,0,[],[],[],377,[],[],0
taming/data/imagenet.py:ImageNetScale,ImageNetScale,class,43,141,89,1626,11.53,0,9,[],[],[],446,[],[],0
taming/data/imagenet.py:ImageNetScaleTrain,ImageNetScaleTrain,class,5,10,9,136,13.6,0,0,[],[],[],510,[],[],0
taming/data/imagenet.py:ImageNetScaleValidation,ImageNetScaleValidation,class,3,4,4,45,11.25,0,0,[],[],[],517,[],[],0
taming/data/imagenet.py:ImageNetEdges,ImageNetEdges,class,21,39,34,447,11.46,0,1,[],[],[],526,[],[],0
taming/data/imagenet.py:ImageNetEdgesTrain,ImageNetEdgesTrain,class,5,10,9,136,13.6,0,0,[],[],[],549,[],[],0
taming/data/imagenet.py:ImageNetEdgesValidation,ImageNetEdgesValidation,class,3,4,4,45,11.25,0,0,[],[],[],556,[],[],0
taming/data/imagenet.py:ImageNetBase:__init__,ImageNetBase:__init__,method,10,13,12,207,15.92,0,1,"['self', 'config']","[None, None]","[None, 'None']",42,[],"['OmegaConf.create', 'type', 'OmegaConf.to_container', 'self._prepare', 'self._prepare_synset_to_human', 'self._prepare_idx_to_synset', 'self._load']",7
taming/data/imagenet.py:ImageNetBase:__len__,ImageNetBase:__len__,method,1,2,2,20,10.0,0,0,['self'],[None],[None],51,[],['len'],1
taming/data/imagenet.py:ImageNetBase:__getitem__,ImageNetBase:__getitem__,method,2,2,2,18,9.0,0,0,"['self', 'i']","[None, None]","[None, None]",54,[],[],0
taming/data/imagenet.py:ImageNetBase:_prepare,ImageNetBase:_prepare,method,1,2,2,26,13.0,0,0,['self'],[None],[None],57,[],['NotImplementedError'],1
taming/data/imagenet.py:ImageNetBase:_filter_relpaths,ImageNetBase:_filter_relpaths,method,12,48,36,400,8.33,1,2,"['self', 'relpaths']","[None, None]","[None, None]",60,[],"['set', 'rpath.split', 'str_to_indices', 'give_synsets_from_indices', 'files.append']",5
taming/data/imagenet.py:ImageNetBase:_prepare_synset_to_human,ImageNetBase:_prepare_synset_to_human,method,6,15,15,253,16.87,0,1,['self'],[None],[None],77,[],['download'],1
taming/data/imagenet.py:ImageNetBase:_prepare_idx_to_synset,ImageNetBase:_prepare_idx_to_synset,method,4,10,10,188,18.8,0,1,['self'],[None],[None],85,[],['download'],1
taming/data/imagenet.py:ImageNetBase:_load,ImageNetBase:_load,method,22,85,64,967,11.38,2,0,['self'],[None],[None],91,[],"['open', 'f.read', 'len', 'self._filter_relpaths', 'print', 'np.unique', 'dict', 'enumerate', 'np.array', 'ImagePaths', 'size=retrieve']",11
taming/data/imagenet.py:ImageNetTrain:_prepare,ImageNetTrain:_prepare,method,34,106,82,1341,12.65,1,3,['self'],[None],[None],57,[],"['retrieve', 'bdu.is_prepared', 'print', 'at.get', 'os.makedirs', 'tarfile.open', 'tar.extractall', 'sorted', 'tqdm', 'glob.glob', 'open', 'f.write', 'bdu.mark_prepared']",13
taming/data/imagenet.py:ImageNetValidation:_prepare,ImageNetValidation:_prepare,method,42,137,104,1614,11.78,2,4,['self'],[None],[None],57,[],"['retrieve', 'bdu.is_prepared', 'print', 'at.get', 'os.makedirs', 'tarfile.open', 'tar.extractall', 'download', 'open', 'f.read', 'dict', 'np.unique', 'synset_dict.items', 'shutil.move', 'glob.glob', 'sorted', 'f.write', 'bdu.mark_prepared']",18
taming/data/imagenet.py:BaseWithDepth:__init__,BaseWithDepth:__init__,method,13,31,23,418,13.48,0,2,"['self', 'config', 'size', 'random_crop', 'crop_size', 'root']","[None, None, None, None, None, None]","[None, 'None', 'None', 'False', 'None', 'None']",285,[],"['self.get_base_dset', 'get_preprocessor', 'albumentations.Compose']",3
taming/data/imagenet.py:BaseWithDepth:__len__,BaseWithDepth:__len__,method,1,2,2,25,12.5,0,0,['self'],[None],[None],51,[],['len'],1
taming/data/imagenet.py:BaseWithDepth:preprocess_depth,BaseWithDepth:preprocess_depth,method,5,12,9,149,12.42,0,0,"['self', 'path']","[None, None]","[None, None]",305,[],"['np.array', 'rgba_to_depth', 'depth.min', 'depth.max']",4
taming/data/imagenet.py:BaseWithDepth:__getitem__,BaseWithDepth:__getitem__,method,13,28,23,381,13.61,0,1,"['self', 'i']","[None, None]","[None, None]",54,[],"['self.preprocess_depth', 'min', 'self.rescaler', 'self.preprocessor']",4
taming/data/imagenet.py:ImageNetTrainWithDepth:__init__,ImageNetTrainWithDepth:__init__,method,3,4,4,79,19.75,0,0,"['self', 'random_crop', 'sub_indices', '**kwargs']","[None, None, None, None]","[None, 'True', 'None', None]",330,[],['super'],1
taming/data/imagenet.py:ImageNetTrainWithDepth:get_base_dset,ImageNetTrainWithDepth:get_base_dset,method,3,10,9,107,10.7,0,1,['self'],[None],[None],334,[],['ImageNetTrain'],1
taming/data/imagenet.py:ImageNetTrainWithDepth:get_depth_path,ImageNetTrainWithDepth:get_depth_path,method,4,8,6,108,13.5,0,0,"['self', 'e']","[None, None]","[None, None]",340,[],[],0
taming/data/imagenet.py:ImageNetValidationWithDepth:__init__,ImageNetValidationWithDepth:__init__,method,3,3,3,55,18.33,0,0,"['self', 'sub_indices', '**kwargs']","[None, None, None]","[None, 'None', None]",347,[],['super'],1
taming/data/imagenet.py:ImageNetValidationWithDepth:get_base_dset,ImageNetValidationWithDepth:get_base_dset,method,3,10,9,117,11.7,0,1,['self'],[None],[None],334,[],['ImageNetValidation'],1
taming/data/imagenet.py:ImageNetValidationWithDepth:get_depth_path,ImageNetValidationWithDepth:get_depth_path,method,4,8,6,106,13.25,0,0,"['self', 'e']","[None, None]","[None, None]",340,[],[],0
taming/data/imagenet.py:RINTrainWithDepth:__init__,RINTrainWithDepth:__init__,method,4,15,15,191,12.73,0,0,"['self', 'config', 'size', 'random_crop', 'crop_size']","[None, None, None, None, None]","[None, 'None', 'None', 'True', 'None']",364,[],['super'],1
taming/data/imagenet.py:RINValidationWithDepth:__init__,RINValidationWithDepth:__init__,method,4,15,15,191,12.73,0,0,"['self', 'config', 'size', 'random_crop', 'crop_size']","[None, None, None, None, None]","[None, 'None', 'None', 'False', 'None']",371,[],['super'],1
taming/data/imagenet.py:DRINExamples:__init__,DRINExamples:__init__,method,7,26,23,340,13.08,0,0,['self'],[None],[None],378,[],"['get_preprocessor', 'open', 'f.read', 'relpath.replace']",4
taming/data/imagenet.py:DRINExamples:__len__,DRINExamples:__len__,method,1,2,2,27,13.5,0,0,['self'],[None],[None],51,[],['len'],1
taming/data/imagenet.py:DRINExamples:preprocess_image,DRINExamples:preprocess_image,method,7,17,12,219,12.88,0,1,"['self', 'image_path']","[None, None]","[None, None]",390,[],"['Image.open', 'image.convert', 'np.array', 'self.preprocessor']",4
taming/data/imagenet.py:DRINExamples:preprocess_depth,DRINExamples:preprocess_depth,method,5,12,9,149,12.42,0,0,"['self', 'path']","[None, None]","[None, None]",305,[],"['np.array', 'rgba_to_depth', 'depth.min', 'depth.max']",4
taming/data/imagenet.py:DRINExamples:__getitem__,DRINExamples:__getitem__,method,8,15,12,253,16.87,0,0,"['self', 'i']","[None, None]","[None, None]",54,[],"['dict', 'self.preprocess_image', 'self.preprocess_depth', 'self.preprocessor']",4
taming/data/imagenet.py:ImageNetScale:__init__,ImageNetScale:__init__,method,28,85,50,967,11.38,0,7,"['self', 'size', 'crop_size', 'random_crop', 'up_factor', 'hr_factor', 'keep_mode']","[None, None, None, None, None, None, None]","[None, 'None', 'None', 'False', 'None', 'None', '""bicubic""']",447,[],"['self.get_base', 'list', 'albumentations.SmallestMaxSize', 'transforms.append', 'len', 'albumentations.CenterCrop', 'albumentations.RandomCrop', 'albumentations.Compose']",8
taming/data/imagenet.py:ImageNetScale:__len__,ImageNetScale:__len__,method,1,2,2,20,10.0,0,0,['self'],[None],[None],51,[],['len'],1
taming/data/imagenet.py:ImageNetScale:__getitem__,ImageNetScale:__getitem__,method,17,41,34,482,11.76,0,2,"['self', 'i']","[None, None]","[None, None]",54,[],"['imscale', 'min', 'self.rescaler', 'self.preprocessor']",4
taming/data/imagenet.py:ImageNetScaleTrain:__init__,ImageNetScaleTrain:__init__,method,1,2,2,50,25.0,0,0,"['self', 'random_crop', '**kwargs']","[None, None, None]","[None, 'True', None]",511,[],['super'],1
taming/data/imagenet.py:ImageNetScaleTrain:get_base,ImageNetScaleTrain:get_base,method,2,2,2,21,10.5,0,0,['self'],[None],[None],514,[],['ImageNetTrain'],1
taming/data/imagenet.py:ImageNetScaleValidation:get_base,ImageNetScaleValidation:get_base,method,2,2,2,26,13.0,0,0,['self'],[None],[None],514,[],['ImageNetValidation'],1
taming/data/imagenet.py:ImageNetEdges:__init__,ImageNetEdges:__init__,method,1,2,2,38,19.0,0,0,"['self', 'up_factor', '**kwargs']","[None, None, None]","[None, '1', None]",527,[],['super'],1
taming/data/imagenet.py:ImageNetEdges:__getitem__,ImageNetEdges:__getitem__,method,18,30,26,344,11.47,0,1,"['self', 'i']","[None, None]","[None, None]",54,[],"['min', 'self.rescaler', 'canny', 'lr.astype', 'self.preprocessor']",5
taming/data/imagenet.py:ImageNetEdgesTrain:__init__,ImageNetEdgesTrain:__init__,method,1,2,2,50,25.0,0,0,"['self', 'random_crop', '**kwargs']","[None, None, None]","[None, 'True', None]",511,[],['super'],1
taming/data/imagenet.py:ImageNetEdgesTrain:get_base,ImageNetEdgesTrain:get_base,method,2,2,2,21,10.5,0,0,['self'],[None],[None],514,[],['ImageNetTrain'],1
taming/data/imagenet.py:ImageNetEdgesValidation:get_base,ImageNetEdgesValidation:get_base,method,2,2,2,26,13.0,0,0,['self'],[None],[None],514,[],['ImageNetValidation'],1
taming/data/sflckr.py:SegmentationBase,SegmentationBase,class,50,183,128,2335,12.76,0,8,[],[],[],9,[],[],0
taming/data/sflckr.py:Examples,Examples,class,7,11,11,266,24.18,0,0,[],[],[],86,[],[],0
taming/data/sflckr.py:SegmentationBase:__init__,SegmentationBase:__init__,method,31,95,73,1246,13.12,0,3,"['self', 'data_csv', 'data_root', 'segmentation_root', 'size', 'random_crop', 'interpolation', 'n_labels', 'shift_segmentation', '']","[None, None, None, None, None, None, None, None, None, None]","[None, None, None, None, 'None', 'False', '""bicubic""', '182', 'False', None]",10,[],"['open', 'f.read', 'len', 'l.replace', 'albumentations.SmallestMaxSize', 'albumentations.CenterCrop', 'albumentations.RandomCrop']",7
taming/data/sflckr.py:SegmentationBase:__len__,SegmentationBase:__len__,method,2,2,2,18,9.0,0,0,['self'],[None],[None],52,[],[],0
taming/data/sflckr.py:SegmentationBase:__getitem__,SegmentationBase:__getitem__,method,21,70,48,876,12.51,0,5,"['self', 'i']","[None, None]","[None, None]",55,[],"['dict', 'Image.open', 'image.convert', 'np.array', 'self.image_rescaler', 'self.segmentation_rescaler', 'self.preprocessor', 'np.eye']",8
taming/data/sflckr.py:Examples:__init__,Examples:__init__,method,6,6,6,195,32.5,0,0,"['self', 'size', 'random_crop', 'interpolation']","[None, None, None, None]","[None, 'None', 'False', '""bicubic""']",87,[],['super'],1
taming/data/utils.py:unpack,unpack,function,7,32,24,406,12.69,0,1,['path'],[None],[None],9,[],"['path.endswith', 'tarfile.open', 'tar.extractall', 'zipfile.ZipFile', 'f.extractall', 'NotImplementedError']",6
taming/data/utils.py:reporthook,reporthook,function,5,16,16,103,6.44,0,1,['bar'],[None],[None],25,"['    """"""tqdm progress bar for downloads.""""""\n']","['hook', 'bar.update']",2
taming/data/utils.py:get_root,get_root,function,5,9,8,84,9.33,0,0,['name'],[None],[None],36,[],['os.makedirs'],1
taming/data/utils.py:is_prepared,is_prepared,function,2,2,2,44,22.0,0,0,['root'],[None],[None],43,[],['Path'],1
taming/data/utils.py:mark_prepared,mark_prepared,function,1,1,1,37,37.0,0,0,['root'],[None],[None],47,[],['Path'],1
taming/data/utils.py:prompt_download,prompt_download,function,9,50,37,398,7.96,1,2,"['file_', 'source', 'target_dir', 'content_dir']","[None, None, None, None]","[None, None, None, 'None']",51,[],"['print', 'input']",2
taming/data/utils.py:download_url,download_url,function,15,37,35,401,10.84,1,0,"['file_', 'url', 'target_dir']","[None, None, None]","[None, None, None]",71,[],"['os.makedirs', 'tqdm', 'reporthook=reporthook', 'download_urls', 'dict', 'urls.items', 'download_url']",7
taming/data/utils.py:download_urls,download_urls,function,8,15,14,116,7.73,1,0,"['urls', 'target_dir']","[None, None]","[None, None]",81,[],"['dict', 'urls.items', 'download_url']",3
taming/data/utils.py:quadratic_crop,quadratic_crop,function,27,91,67,679,7.46,0,1,"['x', 'bbox', 'alpha']","[None, None, None]","[None, None, '1.0']",89,"['    """"""bbox is xmin, ymin, xmax, ymax""""""\n']","['np.array', 'np.clip', 'max', 'int', 'min', 'np.pad']",6
taming/models/cond_transformer.py:disabled_train,disabled_train,function,1,2,2,10,5.0,0,0,"['self', 'mode']","[None, None]","[None, 'True']",10,"['    """"""Overwrite model.train with this function to make sure train/eval mode\n', '    does not change anymore.""""""\n']",[],0
taming/models/cond_transformer.py:Net2NetTransformer,Net2NetTransformer,class,193,810,465,8645,10.67,7,21,[],[],[],16,[],[],0
taming/models/cond_transformer.py:Net2NetTransformer:__init__,Net2NetTransformer:__init__,method,21,33,29,623,18.88,0,2,"['self', 'transformer_config', 'first_stage_config', 'cond_stage_config', 'permuter_config', 'ckpt_path', 'ignore_keys', 'first_stage_key', 'cond_stage_key', 'downsample_cond_size', 'pkeep', 'sos_token', 'unconditional', '']","[None, None, None, None, None, None, None, None, None, None, None, None, None, None]","[None, None, None, None, 'None', 'None', '[]', '""image""', '""depth""', '-1', '1.0', '0', 'False', None]",17,[],"['super', 'self.init_first_stage_from_ckpt', 'self.init_cond_stage_from_ckpt', 'instantiate_from_config', 'self.init_from_ckpt']",5
taming/models/cond_transformer.py:Net2NetTransformer:init_from_ckpt,Net2NetTransformer:init_from_ckpt,method,12,25,22,238,9.52,2,1,"['self', 'path', 'ignore_keys']","[None, None, None]","[None, None, 'list(']",48,[],"['torch.load', 'sd.keys', 'k.startswith', 'self.print', 'self.load_state_dict', 'print']",6
taming/models/cond_transformer.py:Net2NetTransformer:init_first_stage_from_ckpt,Net2NetTransformer:init_first_stage_from_ckpt,method,6,8,6,112,14.0,0,0,"['self', 'config']","[None, None]","[None, None]",58,[],"['instantiate_from_config', 'model.eval']",2
taming/models/cond_transformer.py:Net2NetTransformer:init_cond_stage_from_ckpt,Net2NetTransformer:init_cond_stage_from_ckpt,method,13,51,44,529,10.37,0,1,"['self', 'config']","[None, None]","[None, None]",64,[],"['print', 'SOSProvider', 'instantiate_from_config', 'model.eval']",4
taming/models/cond_transformer.py:Net2NetTransformer:forward,Net2NetTransformer:forward,method,25,41,35,539,13.15,0,1,"['self', 'x', 'c']","[None, None, None]","[None, None, None]",80,[],"['self.encode_to_z', 'self.encode_to_c', 'torch.bernoulli', 'mask.round', 'torch.randint_like', 'torch.cat', 'self.transformer']",7
taming/models/cond_transformer.py:Net2NetTransformer:top_k_logits,Net2NetTransformer:top_k_logits,method,8,13,12,89,6.85,0,0,"['self', 'logits', 'k']","[None, None, None]","[None, None, None]",106,[],"['torch.topk', 'logits.clone']",2
taming/models/cond_transformer.py:Net2NetTransformer:sample,Net2NetTransformer:sample,method,38,126,74,1127,8.94,1,6,"['self', 'x', 'c', 'steps', 'temperature', 'sample', 'top_k', 'callback=lambda k']","[None, None, None, None, None, None, None, '']","[None, None, None, None, '1.0', 'False', 'None', 'lambda k: None']",113,[],"['torch.cat', 'len', 'c.clone', 'self.transformer', 'self.top_k_logits', 'F.softmax', 'probs.reshape', 'torch.multinomial', 'ix.reshape', 'torch.topk', 'range', 'callback', 'x.size']",13
taming/models/cond_transformer.py:Net2NetTransformer:encode_to_z,Net2NetTransformer:encode_to_z,method,8,12,9,142,11.83,0,0,"['self', 'x']","[None, None]","[None, None]",169,[],['self.permuter'],1
taming/models/cond_transformer.py:Net2NetTransformer:encode_to_c,Net2NetTransformer:encode_to_c,method,9,22,18,247,11.23,0,2,"['self', 'c']","[None, None]","[None, None]",176,[],"['F.interpolate', 'len', 'indices.view']",3
taming/models/cond_transformer.py:Net2NetTransformer:decode_to_img,Net2NetTransformer:decode_to_img,method,8,13,12,225,17.31,0,0,"['self', 'index', 'zshape']","[None, None, None]","[None, None, None]",185,[],"['self.permuter', 'index.reshape']",2
taming/models/cond_transformer.py:Net2NetTransformer:log_images,Net2NetTransformer:log_images,method,38,178,92,1944,10.92,0,3,"['self', 'batch', 'temperature', 'top_k', 'callback', 'lr_interface', '**kwargs']","[None, None, None, None, None, None, None]","[None, None, 'None', 'None', 'None', 'False', None]",194,[],"['dict', 'self.get_xc', 'x.to', 'c.to', 'self.encode_to_z', 'self.encode_to_c', 'self.sample', 'self.decode_to_img', 'torch.argmax', 'F.one_hot', 'c.squeeze', 'cond_rec.squeeze']",12
taming/models/cond_transformer.py:Net2NetTransformer:get_input,Net2NetTransformer:get_input,method,9,23,16,172,7.48,0,3,"['self', 'key', 'batch']","[None, None, None]","[None, None, None]",265,[],"['len', 'x.permute', 'x.float']",3
taming/models/cond_transformer.py:Net2NetTransformer:get_xc,Net2NetTransformer:get_xc,method,8,18,14,128,7.11,0,1,"['self', 'batch', 'N']","[None, None, None]","[None, None, 'None']",275,[],['self.get_input'],1
taming/models/cond_transformer.py:Net2NetTransformer:shared_step,Net2NetTransformer:shared_step,method,8,13,12,133,10.23,0,0,"['self', 'batch', 'batch_idx']","[None, None, None]","[None, None, None]",283,[],"['self.get_xc', 'self', 'F.cross_entropy', 'logits.size', 'target.reshape']",5
taming/models/cond_transformer.py:Net2NetTransformer:training_step,Net2NetTransformer:training_step,method,4,11,10,130,11.82,0,0,"['self', 'batch', 'batch_idx']","[None, None, None]","[None, None, None]",289,[],"['self.shared_step', 'self.log']",2
taming/models/cond_transformer.py:Net2NetTransformer:validation_step,Net2NetTransformer:validation_step,method,4,11,10,128,11.64,0,0,"['self', 'batch', 'batch_idx']","[None, None, None]","[None, None, None]",294,[],"['self.shared_step', 'self.log']",2
taming/models/cond_transformer.py:Net2NetTransformer:configure_optimizers,Net2NetTransformer:configure_optimizers,method,24,123,86,1112,9.04,4,1,['self'],[None],[None],299,"['        """"""\n', '        Following minGPT:\n', '        This long function is unfortunately doing something very simple and is being very defensive:\n', '        We are separating out all parameters of the model into two buckets: those that will experience\n', ""        weight decay for regularization and those that won't (biases, and layernorm/embedding weights).\n"", '        We are then returning the PyTorch optimizer object.\n', '        """"""\n']","['set', 'm.named_parameters', 'pn.endswith', 'no_decay.add', 'isinstance', 'decay.add', 'len', 'sorted']",8
taming/models/vqgan.py:VQModel,VQModel,class,98,359,224,4107,11.44,2,9,[],[],[],12,[],[],0
taming/models/vqgan.py:VQSegmentationModel,VQSegmentationModel,class,37,127,82,1475,11.61,0,1,[],[],[],159,[],[],0
taming/models/vqgan.py:VQNoDiscModel,VQNoDiscModel,class,28,100,65,1486,14.86,0,0,[],[],[],211,[],[],0
taming/models/vqgan.py:GumbelVQ,GumbelVQ,class,58,201,128,2635,13.11,0,3,[],[],[],261,[],[],0
taming/models/vqgan.py:VQModel:__init__,VQModel:__init__,method,21,51,38,677,13.27,0,3,"['self', 'ddconfig', 'lossconfig', 'n_embed', 'embed_dim', 'ckpt_path', 'ignore_keys', 'image_key', 'colorize_nlabels', 'monitor', 'remap', 'sane_index_shape', '# tell vector quantizer to return indices as bhw']","[None, None, None, None, None, None, None, None, None, None, None, None, None]","[None, None, None, None, None, 'None', '[]', '""image""', 'None', 'None', 'None', 'False', None]",13,[],"['super', 'Encoder', 'Decoder', 'instantiate_from_config', 'VectorQuantizer', 'self.init_from_ckpt', 'type', 'self.register_buffer', 'torch.randn']",9
taming/models/vqgan.py:VQModel:init_from_ckpt,VQModel:init_from_ckpt,method,11,27,24,249,9.22,2,1,"['self', 'path', 'ignore_keys']","[None, None, None]","[None, None, 'list(']",44,[],"['torch.load', 'list', 'k.startswith', 'print', 'self.load_state_dict']",5
taming/models/vqgan.py:VQModel:encode,VQModel:encode,method,8,12,8,101,8.42,0,0,"['self', 'x']","[None, None]","[None, None]",55,[],"['self.encoder', 'self.quant_conv', 'self.quantize']",3
taming/models/vqgan.py:VQModel:decode,VQModel:decode,method,9,15,12,172,11.47,0,0,"['self', 'quant']","[None, None]","[None, None]",61,[],"['self.post_quant_conv', 'self.decoder', 'decode_code', 'self.decode']",4
taming/models/vqgan.py:VQModel:decode_code,VQModel:decode_code,method,5,6,5,75,12.5,0,0,"['self', 'code_b']","[None, None]","[None, None]",66,[],['self.decode'],1
taming/models/vqgan.py:VQModel:forward,VQModel:forward,method,9,9,9,69,7.67,0,0,"['self', 'input']","[None, None]","[None, None]",71,[],"['self.encode', 'self.decode']",2
taming/models/vqgan.py:VQModel:get_input,VQModel:get_input,method,7,15,13,122,8.13,0,1,"['self', 'batch', 'k']","[None, None, None]","[None, None, None]",76,[],"['len', 'x.permute', 'x.float']",3
taming/models/vqgan.py:VQModel:training_step,VQModel:training_step,method,15,56,32,711,12.7,0,2,"['self', 'batch', 'batch_idx', 'optimizer_idx']","[None, None, None, None]","[None, None, None, None]",83,[],"['self.get_input', 'self', 'self.loss', 'self.log', 'self.log_dict']",5
taming/models/vqgan.py:VQModel:validation_step,VQModel:validation_step,method,14,44,32,592,13.45,0,0,"['self', 'batch', 'batch_idx']","[None, None, None]","[None, None, None]",104,[],"['self.get_input', 'self', 'self.loss', 'self.log', 'self.log_dict']",5
taming/models/vqgan.py:VQModel:configure_optimizers,VQModel:configure_optimizers,method,9,20,17,359,17.95,0,0,['self'],[None],[None],121,[],['list'],1
taming/models/vqgan.py:VQModel:get_last_layer,VQModel:get_last_layer,method,2,2,2,34,17.0,0,0,['self'],[None],[None],133,[],[],0
taming/models/vqgan.py:VQModel:log_images,VQModel:log_images,method,12,28,24,216,7.71,0,1,"['self', 'batch', '**kwargs']","[None, None, None]","[None, None, None]",136,[],"['dict', 'self.get_input', 'x.to', 'self', 'self.to_rgb']",5
taming/models/vqgan.py:VQModel:to_rgb,VQModel:to_rgb,method,6,20,18,217,10.85,0,1,"['self', 'x']","[None, None]","[None, None]",150,[],"['hasattr', 'self.register_buffer', 'torch.randn', 'F.conv2d']",4
taming/models/vqgan.py:VQSegmentationModel:__init__,VQSegmentationModel:__init__,method,2,7,7,93,13.29,0,0,"['self', 'n_labels', '*args', '**kwargs']","[None, None, None, None]","[None, None, None, None]",160,[],"['super', 'self.register_buffer', 'torch.randn']",3
taming/models/vqgan.py:VQSegmentationModel:configure_optimizers,VQSegmentationModel:configure_optimizers,method,7,13,12,258,19.85,0,0,['self'],[None],[None],121,[],['list'],1
taming/models/vqgan.py:VQSegmentationModel:training_step,VQSegmentationModel:training_step,method,10,19,18,208,10.95,0,0,"['self', 'batch', 'batch_idx']","[None, None, None]","[None, None, None]",174,[],"['self.get_input', 'self', 'self.loss', 'self.log_dict']",4
taming/models/vqgan.py:VQSegmentationModel:validation_step,VQSegmentationModel:validation_step,method,13,28,25,354,12.64,0,0,"['self', 'batch', 'batch_idx']","[None, None, None]","[None, None, None]",104,[],"['self.get_input', 'self', 'self.loss', 'self.log_dict', 'self.log']",5
taming/models/vqgan.py:VQSegmentationModel:log_images,VQSegmentationModel:log_images,method,15,40,33,349,8.72,0,1,"['self', 'batch', '**kwargs']","[None, None, None]","[None, None, None]",136,[],"['dict', 'self.get_input', 'x.to', 'self', 'torch.argmax', 'F.one_hot', 'xrec.squeeze', 'self.to_rgb']",8
taming/models/vqgan.py:VQNoDiscModel:__init__,VQNoDiscModel:__init__,method,5,8,8,193,24.12,0,0,"['self', 'ddconfig', 'lossconfig', 'n_embed', 'embed_dim', 'ckpt_path', 'ignore_keys', 'image_key', 'colorize_nlabels']","[None, None, None, None, None, None, None, None, None]","[None, None, None, None, None, 'None', '[]', '""image""', 'None']",13,[],['super'],1
taming/models/vqgan.py:VQNoDiscModel:training_step,VQNoDiscModel:training_step,method,12,28,23,354,12.64,0,0,"['self', 'batch', 'batch_idx']","[None, None, None]","[None, None, None]",174,[],"['self.get_input', 'self', 'self.loss', 'pl.TrainResult', 'output.log', 'output.log_dict']",6
taming/models/vqgan.py:VQNoDiscModel:validation_step,VQNoDiscModel:validation_step,method,14,32,26,429,13.41,0,0,"['self', 'batch', 'batch_idx']","[None, None, None]","[None, None, None]",104,[],"['self.get_input', 'self', 'self.loss', 'pl.EvalResult', 'output.log', 'output.log_dict']",6
taming/models/vqgan.py:VQNoDiscModel:configure_optimizers,VQNoDiscModel:configure_optimizers,method,5,11,10,258,23.45,0,0,['self'],[None],[None],121,[],['list'],1
taming/models/vqgan.py:GumbelVQ:__init__,GumbelVQ:__init__,method,20,36,34,550,15.28,0,1,"['self', 'ddconfig', 'lossconfig', 'n_embed', 'embed_dim', 'temperature_scheduler_config', 'ckpt_path', 'ignore_keys', 'image_key', 'colorize_nlabels', 'monitor', 'kl_weight', 'remap', '']","[None, None, None, None, None, None, None, None, None, None, None, None, None, None]","[None, None, None, None, None, None, 'None', '[]', '""image""', 'None', 'None', '1e-8', 'None', None]",13,[],"['super', 'GumbelQuantize', 'instantiate_from_config', 'self.init_from_ckpt']",4
taming/models/vqgan.py:GumbelVQ:temperature_scheduling,GumbelVQ:temperature_scheduling,method,2,2,2,70,35.0,0,0,['self'],[None],[None],302,[],['self.temperature_scheduler'],1
taming/models/vqgan.py:GumbelVQ:encode_to_prequant,GumbelVQ:encode_to_prequant,method,4,6,4,46,7.67,0,0,"['self', 'x']","[None, None]","[None, None]",305,[],"['self.encoder', 'self.quant_conv']",2
taming/models/vqgan.py:GumbelVQ:decode_code,GumbelVQ:decode_code,method,1,2,2,24,12.0,0,0,"['self', 'code_b']","[None, None]","[None, None]",66,[],[],0
taming/models/vqgan.py:GumbelVQ:training_step,GumbelVQ:training_step,method,16,51,32,671,13.16,0,2,"['self', 'batch', 'batch_idx', 'optimizer_idx']","[None, None, None, None]","[None, None, None, None]",83,[],"['self.temperature_scheduling', 'self.get_input', 'self', 'self.loss', 'self.log_dict', 'self.log']",6
taming/models/vqgan.py:GumbelVQ:validation_step,GumbelVQ:validation_step,method,14,45,33,619,13.76,0,0,"['self', 'batch', 'batch_idx']","[None, None, None]","[None, None, None]",104,[],"['self.get_input', 'self', 'self.loss', 'self.log', 'self.log_dict']",5
taming/models/vqgan.py:GumbelVQ:log_images,GumbelVQ:log_images,method,15,23,20,215,9.35,0,0,"['self', 'batch', '**kwargs']","[None, None, None]","[None, None, None]",136,[],"['dict', 'self.get_input', 'x.to', 'self.encoder', 'self.quant_conv', 'self.quantize', 'self.decode']",7
taming/modules/util.py:count_params,count_params,function,3,8,7,70,8.75,0,0,['model'],[None],[None],5,[],"['sum', 'model.parameters']",2
taming/modules/util.py:ActNorm,ActNorm,class,46,166,110,1662,10.01,0,9,[],[],[],10,[],[],0
taming/modules/util.py:AbstractEncoder,AbstractEncoder,class,4,9,8,94,10.44,0,0,[],[],[],95,[],[],0
taming/modules/util.py:Labelator,Labelator,class,13,24,20,239,9.96,0,1,[],[],[],103,[],[],0
taming/modules/util.py:SOSProvider,SOSProvider,class,14,27,22,286,10.59,0,1,[],[],[],117,[],[],0
taming/modules/util.py:ActNorm:__init__,ActNorm:__init__,method,10,20,17,274,13.7,0,0,"['self', 'num_features', 'logdet', 'affine', 'allow_reverse_init']","[None, None, None, None, None]","[None, None, 'False', 'True', 'False']",11,[],"['super', 'nn.Parameter', 'self.register_buffer', 'torch.tensor']",4
taming/modules/util.py:ActNorm:initialize,ActNorm:initialize,method,8,34,23,319,9.38,0,0,"['self', 'input']","[None, None]","[None, None]",22,[],"['torch.no_grad', 'input.permute', 'flatten.mean', 'flatten.std']",4
taming/modules/util.py:ActNorm:forward,ActNorm:forward,method,25,48,36,481,10.02,0,5,"['self', 'input', 'reverse']","[None, None, None]","[None, None, 'False']",43,[],"['self.reverse', 'len', 'self.initialize', 'h.squeeze', 'torch.log', 'torch.ones']",6
taming/modules/util.py:ActNorm:reverse,ActNorm:reverse,method,15,48,40,412,8.58,0,4,"['self', 'output']","[None, None]","[None, None]",71,[],"['RuntimeError', 'self.initialize', 'len', 'h.squeeze']",4
taming/modules/util.py:AbstractEncoder:__init__,AbstractEncoder:__init__,method,1,1,1,18,18.0,0,0,['self'],[None],[None],96,[],['super'],1
taming/modules/util.py:AbstractEncoder:encode,AbstractEncoder:encode,method,1,2,2,24,12.0,0,0,"['self', '*args', '**kwargs']","[None, None, None]","[None, None, None]",99,[],[],0
taming/modules/util.py:Labelator:__init__,Labelator:__init__,method,5,5,5,86,17.2,0,0,"['self', 'n_classes', 'quantize_interface']","[None, None, None]","[None, None, 'True']",105,[],['super'],1
taming/modules/util.py:Labelator:encode,Labelator:encode,method,7,12,9,80,6.67,0,1,"['self', 'c']","[None, None]","[None, None]",110,[],['c.long'],1
taming/modules/util.py:SOSProvider:__init__,SOSProvider:__init__,method,5,5,5,86,17.2,0,0,"['self', 'sos_token', 'quantize_interface']","[None, None, None]","[None, None, 'True']",119,[],['super'],1
taming/modules/util.py:SOSProvider:encode,SOSProvider:encode,method,8,15,11,127,8.47,0,1,"['self', 'x']","[None, None]","[None, None]",124,[],"['torch.ones', 'c.long']",2
taming/modules/diffusionmodules/model.py:get_timestep_embedding,get_timestep_embedding,function,12,39,32,365,9.36,0,1,"['timesteps', 'embedding_dim']","[None, None]","[None, None]",8,"['    """"""\n', '    This matches the implementation in Denoising Diffusion Probabilistic Models:\n', '    From Fairseq.\n', '    Build sinusoidal embeddings.\n', '    This matches the implementation in tensor2tensor, but differs slightly\n', '    from the description in Section 3.5 of ""Attention Is All You Need"".\n', '    """"""\n']","['len', 'math.log', 'torch.exp', 'emb.to', 'timesteps.float', 'torch.cat', 'torch.cos']",7
taming/modules/diffusionmodules/model.py:nonlinearity,nonlinearity,function,2,2,2,24,12.0,0,0,['x'],[None],[None],29,[],[],0
taming/modules/diffusionmodules/model.py:Normalize,Normalize,function,2,5,5,85,17.0,0,0,['in_channels'],[None],[None],34,[],[],0
taming/modules/diffusionmodules/model.py:Upsample,Upsample,class,10,28,23,320,11.43,0,2,[],[],[],38,[],[],0
taming/modules/diffusionmodules/model.py:Downsample,Downsample,class,12,36,30,387,10.75,0,2,[],[],[],56,[],[],0
taming/modules/diffusionmodules/model.py:ResnetBlock,ResnetBlock,class,28,106,68,1254,11.83,0,7,[],[],[],78,[],[],0
taming/modules/diffusionmodules/model.py:AttnBlock,AttnBlock,class,30,100,65,942,9.42,0,0,[],[],[],140,[],[],0
taming/modules/diffusionmodules/model.py:Model,Model,class,84,264,153,3284,12.44,8,11,[],[],[],195,[],[],0
taming/modules/diffusionmodules/model.py:Encoder,Encoder,class,64,157,111,2022,12.88,4,4,[],[],[],342,[],[],0
taming/modules/diffusionmodules/model.py:Decoder,Decoder,class,68,179,130,2197,12.27,4,5,[],[],[],436,[],[],0
taming/modules/diffusionmodules/model.py:VUNet,VUNet,class,87,275,163,3415,12.42,8,11,[],[],[],540,[],[],0
taming/modules/diffusionmodules/model.py:SimpleDecoder,SimpleDecoder,class,17,67,49,805,12.01,1,1,[],[],[],694,[],[],0
taming/modules/diffusionmodules/model.py:UpsampleDecoder,UpsampleDecoder,class,36,99,72,1180,11.92,4,2,[],[],[],730,[],[],0
taming/modules/diffusionmodules/model.py:Upsample:__init__,Upsample:__init__,method,5,11,11,149,13.55,0,1,"['self', 'in_channels', 'with_conv']","[None, None, None]","[None, None, None]",39,[],['super'],1
taming/modules/diffusionmodules/model.py:Upsample:forward,Upsample:forward,method,5,10,8,109,10.9,0,1,"['self', 'x']","[None, None]","[None, None]",49,[],['self.conv'],1
taming/modules/diffusionmodules/model.py:Downsample:__init__,Downsample:__init__,method,5,11,11,149,13.55,0,1,"['self', 'in_channels', 'with_conv']","[None, None, None]","[None, None, None]",39,[],['super'],1
taming/modules/diffusionmodules/model.py:Downsample:forward,Downsample:forward,method,7,18,15,176,9.78,0,1,"['self', 'x']","[None, None]","[None, None]",49,[],['self.conv'],1
taming/modules/diffusionmodules/model.py:ResnetBlock:__init__,ResnetBlock:__init__,method,20,58,40,810,13.97,0,4,"['self', '*', 'in_channels', 'out_channels', 'conv_shortcut', 'dropout', 'temb_channels']","[None, None, None, None, None, None, None]","[None, None, None, 'None', 'False', None, '512']",79,[],"['super', 'Normalize']",2
taming/modules/diffusionmodules/model.py:ResnetBlock:forward,ResnetBlock:forward,method,17,36,23,320,8.89,0,3,"['self', 'x', 'temb']","[None, None, None]","[None, None, None]",117,[],"['self.norm1', 'nonlinearity', 'self.conv1', 'self.temb_proj', 'self.norm2', 'self.dropout', 'self.conv2', 'self.conv_shortcut', 'self.nin_shortcut']",9
taming/modules/diffusionmodules/model.py:AttnBlock:__init__,AttnBlock:__init__,method,10,29,15,427,14.72,0,0,"['self', 'in_channels']","[None, None]","[None, None]",141,[],"['super', 'Normalize']",2
taming/modules/diffusionmodules/model.py:AttnBlock:forward,AttnBlock:forward,method,23,65,46,463,7.12,0,0,"['self', 'x']","[None, None]","[None, None]",49,[],"['self.norm', 'self.q', 'self.k', 'self.v', 'q.reshape', 'q.permute', 'k.reshape', 'torch.bmm', 'v.reshape', 'w_.permute', 'h_.reshape', 'self.proj_out']",12
taming/modules/diffusionmodules/model.py:Model:__init__,Model:__init__,method,60,161,101,2177,13.52,4,6,"['self', '*', 'ch', 'out_ch', 'ch_mult', '2', '4', '8)', 'num_res_blocks', 'attn_resolutions', 'dropout', 'resamp_with_conv', 'in_channels', 'resolution', 'use_timestep']","[None, None, None, None, None, None, None, None, None, None, None, None, None, None, None]","[None, None, None, None, '(1', None, None, None, None, None, '0.0', 'True', None, None, 'True']",196,[],"['super', 'len', 'nn.Module', 'nn.ModuleList', 'range', 'block.append', 'attn.append', 'Downsample', 'ResnetBlock', 'AttnBlock', 'reversed', 'Upsample', 'Normalize']",13
taming/modules/diffusionmodules/model.py:Model:forward,Model:forward,method,27,86,49,921,10.71,4,5,"['self', 'x', 't']","[None, None, None]","[None, None, 'None']",295,[],"['get_timestep_embedding', 'nonlinearity', 'range', 'len', 'hs.append', 'reversed', 'torch.cat', 'hs.pop', 'self.norm_out', 'self.conv_out']",10
taming/modules/diffusionmodules/model.py:Encoder:__init__,Encoder:__init__,method,46,95,74,1343,14.14,2,2,"['self', '*', 'ch', 'out_ch', 'ch_mult', '2', '4', '8)', 'num_res_blocks', 'attn_resolutions', 'dropout', 'resamp_with_conv', 'in_channels', 'resolution', 'z_channels', 'double_z', '**ignore_kwargs']","[None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None]","[None, None, None, None, '(1', None, None, None, None, None, '0.0', 'True', None, None, None, 'True', None]",196,[],"['super', 'len', 'nn.ModuleList', 'range', 'block.append', 'attn.append', 'nn.Module', 'Downsample', 'ResnetBlock', 'AttnBlock', 'Normalize']",11
taming/modules/diffusionmodules/model.py:Encoder:forward,Encoder:forward,method,18,44,29,477,10.84,2,2,"['self', 'x']","[None, None]","[None, None]",49,[],"['range', 'len', 'hs.append', 'self.norm_out', 'nonlinearity', 'self.conv_out']",6
taming/modules/diffusionmodules/model.py:Decoder:__init__,Decoder:__init__,method,50,113,93,1522,13.47,2,2,"['self', '*', 'ch', 'out_ch', 'ch_mult', '2', '4', '8)', 'num_res_blocks', 'attn_resolutions', 'dropout', 'resamp_with_conv', 'in_channels', 'resolution', 'z_channels', 'give_pre_end', '**ignorekwargs']","[None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None]","[None, None, None, None, '(1', None, None, None, None, None, '0.0', 'True', None, None, None, 'False', None]",196,[],"['super', 'len', 'print', 'np.prod', 'nn.Module', 'ResnetBlock', 'AttnBlock', 'nn.ModuleList', 'reversed', 'range', 'block.append', 'attn.append', 'Upsample', 'Normalize']",14
taming/modules/diffusionmodules/model.py:Decoder:forward,Decoder:forward,method,20,48,29,469,9.77,2,3,"['self', 'z']","[None, None]","[None, None]",506,[],"['self.conv_in', 'reversed', 'range', 'len', 'self.norm_out', 'nonlinearity', 'self.conv_out']",7
taming/modules/diffusionmodules/model.py:VUNet:__init__,VUNet:__init__,method,59,165,105,2233,13.53,4,6,"['self', '*', 'ch', 'out_ch', 'ch_mult', '2', '4', '8)', 'num_res_blocks', 'attn_resolutions', 'dropout', 'resamp_with_conv', 'in_channels', 'c_channels', 'resolution', 'z_channels', 'use_timestep', '**ignore_kwargs']","[None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None]","[None, None, None, None, '(1', None, None, None, None, None, '0.0', 'True', None, None, None, None, 'False', None]",196,[],"['super', 'len', 'nn.Module', 'nn.ModuleList', 'range', 'block.append', 'attn.append', 'Downsample', 'ResnetBlock', 'AttnBlock', 'reversed', 'Upsample', 'Normalize']",13
taming/modules/diffusionmodules/model.py:VUNet:forward,VUNet:forward,method,30,90,52,961,10.68,4,5,"['self', 'x', 'z']","[None, None, None]","[None, None, None]",645,[],"['get_timestep_embedding', 'nonlinearity', 'range', 'len', 'hs.append', 'self.z_in', 'torch.cat', 'reversed', 'hs.pop', 'self.norm_out', 'self.conv_out']",11
taming/modules/diffusionmodules/model.py:SimpleDecoder:__init__,SimpleDecoder:__init__,method,7,35,24,580,16.57,0,0,"['self', 'in_channels', 'out_channels', '*args', '**kwargs']","[None, None, None, None, None]","[None, None, None, None, None]",695,[],"['super', 'nn.ModuleList', 'ResnetBlock', 'nn.Conv2d', 'Upsample', 'Normalize']",6
taming/modules/diffusionmodules/model.py:SimpleDecoder:forward,SimpleDecoder:forward,method,10,23,18,145,6.3,1,1,"['self', 'x']","[None, None]","[None, None]",49,[],"['enumerate', 'layer', 'self.norm_out', 'nonlinearity', 'self.conv_out']",5
taming/modules/diffusionmodules/model.py:UpsampleDecoder:__init__,UpsampleDecoder:__init__,method,26,58,50,788,13.59,2,1,"['self', 'in_channels', 'out_channels', 'ch', 'num_res_blocks', 'resolution', '2', '2)', 'dropout=0.0)']","[None, None, None, None, None, None, None, None, '']","[None, None, None, None, None, None, None, None, '0.0):']",731,[],"['super', 'len', 'nn.ModuleList', 'range', 'res_block.append', 'Normalize']",6
taming/modules/diffusionmodules/model.py:UpsampleDecoder:forward,UpsampleDecoder:forward,method,14,29,20,271,9.34,2,1,"['self', 'x']","[None, None]","[None, None]",49,[],"['enumerate', 'range', 'self.norm_out', 'nonlinearity', 'self.conv_out']",5
taming/modules/discriminator/model.py:weights_init,weights_init,function,5,16,13,210,13.12,0,1,['m'],[None],[None],8,[],['classname.find'],1
taming/modules/discriminator/model.py:NLayerDiscriminator,NLayerDiscriminator,class,21,133,87,1122,8.44,1,2,[],[],[],17,[],[],0
taming/modules/discriminator/model.py:NLayerDiscriminator:__init__,NLayerDiscriminator:__init__,method,18,122,77,1009,8.27,1,2,"['self', 'input_nc', 'ndf', 'n_layers', 'use_actnorm']","[None, None, None, None, None]","[None, '3', '64', '3', 'False']",21,"['        """"""Construct a PatchGAN discriminator\n', '        Parameters:\n', '            input_nc (int)  -- the number of channels in input images\n', '            ndf (int)       -- the number of filters in the last conv layer\n', '            n_layers (int)  -- the number of conv layers in the discriminator\n', '            norm_layer      -- normalization layer\n', '        """"""\n']","['super', 'type', 'nn.LeakyReLU', 'range', 'min', 'nn.Conv2d', 'norm_layer', 'nn.Sequential']",8
taming/modules/discriminator/model.py:NLayerDiscriminator:forward,NLayerDiscriminator:forward,method,2,2,2,22,11.0,0,0,"['self', 'input']","[None, None]","[None, None]",65,"['        """"""Standard forward.""""""\n']",['self.main'],1
taming/modules/losses/lpips.py:normalize_tensor,normalize_tensor,function,4,4,4,84,21.0,0,0,"['x', 'eps']","[None, None]","[None, '1e-10']",116,[],['torch.sqrt'],1
taming/modules/losses/lpips.py:spatial_average,spatial_average,function,2,2,2,35,17.5,0,0,"['x', 'keepdim']","[None, None]","[None, 'True']",121,[],['x.mean'],1
taming/modules/losses/lpips.py:LPIPS,LPIPS,class,55,131,109,1596,12.18,3,1,[],[],[],11,[],[],0
taming/modules/losses/lpips.py:ScalingLayer,ScalingLayer,class,5,25,21,271,10.84,0,0,[],[],[],57,[],[],0
taming/modules/losses/lpips.py:NetLinLayer,NetLinLayer,class,5,25,24,235,9.4,0,0,[],[],[],67,[],[],0
taming/modules/losses/lpips.py:vgg16,vgg16,class,33,101,69,1127,11.16,6,1,[],[],[],76,[],[],0
taming/modules/losses/lpips.py:LPIPS:__init__,LPIPS:__init__,method,18,37,33,531,14.35,1,0,"['self', 'use_dropout']","[None, None]","[None, 'True']",13,[],"['super', 'ScalingLayer', 'vgg16', 'NetLinLayer', 'self.load_from_pretrained', 'self.parameters']",6
taming/modules/losses/lpips.py:LPIPS:load_from_pretrained,LPIPS:load_from_pretrained,method,4,12,12,198,16.5,0,0,"['self', 'name']","[None, None]","[None, '""vgg_lpips""']",27,[],"['get_ckpt_path', 'self.load_state_dict', 'print']",3
taming/modules/losses/lpips.py:LPIPS:from_pretrained,LPIPS:from_pretrained,method,8,16,15,183,11.44,0,1,"['cls', 'name']","[None, None]","[None, '""vgg_lpips""']",33,[],"['cls', 'get_ckpt_path', 'model.load_state_dict']",3
taming/modules/losses/lpips.py:LPIPS:forward,LPIPS:forward,method,25,52,44,511,9.83,2,0,"['self', 'input', 'target']","[None, None, None]","[None, None, None]",41,[],"['self.scaling_layer', 'self.net', 'range', 'normalize_tensor', 'len']",5
taming/modules/losses/lpips.py:ScalingLayer:__init__,ScalingLayer:__init__,method,2,16,14,196,12.25,0,0,['self'],[None],[None],58,[],"['super', 'self.register_buffer', 'torch.Tensor']",3
taming/modules/losses/lpips.py:ScalingLayer:forward,ScalingLayer:forward,method,1,4,4,33,8.25,0,0,"['self', 'inp']","[None, None]","[None, None]",63,[],[],0
taming/modules/losses/lpips.py:NetLinLayer:__init__,NetLinLayer:__init__,method,4,20,19,181,9.05,0,0,"['self', 'chn_in', 'chn_out', 'use_dropout']","[None, None, None, None]","[None, None, '1', 'False']",69,[],"['super', 'nn.Sequential']",2
taming/modules/losses/lpips.py:vgg16:__init__,vgg16:__init__,method,21,59,37,741,12.56,6,1,"['self', 'requires_grad', 'pretrained']","[None, None, None]","[None, 'False', 'True']",77,[],"['super', 'models.vgg16', 'range', 'self.parameters']",4
taming/modules/losses/lpips.py:vgg16:forward,vgg16:forward,method,15,35,26,310,8.86,0,0,"['self', 'X']","[None, None]","[None, None]",100,[],"['self.slice1', 'self.slice2', 'self.slice3', 'self.slice4', 'self.slice5', 'namedtuple', 'vgg_outputs']",7
taming/modules/losses/segmentation.py:BCELoss,BCELoss,class,5,9,9,108,12.0,0,0,[],[],[],5,[],[],0
taming/modules/losses/segmentation.py:BCELossWithQuant,BCELossWithQuant,class,11,26,24,425,16.35,0,0,[],[],[],11,[],[],0
taming/modules/losses/segmentation.py:BCELoss:forward,BCELoss:forward,method,4,5,5,72,14.4,0,0,"['self', 'prediction', 'target']","[None, None, None]","[None, None, None]",6,[],['F.binary_cross_entropy_with_logits'],1
taming/modules/losses/segmentation.py:BCELossWithQuant:__init__,BCELossWithQuant:__init__,method,3,3,3,55,18.33,0,0,"['self', 'codebook_weight']","[None, None]","[None, '1.']",12,[],['super'],1
taming/modules/losses/segmentation.py:BCELossWithQuant:forward,BCELossWithQuant:forward,method,6,14,13,283,20.21,0,0,"['self', 'qloss', 'target', 'prediction', 'split']","[None, None, None, None, None]","[None, None, None, None, None]",16,[],"['F.binary_cross_entropy_with_logits', 'loss.clone', 'bce_loss.detach', 'qloss.detach']",4
taming/modules/losses/vqperceptual.py:adopt_weight,adopt_weight,function,5,8,7,50,6.25,0,1,"['weight', 'global_step', 'threshold', 'value']","[None, None, None, None]","[None, None, '0', '0.']",14,[],[],0
taming/modules/losses/vqperceptual.py:hinge_d_loss,hinge_d_loss,function,5,12,10,135,11.25,0,0,"['logits_real', 'logits_fake']","[None, None]","[None, None]",20,[],['torch.mean'],1
taming/modules/losses/vqperceptual.py:vanilla_d_loss,vanilla_d_loss,function,3,8,7,136,17.0,0,0,"['logits_real', 'logits_fake']","[None, None]","[None, None]",27,[],['torch.mean'],1
taming/modules/losses/vqperceptual.py:DummyLoss,DummyLoss,class,2,3,3,37,12.33,0,0,[],[],[],9,[],[],0
taming/modules/losses/vqperceptual.py:VQLPIPSWithDiscriminator,VQLPIPSWithDiscriminator,class,57,238,163,3720,15.63,0,7,[],[],[],34,[],[],0
taming/modules/losses/vqperceptual.py:DummyLoss:__init__,DummyLoss:__init__,method,1,1,1,18,18.0,0,0,['self'],[None],[None],10,[],['super'],1
taming/modules/losses/vqperceptual.py:VQLPIPSWithDiscriminator:__init__,VQLPIPSWithDiscriminator:__init__,method,25,49,46,726,14.82,0,1,"['self', 'disc_start', 'codebook_weight', 'pixelloss_weight', 'disc_num_layers', 'disc_in_channels', 'disc_factor', 'disc_weight', 'perceptual_weight', 'use_actnorm', 'disc_conditional', 'disc_ndf', 'disc_loss']","[None, None, None, None, None, None, None, None, None, None, None, None, None]","[None, None, '1.0', '1.0', '3', '3', '1.0', '1.0', '1.0', 'False', 'False', '64', '""hinge""']",35,[],"['super', 'LPIPS', 'NLayerDiscriminator', 'ValueError', 'print']",5
taming/modules/losses/vqperceptual.py:VQLPIPSWithDiscriminator:calculate_adaptive_weight,VQLPIPSWithDiscriminator:calculate_adaptive_weight,method,9,35,22,489,13.97,0,1,"['self', 'nll_loss', 'g_loss', 'last_layer']","[None, None, None, None]","[None, None, None, 'None']",63,[],"['torch.norm', 'torch.clamp']",2
taming/modules/losses/vqperceptual.py:VQLPIPSWithDiscriminator:forward,VQLPIPSWithDiscriminator:forward,method,30,125,82,2077,16.62,0,5,"['self', 'codebook_loss', 'inputs', 'reconstructions', 'optimizer_idx', 'global_step', 'last_layer', 'cond', 'split']","[None, None, None, None, None, None, None, None, None]","[None, None, None, None, None, None, 'None', 'None', '""train""']",76,[],"['torch.abs', 'reconstructions.contiguous', 'self.perceptual_loss', 'torch.tensor', 'torch.mean', 'self.discriminator', 'self.calculate_adaptive_weight', 'adopt_weight', 'codebook_loss.mean', 'loss.clone', 'codebook_loss.detach', 'nll_loss.detach', 'rec_loss.detach', 'p_loss.detach', 'd_weight.detach', 'g_loss.detach', 'self.disc_loss', 'd_loss.clone', 'logits_real.detach', 'logits_fake.detach']",20
taming/modules/misc/coord.py:CoordStage,CoordStage,class,25,58,43,539,9.29,0,0,[],[],[],3,[],[],0
taming/modules/misc/coord.py:CoordStage:__init__,CoordStage:__init__,method,4,4,4,49,12.25,0,0,"['self', 'n_embed', 'down_factor']","[None, None, None]","[None, None, None]",4,[],[],0
taming/modules/misc/coord.py:CoordStage:eval,CoordStage:eval,method,1,2,2,10,5.0,0,0,['self'],[None],[None],8,[],[],0
taming/modules/misc/coord.py:CoordStage:encode,CoordStage:encode,method,16,32,26,278,8.69,0,0,"['self', 'c']","[None, None]","[None, None]",11,"['        """"""fake vqmodel interface""""""\n']","['c.min', 'c.max', 'c.clamp', 'c.round', 'c_quant.to']",5
taming/modules/misc/coord.py:CoordStage:decode,CoordStage:decode,method,4,8,6,107,13.38,0,0,"['self', 'c']","[None, None]","[None, None]",27,[],[],0
taming/modules/transformer/mingpt.py:top_k_logits,top_k_logits,function,8,13,12,87,6.69,0,0,"['logits', 'k']","[None, None]","[None, None]",286,[],"['torch.topk', 'logits.clone']",2
taming/modules/transformer/mingpt.py:sample,sample,function,25,56,47,405,7.23,1,3,"['model', 'x', 'steps', 'temperature', 'sample', 'top_k']","[None, None, None, None, None, None]","[None, None, None, '1.0', 'False', 'None']",293,"['    """"""\n', '    take a conditioning sequence of indices in x (of shape (b,t)) and predict the next token in\n', '    the sequence, feeding the predictions back into the model each time. Clearly the sampling\n', '    has quadratic complexity unlike an RNN that is only linear, and has a finite context window\n', '    of block_size, unlike an RNN that has an infinite context window.\n', '    """"""\n']","['model.get_block_size', 'model.eval', 'range', 'x.size', 'model', 'top_k_logits', 'F.softmax', 'torch.multinomial', 'torch.topk', 'torch.cat']",10
taming/modules/transformer/mingpt.py:sample_with_past,sample_with_past,function,31,74,55,574,7.76,1,4,"['x', 'model', 'steps', 'temperature', 'sample_logits', 'top_k', 'top_p', 'callback']","[None, None, None, None, None, None, None, None]","[None, None, None, '1.', 'True', 'None', 'None', 'None']",324,[],"['range', 'callback', 'model.forward_with_past', 'past.append', 'top_k_top_p_filtering', 'F.softmax', 'torch.topk', 'torch.multinomial', 'torch.cat']",9
taming/modules/transformer/mingpt.py:GPTConfig,GPTConfig,class,11,22,20,191,8.68,1,0,[],[],[],22,[],[],0
taming/modules/transformer/mingpt.py:GPT1Config,GPT1Config,class,3,6,5,31,5.17,0,0,[],[],[],35,[],[],0
taming/modules/transformer/mingpt.py:CausalSelfAttention,CausalSelfAttention,class,47,173,108,1453,8.4,0,3,[],[],[],42,[],[],0
taming/modules/transformer/mingpt.py:Block,Block,class,23,55,44,567,10.31,0,2,[],[],[],98,[],[],0
taming/modules/transformer/mingpt.py:GPT,GPT,class,75,300,180,3031,10.1,1,7,[],[],[],125,[],[],0
taming/modules/transformer/mingpt.py:DummyGPT,DummyGPT,class,8,13,12,126,9.69,0,0,[],[],[],215,[],[],0
taming/modules/transformer/mingpt.py:CodeGPT,CodeGPT,class,51,160,128,1835,11.47,0,4,[],[],[],225,[],[],0
taming/modules/transformer/mingpt.py:KMeans,KMeans,class,45,132,105,1173,8.89,1,2,[],[],[],356,[],[],0
taming/modules/transformer/mingpt.py:GPTConfig:__init__,GPTConfig:__init__,method,7,11,11,95,8.64,1,0,"['self', 'vocab_size', 'block_size', '**kwargs']","[None, None, None, None]","[None, None, None, None]",28,[],"['kwargs.items', 'setattr']",2
taming/modules/transformer/mingpt.py:CausalSelfAttention:__init__,CausalSelfAttention:__init__,method,18,38,30,594,15.63,0,1,"['self', 'config']","[None, None]","[None, None]",49,[],"['super', 'nn.Linear', 'nn.Dropout', 'torch.tril', 'hasattr', 'self.register_buffer', 'mask.view']",7
taming/modules/transformer/mingpt.py:CausalSelfAttention:forward,CausalSelfAttention:forward,method,32,128,72,796,6.22,0,2,"['self', 'x', 'layer_past']","[None, None, None]","[None, None, 'None']",69,[],"['x.size', 'self.key', 'self.query', 'self.value', 'torch.stack', 'torch.cat', 'k.transpose', 'math.sqrt', 'att.masked_fill', 'float', 'F.softmax', 'self.attn_drop', 'y.transpose', 'self.resid_drop']",14
taming/modules/transformer/mingpt.py:Block:__init__,Block:__init__,method,11,20,18,288,14.4,0,0,"['self', 'config']","[None, None]","[None, None]",49,[],"['super', 'nn.LayerNorm', 'CausalSelfAttention', 'nn.Sequential', 'nn.Linear', 'nn.GELU', 'nn.Dropout']",7
taming/modules/transformer/mingpt.py:Block:forward,Block:forward,method,12,27,20,195,7.22,0,2,"['self', 'x', 'layer_past', 'return_present']","[None, None, None, None]","[None, None, 'None', 'False']",112,[],"['self.attn', 'self.mlp']",2
taming/modules/transformer/mingpt.py:GPT:__init__,GPT:__init__,method,20,46,44,722,15.7,0,0,"['self', 'vocab_size', 'block_size', 'n_layer', 'n_head', 'n_embd', 'embd_pdrop', 'resid_pdrop', 'attn_pdrop', 'n_unmasked']","[None, None, None, None, None, None, None, None, None, None]","[None, None, None, '12', '8', '256', '0.', '0.', '0.', '0']",127,[],"['super', 'GPTConfig', 'nn.Embedding', 'nn.Parameter', 'nn.Dropout', 'nn.Sequential', 'range', 'nn.LayerNorm', 'nn.Linear', 'self.apply', 'logger.info', 'sum', 'self.parameters']",13
taming/modules/transformer/mingpt.py:GPT:get_block_size,GPT:get_block_size,method,2,2,2,21,10.5,0,0,['self'],[None],[None],148,[],[],0
taming/modules/transformer/mingpt.py:GPT:_init_weights,GPT:_init_weights,method,4,20,16,264,13.2,0,2,"['self', 'module']","[None, None]","[None, None]",151,[],['isinstance'],1
taming/modules/transformer/mingpt.py:GPT:forward,GPT:forward,method,52,211,110,1785,8.46,1,5,"['self', 'idx', 'embeddings', 'targets']","[None, None, None, None]","[None, None, 'None', 'None']",160,[],"['self.tok_emb', 'torch.cat', 'self.drop', 'self.blocks', 'self.ln_f', 'self.head', 'F.cross_entropy', 'logits.size', 'targets.view', 'forward_with_past', 'list', 'enumerate', 'block', 'presents.append', 'torch.stack']",15
taming/modules/transformer/mingpt.py:GPT:forward_with_past,GPT:forward_with_past,method,46,134,92,1120,8.36,1,3,"['self', 'idx', 'embeddings', 'targets', 'past', 'past_length']","[None, None, None, None, None, None]","[None, None, 'None', 'None', 'None', 'None']",182,[],"['self.tok_emb', 'torch.cat', 'list', 'self.drop', 'enumerate', 'block', 'presents.append', 'self.ln_f', 'self.head', 'F.cross_entropy', 'logits.size', 'targets.view', 'torch.stack']",13
taming/modules/transformer/mingpt.py:DummyGPT:__init__,DummyGPT:__init__,method,3,3,3,43,14.33,0,0,"['self', 'add_value']","[None, None]","[None, '1']",217,[],['super'],1
taming/modules/transformer/mingpt.py:DummyGPT:forward,DummyGPT:forward,method,3,4,4,29,7.25,0,0,"['self', 'idx']","[None, None]","[None, None]",221,[],[],0
taming/modules/transformer/mingpt.py:CodeGPT:__init__,CodeGPT:__init__,method,19,46,44,713,15.5,0,0,"['self', 'vocab_size', 'block_size', 'in_channels', 'n_layer', 'n_head', 'n_embd', 'embd_pdrop', 'resid_pdrop', 'attn_pdrop', 'n_unmasked']","[None, None, None, None, None, None, None, None, None, None, None]","[None, None, None, None, '12', '8', '256', '0.', '0.', '0.', '0']",227,[],"['super', 'GPTConfig', 'nn.Linear', 'nn.Parameter', 'nn.Dropout', 'nn.Sequential', 'range', 'nn.LayerNorm', 'self.apply', 'logger.info', 'sum', 'self.parameters']",12
taming/modules/transformer/mingpt.py:CodeGPT:get_block_size,CodeGPT:get_block_size,method,2,2,2,21,10.5,0,0,['self'],[None],[None],148,[],[],0
taming/modules/transformer/mingpt.py:CodeGPT:_init_weights,CodeGPT:_init_weights,method,4,20,16,264,13.2,0,2,"['self', 'module']","[None, None]","[None, None]",151,[],['isinstance'],1
taming/modules/transformer/mingpt.py:CodeGPT:forward,CodeGPT:forward,method,28,70,52,586,8.37,0,2,"['self', 'idx', 'embeddings', 'targets']","[None, None, None, None]","[None, None, 'None', 'None']",160,[],"['self.tok_emb', 'torch.cat', 'self.drop', 'self.blocks', 'self.taming_cinln_f', 'self.head', 'F.cross_entropy', 'logits.size', 'targets.view']",9
taming/modules/transformer/mingpt.py:KMeans:__init__,KMeans:__init__,method,9,14,14,216,15.43,0,0,"['self', 'ncluster', 'nc', 'niter']","[None, None, None, None]","[None, '512', '3', '10']",357,[],"['super', 'self.register_buffer', 'torch.zeros', 'torch.tensor']",4
taming/modules/transformer/mingpt.py:KMeans:is_initialized,KMeans:is_initialized,method,2,3,3,32,10.67,0,0,['self'],[None],[None],366,[],[],0
taming/modules/transformer/mingpt.py:KMeans:initialize,KMeans:initialize,method,18,55,49,463,8.42,1,0,"['self', 'x']","[None, None]","[None, None]",370,[],"['range', 'torch.stack', 'torch.any', 'nanix.sum', 'print']",5
taming/modules/transformer/mingpt.py:KMeans:forward,KMeans:forward,method,18,44,32,303,6.89,0,2,"['self', 'x', 'reverse', 'shape']","[None, None, None, None]","[None, None, 'False', 'None']",389,[],"['x.reshape', 'C.reshape', 'x.permute']",3
taming/modules/transformer/permuter.py:mortonify,mortonify,function,8,31,22,186,6.0,1,0,"['i', 'j']","[None, None]","[None, None]",47,"['    """"""(i,j) index to linear morton code""""""\n']","['np.uint64', 'np.uint', 'range']",3
taming/modules/transformer/permuter.py:AbstractPermuter,AbstractPermuter,class,4,11,10,111,10.09,0,0,[],[],[],6,[],[],0
taming/modules/transformer/permuter.py:Identity,Identity,class,5,9,8,79,8.78,0,0,[],[],[],13,[],[],0
taming/modules/transformer/permuter.py:Subsample,Subsample,class,22,53,42,594,11.21,1,1,[],[],[],21,[],[],0
taming/modules/transformer/permuter.py:ZCurve,ZCurve,class,13,39,32,425,10.9,0,1,[],[],[],62,[],[],0
taming/modules/transformer/permuter.py:SpiralOut,SpiralOut,class,28,117,58,825,7.05,6,2,[],[],[],81,[],[],0
taming/modules/transformer/permuter.py:SpiralIn,SpiralIn,class,29,119,59,839,7.05,6,2,[],[],[],141,[],[],0
taming/modules/transformer/permuter.py:Random,Random,class,13,27,24,364,13.48,0,1,[],[],[],202,[],[],0
taming/modules/transformer/permuter.py:AlternateParsing,AlternateParsing,class,17,42,36,429,10.21,1,1,[],[],[],217,[],[],0
taming/modules/transformer/permuter.py:AbstractPermuter:__init__,AbstractPermuter:__init__,method,1,1,1,18,18.0,0,0,"['self', '*args', '**kwargs']","[None, None, None]","[None, None, None]",7,[],['super'],1
taming/modules/transformer/permuter.py:AbstractPermuter:forward,AbstractPermuter:forward,method,1,2,2,24,12.0,0,0,"['self', 'x', 'reverse']","[None, None, None]","[None, None, 'False']",9,[],[],0
taming/modules/transformer/permuter.py:Identity:__init__,Identity:__init__,method,1,1,1,18,18.0,0,0,['self'],[None],[None],14,[],['super'],1
taming/modules/transformer/permuter.py:Identity:forward,Identity:forward,method,2,2,2,7,3.5,0,0,"['self', 'x', 'reverse']","[None, None, None]","[None, None, 'False']",9,[],[],0
taming/modules/transformer/permuter.py:Subsample:__init__,Subsample:__init__,method,15,35,28,444,12.69,1,0,"['self', 'H', 'W']","[None, None, None]","[None, None, None]",22,[],"['super', 'np.arange', 'min', 'indices.reshape', 'indices.transpose', 'torch.tensor', 'self.register_buffer', 'nn.Parameter']",8
taming/modules/transformer/permuter.py:Subsample:forward,Subsample:forward,method,5,10,8,92,9.2,0,1,"['self', 'x', 'reverse']","[None, None, None]","[None, None, 'False']",9,[],[],0
taming/modules/transformer/permuter.py:ZCurve:__init__,ZCurve:__init__,method,6,21,17,275,13.1,0,0,"['self', 'H', 'W']","[None, None, None]","[None, None, None]",22,[],"['super', 'range', 'np.argsort', 'torch.tensor', 'self.register_buffer']",5
taming/modules/transformer/permuter.py:ZCurve:forward,ZCurve:forward,method,5,10,8,92,9.2,0,1,"['self', 'x', 'reverse']","[None, None, None]","[None, None, 'False']",9,[],[],0
taming/modules/transformer/permuter.py:SpiralOut:__init__,SpiralOut:__init__,method,21,99,45,675,6.82,6,1,"['self', 'H', 'W']","[None, None, None]","[None, None, None]",22,[],"['super', 'np.arange', 'range', 'idx.append', 'len', 'torch.tensor', 'self.register_buffer', 'torch.argsort']",8
taming/modules/transformer/permuter.py:SpiralOut:forward,SpiralOut:forward,method,5,10,8,92,9.2,0,1,"['self', 'x', 'reverse']","[None, None, None]","[None, None, 'False']",9,[],[],0
taming/modules/transformer/permuter.py:SpiralIn:__init__,SpiralIn:__init__,method,22,101,46,689,6.82,6,1,"['self', 'H', 'W']","[None, None, None]","[None, None, None]",22,[],"['super', 'np.arange', 'range', 'idx.append', 'len', 'torch.tensor', 'self.register_buffer', 'torch.argsort']",8
taming/modules/transformer/permuter.py:SpiralIn:forward,SpiralIn:forward,method,5,10,8,92,9.2,0,1,"['self', 'x', 'reverse']","[None, None, None]","[None, None, 'False']",9,[],[],0
taming/modules/transformer/permuter.py:Random:__init__,Random:__init__,method,6,9,9,214,23.78,0,0,"['self', 'H', 'W']","[None, None, None]","[None, None, None]",22,[],"['super', 'torch.tensor', 'self.register_buffer', 'torch.argsort']",4
taming/modules/transformer/permuter.py:Random:forward,Random:forward,method,5,10,8,92,9.2,0,1,"['self', 'x', 'reverse']","[None, None, None]","[None, None, 'False']",9,[],[],0
taming/modules/transformer/permuter.py:AlternateParsing:__init__,AlternateParsing:__init__,method,10,24,22,279,11.62,1,0,"['self', 'H', 'W']","[None, None, None]","[None, None, None]",22,[],"['super', 'np.arange', 'range', 'indices.flatten', 'len', 'torch.tensor', 'self.register_buffer', 'torch.argsort']",8
taming/modules/transformer/permuter.py:AlternateParsing:forward,AlternateParsing:forward,method,5,10,8,92,9.2,0,1,"['self', 'x', 'reverse']","[None, None, None]","[None, None, 'False']",9,[],[],0
taming/modules/vqvae/quantize.py:VectorQuantizer,VectorQuantizer,class,37,114,90,1320,11.58,0,1,[],[],[],9,[],[],0
taming/modules/vqvae/quantize.py:GumbelQuantize,GumbelQuantize,class,88,279,176,2709,9.71,0,12,[],[],[],110,[],[],0
taming/modules/vqvae/quantize.py:VectorQuantizer2,VectorQuantizer2,class,80,303,185,3090,10.2,0,9,[],[],[],213,[],[],0
taming/modules/vqvae/quantize.py:VectorQuantizer:__init__,VectorQuantizer:__init__,method,10,15,15,196,13.07,0,0,"['self', 'n_e', 'e_dim', 'beta']","[None, None, None, None]","[None, None, None, None]",25,[],"['super', 'nn.Embedding']",2
taming/modules/vqvae/quantize.py:VectorQuantizer:forward,VectorQuantizer:forward,method,22,64,56,769,12.02,0,0,"['self', 'z']","[None, None]","[None, None]",34,"['        """"""\n', '        Inputs the output of the encoder network z and maps it to a discrete\n', '        one-hot vector that is the index of the closest embedding vector e_j\n', '        z (continuous) -> z_q (discrete)\n', '        z.shape = (batch, channel, height, width)\n', '        quantization pipeline:\n', '            1. get encoder input (B,C,H,W)\n', '            2. flatten input to (B*H*W,C)\n', '        """"""\n']","['z.permute', 'z.view', 'torch.sum', 'torch.matmul', 'torch.argmin', 'torch.zeros', 'min_encodings.scatter_', 'torch.mean', 'z.detach', 'torch.exp', 'torch.log', 'z_q.permute']",12
taming/modules/vqvae/quantize.py:VectorQuantizer:get_codebook_entry,VectorQuantizer:get_codebook_entry,method,9,23,20,256,11.13,0,1,"['self', 'indices', 'shape']","[None, None, None]","[None, None, None]",92,[],"['torch.zeros', 'min_encodings.scatter_', 'torch.matmul', 'z_q.view', 'z_q.permute']",5
taming/modules/vqvae/quantize.py:GumbelQuantize:__init__,GumbelQuantize:__init__,method,26,61,54,721,11.82,0,2,"['self', 'num_hiddens', 'embedding_dim', 'n_embed', 'straight_through', 'kl_weight', 'temp_init', 'use_vqinterface', 'remap', 'unknown_index']","[None, None, None, None, None, None, None, None, None, None]","[None, None, None, None, 'True', '5e-4', '1.0', 'True', 'None', '""random""']",117,[],"['super', 'nn.Conv2d', 'nn.Embedding', 'self.register_buffer', 'torch.tensor', 'print']",6
taming/modules/vqvae/quantize.py:GumbelQuantize:remap_to_used,GumbelQuantize:remap_to_used,method,16,23,23,374,16.26,0,1,"['self', 'inds']","[None, None]","[None, None]",147,[],"['len', 'inds.reshape', 'match.argmax', 'match.sum', 'new.reshape']",5
taming/modules/vqvae/quantize.py:GumbelQuantize:unmap_to_all,GumbelQuantize:unmap_to_all,method,17,27,26,276,10.22,0,1,"['self', 'inds']","[None, None]","[None, None]",161,[],"['len', 'inds.reshape', 'back.reshape']",3
taming/modules/vqvae/quantize.py:GumbelQuantize:forward,GumbelQuantize:forward,method,29,93,61,729,7.84,0,7,"['self', 'z', 'temp', 'return_logits']","[None, None, None, None]","[None, None, 'None', 'False']",171,[],"['self.proj', 'torch.zeros_like', 'F.gumbel_softmax', 'einsum', 'F.softmax', 'torch.sum', 'torch.log', 'soft_one_hot.argmax', 'self.remap_to_used']",9
taming/modules/vqvae/quantize.py:GumbelQuantize:get_codebook_entry,GumbelQuantize:get_codebook_entry,method,16,49,38,294,6.0,0,1,"['self', 'indices', 'shape']","[None, None, None]","[None, None, None]",92,[],"['rearrange', 'self.unmap_to_all', 'F.one_hot', 'einsum']",4
taming/modules/vqvae/quantize.py:VectorQuantizer2:__init__,VectorQuantizer2:__init__,method,23,59,51,657,11.14,0,2,"['self', 'n_e', 'e_dim', 'beta', 'remap', 'unknown_index', 'sane_index_shape', 'legacy']","[None, None, None, None, None, None, None, None]","[None, None, None, None, 'None', '""random""', 'False', 'True']",221,[],"['super', 'nn.Embedding', 'self.register_buffer', 'torch.tensor', 'print']",5
taming/modules/vqvae/quantize.py:VectorQuantizer2:remap_to_used,VectorQuantizer2:remap_to_used,method,16,23,23,374,16.26,0,1,"['self', 'inds']","[None, None]","[None, None]",147,[],"['len', 'inds.reshape', 'match.argmax', 'match.sum', 'new.reshape']",5
taming/modules/vqvae/quantize.py:VectorQuantizer2:unmap_to_all,VectorQuantizer2:unmap_to_all,method,17,27,26,276,10.22,0,1,"['self', 'inds']","[None, None]","[None, None]",161,[],"['len', 'inds.reshape', 'back.reshape']",3
taming/modules/vqvae/quantize.py:VectorQuantizer2:forward,VectorQuantizer2:forward,method,29,135,84,1242,9.2,0,3,"['self', 'z', 'temp', 'rescale_logits', 'return_logits']","[None, None, None, None, None]","[None, None, 'None', 'False', 'False']",271,[],"['rearrange', 'z.view', 'torch.sum', 'torch.einsum', 'torch.argmin', 'self.embedding', 'torch.mean', 'z.detach', 'min_encoding_indices.reshape', 'self.remap_to_used']",10
taming/modules/vqvae/quantize.py:VectorQuantizer2:get_codebook_entry,VectorQuantizer2:get_codebook_entry,method,10,34,24,262,7.71,0,2,"['self', 'indices', 'shape']","[None, None, None]","[None, None, None]",92,[],"['indices.reshape', 'self.unmap_to_all', 'self.embedding', 'z_q.view', 'z_q.permute']",5

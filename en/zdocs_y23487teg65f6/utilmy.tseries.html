<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8" /><meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>utilmy.tseries package &mdash; utilmy 1.0.0 documentation</title>
      <link rel="stylesheet" href="_static/pygments.css" type="text/css" />
      <link rel="stylesheet" href="_static/css/theme.css" type="text/css" />
  <!--[if lt IE 9]>
    <script src="_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
        <script data-url_root="./" id="documentation_options" src="_static/documentation_options.js"></script>
        <script src="_static/jquery.js"></script>
        <script src="_static/underscore.js"></script>
        <script src="_static/_sphinx_javascript_frameworks_compat.js"></script>
        <script src="_static/doctools.js"></script>
    <script src="_static/js/theme.js"></script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="utilmy.viz package" href="utilmy.viz.html" />
    <link rel="prev" title="utilmy.tools package" href="utilmy.tools.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search"  style="background: white" >
            <a href="index.html" class="icon icon-home"> utilmy
          </a>
              <div class="version">
                zdocs_y23487teg65f6
              </div>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption" role="heading"><span class="caption-text">Contents:</span></p>
<ul class="current">
<li class="toctree-l1 current"><a class="reference internal" href="modules.html">utilmy</a><ul class="current">
<li class="toctree-l2 current"><a class="reference internal" href="utilmy.html">utilmy package</a><ul class="current">
<li class="toctree-l3 current"><a class="reference internal" href="utilmy.html#subpackages">Subpackages</a><ul class="current">
<li class="toctree-l4"><a class="reference internal" href="utilmy.configs.html">utilmy.configs package</a></li>
<li class="toctree-l4"><a class="reference internal" href="utilmy.deeplearning.html">utilmy.deeplearning package</a></li>
<li class="toctree-l4"><a class="reference internal" href="utilmy.docs.html">utilmy.docs package</a></li>
<li class="toctree-l4"><a class="reference internal" href="utilmy.excel.html">utilmy.excel package</a></li>
<li class="toctree-l4"><a class="reference internal" href="utilmy.images.html">utilmy.images package</a></li>
<li class="toctree-l4"><a class="reference internal" href="utilmy.nlp.html">utilmy.nlp package</a></li>
<li class="toctree-l4"><a class="reference internal" href="utilmy.optim.html">utilmy.optim package</a></li>
<li class="toctree-l4"><a class="reference internal" href="utilmy.prepro.html">utilmy.prepro package</a></li>
<li class="toctree-l4"><a class="reference internal" href="utilmy.recsys.html">utilmy.recsys package</a></li>
<li class="toctree-l4"><a class="reference internal" href="utilmy.sspark.html">utilmy.sspark package</a></li>
<li class="toctree-l4"><a class="reference internal" href="utilmy.stats.html">utilmy.stats package</a></li>
<li class="toctree-l4"><a class="reference internal" href="utilmy.tabular.html">utilmy.tabular package</a></li>
<li class="toctree-l4"><a class="reference internal" href="utilmy.templates.html">utilmy.templates package</a></li>
<li class="toctree-l4"><a class="reference internal" href="utilmy.tools.html">utilmy.tools package</a></li>
<li class="toctree-l4 current"><a class="current reference internal" href="#">utilmy.tseries package</a><ul>
<li class="toctree-l5"><a class="reference internal" href="#submodules">Submodules</a></li>
<li class="toctree-l5"><a class="reference internal" href="#utilmy-tseries-prepro-tseries-module">utilmy.tseries.prepro_tseries module</a></li>
<li class="toctree-l5"><a class="reference internal" href="#utilmy-tseries-torch-lstm-module">utilmy.tseries.torch_lstm module</a></li>
<li class="toctree-l5"><a class="reference internal" href="#utilmy-tseries-torch-lstm-1-module">utilmy.tseries.torch_lstm (1) module</a></li>
<li class="toctree-l5"><a class="reference internal" href="#module-utilmy.tseries.torch_outlier">utilmy.tseries.torch_outlier module</a></li>
<li class="toctree-l5"><a class="reference internal" href="#module-utilmy.tseries.torch_outlier_comment">utilmy.tseries.torch_outlier_comment module</a></li>
<li class="toctree-l5"><a class="reference internal" href="#module-utilmy.tseries.util_tseries">utilmy.tseries.util_tseries module</a></li>
<li class="toctree-l5"><a class="reference internal" href="#module-utilmy.tseries">Module contents</a></li>
</ul>
</li>
<li class="toctree-l4"><a class="reference internal" href="utilmy.viz.html">utilmy.viz package</a></li>
<li class="toctree-l4"><a class="reference internal" href="utilmy.webscraper.html">utilmy.webscraper package</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="utilmy.html#submodules">Submodules</a></li>
<li class="toctree-l3"><a class="reference internal" href="utilmy.html#module-utilmy.adatasets">utilmy.adatasets module</a></li>
<li class="toctree-l3"><a class="reference internal" href="utilmy.html#module-utilmy.cli">utilmy.cli module</a></li>
<li class="toctree-l3"><a class="reference internal" href="utilmy.html#module-utilmy.data">utilmy.data module</a></li>
<li class="toctree-l3"><a class="reference internal" href="utilmy.html#module-utilmy.dates">utilmy.dates module</a></li>
<li class="toctree-l3"><a class="reference internal" href="utilmy.html#module-utilmy.debug">utilmy.debug module</a></li>
<li class="toctree-l3"><a class="reference internal" href="utilmy.html#module-utilmy.decorators">utilmy.decorators module</a></li>
<li class="toctree-l3"><a class="reference internal" href="utilmy.html#module-utilmy.distributed">utilmy.distributed module</a></li>
<li class="toctree-l3"><a class="reference internal" href="utilmy.html#module-utilmy.graph">utilmy.graph module</a></li>
<li class="toctree-l3"><a class="reference internal" href="utilmy.html#module-utilmy.iio">utilmy.iio module</a></li>
<li class="toctree-l3"><a class="reference internal" href="utilmy.html#module-utilmy.nnumpy">utilmy.nnumpy module</a></li>
<li class="toctree-l3"><a class="reference internal" href="utilmy.html#module-utilmy.oos">utilmy.oos module</a></li>
<li class="toctree-l3"><a class="reference internal" href="utilmy.html#module-utilmy.parallel">utilmy.parallel module</a></li>
<li class="toctree-l3"><a class="reference internal" href="utilmy.html#module-utilmy.ppandas">utilmy.ppandas module</a></li>
<li class="toctree-l3"><a class="reference internal" href="utilmy.html#module-utilmy.ppolars">utilmy.ppolars module</a></li>
<li class="toctree-l3"><a class="reference internal" href="utilmy.html#module-utilmy.util_batch">utilmy.util_batch module</a></li>
<li class="toctree-l3"><a class="reference internal" href="utilmy.html#module-utilmy.util_colab">utilmy.util_colab module</a></li>
<li class="toctree-l3"><a class="reference internal" href="utilmy.html#module-utilmy.util_conda">utilmy.util_conda module</a></li>
<li class="toctree-l3"><a class="reference internal" href="utilmy.html#utilmy-util-cpu-module">utilmy.util_cpu module</a></li>
<li class="toctree-l3"><a class="reference internal" href="utilmy.html#module-utilmy.util_download">utilmy.util_download module</a></li>
<li class="toctree-l3"><a class="reference internal" href="utilmy.html#module-utilmy.util_zip">utilmy.util_zip module</a></li>
<li class="toctree-l3"><a class="reference internal" href="utilmy.html#module-utilmy.utilmy">utilmy.utilmy module</a></li>
<li class="toctree-l3"><a class="reference internal" href="utilmy.html#module-utilmy.utils">utilmy.utils module</a></li>
<li class="toctree-l3"><a class="reference internal" href="utilmy.html#module-utilmy.z_test">utilmy.z_test module</a></li>
<li class="toctree-l3"><a class="reference internal" href="utilmy.html#module-utilmy.zdocstring">utilmy.zdocstring module</a></li>
<li class="toctree-l3"><a class="reference internal" href="utilmy.html#module-utilmy">Module contents</a></li>
</ul>
</li>
</ul>
</li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu"  style="background: white" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="index.html">utilmy</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="index.html" class="icon icon-home"></a> &raquo;</li>
          <li><a href="modules.html">utilmy</a> &raquo;</li>
          <li><a href="utilmy.html">utilmy package</a> &raquo;</li>
      <li>utilmy.tseries package</li>
      <li class="wy-breadcrumbs-aside">
              <a href="https://github.com/maltfield/rtd-github-pages/blob/master/docs/utilmy.tseries.rst" class="fa fa-github"> Edit on GitHub</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <section id="utilmy-tseries-package">
<h1>utilmy.tseries package<a class="headerlink" href="#utilmy-tseries-package" title="Permalink to this heading"></a></h1>
<section id="submodules">
<h2>Submodules<a class="headerlink" href="#submodules" title="Permalink to this heading"></a></h2>
</section>
<section id="utilmy-tseries-prepro-tseries-module">
<h2>utilmy.tseries.prepro_tseries module<a class="headerlink" href="#utilmy-tseries-prepro-tseries-module" title="Permalink to this heading"></a></h2>
</section>
<section id="utilmy-tseries-torch-lstm-module">
<h2>utilmy.tseries.torch_lstm module<a class="headerlink" href="#utilmy-tseries-torch-lstm-module" title="Permalink to this heading"></a></h2>
</section>
<section id="utilmy-tseries-torch-lstm-1-module">
<h2>utilmy.tseries.torch_lstm (1) module<a class="headerlink" href="#utilmy-tseries-torch-lstm-1-module" title="Permalink to this heading"></a></h2>
</section>
<section id="module-utilmy.tseries.torch_outlier">
<span id="utilmy-tseries-torch-outlier-module"></span><h2>utilmy.tseries.torch_outlier module<a class="headerlink" href="#module-utilmy.tseries.torch_outlier" title="Permalink to this heading"></a></h2>
<dl class="py class">
<dt class="sig sig-object py" id="utilmy.tseries.torch_outlier.DynamicLSTM">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">utilmy.tseries.torch_outlier.</span></span><span class="sig-name descname"><span class="pre">DynamicLSTM</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">input_size</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">hidden_size</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">100</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">num_layers</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dropout</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">bidirectional</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#utilmy.tseries.torch_outlier.DynamicLSTM" title="Permalink to this definition"></a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code></p>
<p>Dynamic LSTM module, which can handle variable length input sequence.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>input_size</strong> (<em>input size</em>) – </p></li>
<li><p><strong>hidden_size</strong> (<em>hidden size</em>) – </p></li>
<li><p><strong>num_layers</strong> (<em>number of hidden layers. Default: 1</em>) – </p></li>
<li><p><strong>dropout</strong> (<em>dropout rate. Default: 0.5</em>) – </p></li>
<li><p><strong>bidirectional</strong> (<em>If True</em><em>, </em><em>becomes a bidirectional RNN. Default: False.</em>) – </p></li>
<li><p><strong>Inputs</strong> – </p></li>
<li><p><strong>------</strong> – </p></li>
<li><p><strong>input</strong> (<em>tensor</em><em>, </em><em>shaped</em><em> [</em><em>batch</em><em>, </em><em>max_step</em><em>, </em><em>input_size</em><em>]</em>) – </p></li>
<li><p><strong>seq_lens</strong> (<em>tensor</em><em>, </em><em>shaped</em><em> [</em><em>batch</em><em>]</em><em>, </em><em>sequence lengths of batch</em>) – </p></li>
<li><p><strong>Outputs</strong> – </p></li>
<li><p><strong>-------</strong> – </p></li>
<li><p><strong>output</strong> (<em>tensor</em><em>, </em><em>shaped</em><em> [</em><em>batch</em><em>, </em><em>max_step</em><em>, </em><em>num_directions * hidden_size</em><em>]</em><em>,</em>) – tensor containing the output features (h_t) from the last layer
of the LSTM, for each t.</p></li>
</ul>
</dd>
</dl>
<dl class="py method">
<dt class="sig sig-object py" id="utilmy.tseries.torch_outlier.DynamicLSTM.forward">
<span class="sig-name descname"><span class="pre">forward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">x</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">seq_lens</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#utilmy.tseries.torch_outlier.DynamicLSTM.forward" title="Permalink to this definition"></a></dt>
<dd><p>Defines the computation performed at every call.</p>
<p>Should be overridden by all subclasses.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Although the recipe for forward pass needs to be defined within
this function, one should call the <code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code> instance afterwards
instead of this since the former takes care of running the
registered hooks while the latter silently ignores them.</p>
</div>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="utilmy.tseries.torch_outlier.DynamicLSTM.training">
<span class="sig-name descname"><span class="pre">training</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">bool</span></em><a class="headerlink" href="#utilmy.tseries.torch_outlier.DynamicLSTM.training" title="Permalink to this definition"></a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="py data">
<dt class="sig sig-object py" id="utilmy.tseries.torch_outlier.MNAME">
<span class="sig-prename descclassname"><span class="pre">utilmy.tseries.torch_outlier.</span></span><span class="sig-name descname"><span class="pre">MNAME</span></span><em class="property"><span class="w"> </span><span class="p"><span class="pre">=</span></span><span class="w"> </span><span class="pre">''</span></em><a class="headerlink" href="#utilmy.tseries.torch_outlier.MNAME" title="Permalink to this definition"></a></dt>
<dd><p>#
Doc:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">Time</span> <span class="n">Series</span> <span class="n">Anomaly</span> <span class="n">Detection</span> <span class="n">using</span> <span class="n">LSTM</span> <span class="n">Autoencoders</span> <span class="k">with</span> <span class="n">PyTorch</span> <span class="ow">in</span> <span class="n">Python</span>
</pre></div>
</div>
<p>pip install -qq arff2pandas watermark</p>
<p># %reload_ext watermark
# %watermark -v -p numpy,pandas,torch,arff2pandas</p>
<blockquote>
<div><p><a class="reference external" href="https://prod.liveshare.vsengsaas.visualstudio.com/join?B79A51FE5D82309D32D1283340E14D9C411C">https://prod.liveshare.vsengsaas.visualstudio.com/join?B79A51FE5D82309D32D1283340E14D9C411C</a></p>
</div></blockquote>
<p>Join Zoom Meeting
<a class="reference external" href="https://us05web.zoom.us/j/2933746463?pwd=WUhRWkx0NWNZRVBFVjZ4enV6Y1R2QT09">https://us05web.zoom.us/j/2933746463?pwd=WUhRWkx0NWNZRVBFVjZ4enV6Y1R2QT09</a></p>
<p>Meeting ID: 293 374 6463
Passcode: J50Muh</p>
</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="utilmy.tseries.torch_outlier.QuoraModel">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">utilmy.tseries.torch_outlier.</span></span><span class="sig-name descname"><span class="pre">QuoraModel</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">args</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#utilmy.tseries.torch_outlier.QuoraModel" title="Permalink to this definition"></a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code></p>
<p>Model for quora insincere question classification.</p>
<dl class="py method">
<dt class="sig sig-object py" id="utilmy.tseries.torch_outlier.QuoraModel.forward">
<span class="sig-name descname"><span class="pre">forward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">word_seq</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">seq_len</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#utilmy.tseries.torch_outlier.QuoraModel.forward" title="Permalink to this definition"></a></dt>
<dd><p>Defines the computation performed at every call.</p>
<p>Should be overridden by all subclasses.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Although the recipe for forward pass needs to be defined within
this function, one should call the <code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code> instance afterwards
instead of this since the former takes care of running the
registered hooks while the latter silently ignores them.</p>
</div>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="utilmy.tseries.torch_outlier.QuoraModel.training">
<span class="sig-name descname"><span class="pre">training</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">bool</span></em><a class="headerlink" href="#utilmy.tseries.torch_outlier.QuoraModel.training" title="Permalink to this definition"></a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="utilmy.tseries.torch_outlier.dataset_ECG5000_fetch_pandas">
<span class="sig-prename descclassname"><span class="pre">utilmy.tseries.torch_outlier.</span></span><span class="sig-name descname"><span class="pre">dataset_ECG5000_fetch_pandas</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">nrows</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">100</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dirout</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'./ztmp/'</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#utilmy.tseries.torch_outlier.dataset_ECG5000_fetch_pandas" title="Permalink to this definition"></a></dt>
<dd><p>combine  training and test data into a single data frame.
This will give us more data to train our Autoencoder. also shuffle it:</p>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="utilmy.tseries.torch_outlier.dataset_ECG5000_prep">
<span class="sig-prename descclassname"><span class="pre">utilmy.tseries.torch_outlier.</span></span><span class="sig-name descname"><span class="pre">dataset_ECG5000_prep</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">df</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#utilmy.tseries.torch_outlier.dataset_ECG5000_prep" title="Permalink to this definition"></a></dt>
<dd><p>have 5,000 examples. Each row represents a single heartbeat record. Let’s name  possible classes:</p>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="utilmy.tseries.torch_outlier.dataset_create">
<span class="sig-prename descclassname"><span class="pre">utilmy.tseries.torch_outlier.</span></span><span class="sig-name descname"><span class="pre">dataset_create</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">df</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#utilmy.tseries.torch_outlier.dataset_create" title="Permalink to this definition"></a></dt>
<dd></dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="utilmy.tseries.torch_outlier.help">
<span class="sig-prename descclassname"><span class="pre">utilmy.tseries.torch_outlier.</span></span><span class="sig-name descname"><span class="pre">help</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#utilmy.tseries.torch_outlier.help" title="Permalink to this definition"></a></dt>
<dd><p>function help</p>
</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="utilmy.tseries.torch_outlier.modelDecoder">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">utilmy.tseries.torch_outlier.</span></span><span class="sig-name descname"><span class="pre">modelDecoder</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">seq_len</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">input_dim</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">64</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">n_features</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#utilmy.tseries.torch_outlier.modelDecoder" title="Permalink to this definition"></a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code></p>
<p>Our Decoder contains two LSTM layers and an output layer that gives  final reconstruction.
#
# Time to wrap everything into an easy to use module:</p>
<dl class="py method">
<dt class="sig sig-object py" id="utilmy.tseries.torch_outlier.modelDecoder.forward">
<span class="sig-name descname"><span class="pre">forward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">x</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#utilmy.tseries.torch_outlier.modelDecoder.forward" title="Permalink to this definition"></a></dt>
<dd><p>Defines the computation performed at every call.</p>
<p>Should be overridden by all subclasses.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Although the recipe for forward pass needs to be defined within
this function, one should call the <code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code> instance afterwards
instead of this since the former takes care of running the
registered hooks while the latter silently ignores them.</p>
</div>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="utilmy.tseries.torch_outlier.modelDecoder.training">
<span class="sig-name descname"><span class="pre">training</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">bool</span></em><a class="headerlink" href="#utilmy.tseries.torch_outlier.modelDecoder.training" title="Permalink to this definition"></a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="utilmy.tseries.torch_outlier.modelDecoder3">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">utilmy.tseries.torch_outlier.</span></span><span class="sig-name descname"><span class="pre">modelDecoder3</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">seq_len</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">input_dim</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">64</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">n_features</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#utilmy.tseries.torch_outlier.modelDecoder3" title="Permalink to this definition"></a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code></p>
<p>Our Decoder contains two LSTM layers and an output layer that gives  final reconstruction.
#
# Time to wrap everything into an easy to use module:</p>
<dl class="py method">
<dt class="sig sig-object py" id="utilmy.tseries.torch_outlier.modelDecoder3.forward">
<span class="sig-name descname"><span class="pre">forward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">x</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#utilmy.tseries.torch_outlier.modelDecoder3.forward" title="Permalink to this definition"></a></dt>
<dd><p>Defines the computation performed at every call.</p>
<p>Should be overridden by all subclasses.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Although the recipe for forward pass needs to be defined within
this function, one should call the <code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code> instance afterwards
instead of this since the former takes care of running the
registered hooks while the latter silently ignores them.</p>
</div>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="utilmy.tseries.torch_outlier.modelDecoder3.training">
<span class="sig-name descname"><span class="pre">training</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">bool</span></em><a class="headerlink" href="#utilmy.tseries.torch_outlier.modelDecoder3.training" title="Permalink to this definition"></a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="utilmy.tseries.torch_outlier.modelEncoder">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">utilmy.tseries.torch_outlier.</span></span><span class="sig-name descname"><span class="pre">modelEncoder</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">seq_len</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">n_features</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">embedding_dim</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">64</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#utilmy.tseries.torch_outlier.modelEncoder" title="Permalink to this definition"></a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code></p>
<p><em>Encoder</em> uses two LSTM layers to compress  Time Series data input.</p>
<p>Next, decode  compressed representation using a <em>Decoder</em>:</p>
<dl class="py method">
<dt class="sig sig-object py" id="utilmy.tseries.torch_outlier.modelEncoder.forward">
<span class="sig-name descname"><span class="pre">forward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">x</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#utilmy.tseries.torch_outlier.modelEncoder.forward" title="Permalink to this definition"></a></dt>
<dd><p>Defines the computation performed at every call.</p>
<p>Should be overridden by all subclasses.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Although the recipe for forward pass needs to be defined within
this function, one should call the <code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code> instance afterwards
instead of this since the former takes care of running the
registered hooks while the latter silently ignores them.</p>
</div>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="utilmy.tseries.torch_outlier.modelEncoder.training">
<span class="sig-name descname"><span class="pre">training</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">bool</span></em><a class="headerlink" href="#utilmy.tseries.torch_outlier.modelEncoder.training" title="Permalink to this definition"></a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="utilmy.tseries.torch_outlier.modelEncoder2">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">utilmy.tseries.torch_outlier.</span></span><span class="sig-name descname"><span class="pre">modelEncoder2</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">seq_len</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">n_features</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">embedding_dim</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">64</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dropout</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.3</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#utilmy.tseries.torch_outlier.modelEncoder2" title="Permalink to this definition"></a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code></p>
<dl>
<dt><em>Encoder</em> uses two LSTM layers to compress  Time Series data input.</dt><dd><blockquote>
<div><dl>
<dt>Args:</dt><dd><p>input_size : The number of expected features in the input <cite>x</cite>
hidden_size: The number of features in the hidden state <cite>h</cite>
num_layers : Number of recurrent layers. E.g., setting <code class="docutils literal notranslate"><span class="pre">num_layers=2</span></code></p>
<blockquote>
<div><p>would mean stacking two LSTMs together to form a <cite>stacked LSTM</cite>,
with the second LSTM taking in outputs of the first LSTM and
computing the final results. Default: 1</p>
</div></blockquote>
<p>bias:        If <code class="docutils literal notranslate"><span class="pre">False</span></code>, then the layer does not use bias weights <cite>b_ih</cite> and <cite>b_hh</cite>. Default: <code class="docutils literal notranslate"><span class="pre">True</span></code>
batch_first: If <code class="docutils literal notranslate"><span class="pre">True</span></code>, then the input and output tensors are provided as (batch, seq, feature). Default: <code class="docutils literal notranslate"><span class="pre">False</span></code>
dropout    : If non-zero, introduces a <cite>Dropout</cite> layer on the outputs of each</p>
<blockquote>
<div><p>LSTM layer except the last layer, with dropout probability equal to <code class="xref py py-attr docutils literal notranslate"><span class="pre">dropout</span></code>. Default: 0</p>
</div></blockquote>
<p>bidirectional: If <code class="docutils literal notranslate"><span class="pre">True</span></code>, becomes a bidirectional LSTM. Default: <code class="docutils literal notranslate"><span class="pre">False</span></code></p>
</dd>
<dt>Inpu Dimmension :</dt><dd><p>embedding_dim= 64
hidden_dim =  2 * 64</p>
</dd>
</dl>
</div></blockquote>
<p>encoder is 2 LSTM stacked as below:</p>
<dl>
<dt>Let’s continiue on the input like this :</dt><dd><blockquote>
<div><p>Xinput dim :    torch.Size([140, 1])
Ypred_dim  :    torch.Size([140, 1])</p>
</div></blockquote>
<dl class="simple">
<dt>self.rnn1 = nn.LSTM(</dt><dd><p>input_size=n_features,
hidden_size=self.hidden_dim,
num_layers=1,</p>
</dd>
</dl>
<p>)</p>
<dl class="simple">
<dt>self.rnn2 = nn.LSTM(</dt><dd><p>input_size=self.hidden_dim,
hidden_size=embedding_dim,
num_layers=1,</p>
</dd>
</dl>
<p>)</p>
</dd>
</dl>
<p>what does it mean the n_features ???</p>
<blockquote>
<div><p>seq_len : lenght of the sequence (time series ??)</p>
<dl class="simple">
<dt>Example<span class="classifier">multi-variate time seeries , stacked together ?</span></dt><dd><p>time sereis 1
time series 2
time series 3</p>
</dd>
</dl>
<p>=  n_features = 3  ????</p>
<p>ok,   What about the stakced LSTM parts ??/</p>
<blockquote>
<div><blockquote>
<div><p>x, (_, _)        = self.rnn1(x)
x, (hidden_n, _) = self.rnn2(x)</p>
</div></blockquote>
<dl>
<dt>Sure, why using</dt><dd><dl>
<dt>self.rnn1 = nn.LSTM(</dt><dd><blockquote>
<div><p>…</p>
</div></blockquote>
<p>num_layers=  2</p>
</dd>
</dl>
</dd>
</dl>
<p>because the hidden layer is not passed</p>
<dl class="simple">
<dt>How to find dimension of the hidden layer ?</dt><dd><dl class="simple">
<dt>ok, we reshape 1 embedding per channel (time series)  ??</dt><dd><p>return hidden_n.reshape((self.n_features, self.embedding_dim))</p>
</dd>
</dl>
</dd>
</dl>
<p>Other question, the forward pass only return the hidden layer…
Thhough it will return the x too ???</p>
</div></blockquote>
<p>I would do it with 2 layers in one module
that is for 2 layers
allows more representation power for the model</p>
</div></blockquote>
</dd>
<dt>correct</dt><dd><p>seq_len - is obvi ous
n_features is number of channels in input  (==nb of time series)</p>
</dd>
</dl>
<dl class="py method">
<dt class="sig sig-object py" id="utilmy.tseries.torch_outlier.modelEncoder2.forward">
<span class="sig-name descname"><span class="pre">forward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">x</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#utilmy.tseries.torch_outlier.modelEncoder2.forward" title="Permalink to this definition"></a></dt>
<dd><p>Defines the computation performed at every call.</p>
<p>Should be overridden by all subclasses.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Although the recipe for forward pass needs to be defined within
this function, one should call the <code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code> instance afterwards
instead of this since the former takes care of running the
registered hooks while the latter silently ignores them.</p>
</div>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="utilmy.tseries.torch_outlier.modelEncoder2.training">
<span class="sig-name descname"><span class="pre">training</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">bool</span></em><a class="headerlink" href="#utilmy.tseries.torch_outlier.modelEncoder2.training" title="Permalink to this definition"></a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="utilmy.tseries.torch_outlier.modelEncoder3">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">utilmy.tseries.torch_outlier.</span></span><span class="sig-name descname"><span class="pre">modelEncoder3</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">seq_len</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">n_features</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">embedding_dim</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">64</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#utilmy.tseries.torch_outlier.modelEncoder3" title="Permalink to this definition"></a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code></p>
<p><em>Encoder</em> uses two LSTM layers to compress  Time Series data input.</p>
<p>Next, decode  compressed representation using a <em>Decoder</em>:</p>
<dl class="py attribute">
<dt class="sig sig-object py" id="utilmy.tseries.torch_outlier.modelEncoder3.embedding_dim">
<span class="sig-name descname"><span class="pre">embedding_dim</span></span><a class="headerlink" href="#utilmy.tseries.torch_outlier.modelEncoder3.embedding_dim" title="Permalink to this definition"></a></dt>
<dd><dl>
<dt>its more an EXERCICE to try to plug in the Transformer Encoder</dt><dd><blockquote>
<div><blockquote>
<div><p>and match the dimensions.
(  therey are many transformer things (ie sentence tranfsormer… ), next time
we can try another one more adapted. )</p>
<blockquote>
<div><p><a class="reference external" href="https://github.com/lucidrains/linear-attention-transformer">https://github.com/lucidrains/linear-attention-transformer</a></p>
<p>TransformerEncoderLayer(d_model,
nhead, dim_feedforward=2048,</p>
<blockquote>
<div><p>dropout=0.1,</p>
<blockquote>
<div><p>layer_norm_eps=1e-05,
batch_first=False,</p>
<blockquote>
<div><p>norm_first=False,
device=None, dtype=None)</p>
</div></blockquote>
</div></blockquote>
</div></blockquote>
</div></blockquote>
</div></blockquote>
<p>bs = 5  ### 5 odd number for easy debug…
seq_len =  10   ## seq lenth, nb of tokens.
d_in = 32   #####  In features size   //    d_in = 16 -&gt; Linear(16, 32) -&gt; d_model 32 tor march</p>
<p>#### LSTM       batch,      seq_len,      n_features
x = torch.randn(bs, seq_len, d_in)
d_h = 32   ### 32 of features…. for ONE token   ===  d_model ===  self.hidden_dim</p>
<p>### dim_feedforward=2048   ### Internal MLP
Linear(d_model, d_ff) Linear(d_ff, d_model)
enc = nn.TransformerEncoderLayer(d_h, 4, dim_feedforward=128,</p>
<blockquote>
<div><blockquote>
<div><blockquote>
<div><p>batch_first=True   #### Ordering the input (batch size,  ….  )</p>
</div></blockquote>
<p>)</p>
</div></blockquote>
<p>Xencode = enc(X)</p>
<p>### torch.Size([5, 10, 32])  ####   out = enc(x)  print(out.shape)
#### same dimension, in float ???
# ###  cannot use as “embedding” …</p>
<p>TimSeries= bs, seq_len n_features -&gt;ConveLayer–&gt; (bs new_seq_len d_model)</p>
<p>(bs new_seq_len d_model)   —&gt; TFLayer  —&gt;  (bs new_seq_len d_model)</p>
<blockquote>
<div><blockquote>
<div><p>TLayerEncoder –&gt;   (bs, seq_len_v2 d_model)
Pooling –&gt; (bs, 0, d_model)    ### Role of middle “compressor””</p>
</div></blockquote>
<p>(bs, seq_len_v3 d_model), (bs, 0, d_model)–&gt; TFLayerDecode –&gt;  (bs, seq_len_v3 d_model)</p>
</div></blockquote>
<p>### this is what they are using:  Pooling in the middle: here this one below:
<a class="reference external" href="https://github.com/UKPLab/sentence-transformers/tree/master/examples/unsupervised_learning/TSDAE">https://github.com/UKPLab/sentence-transformers/tree/master/examples/unsupervised_learning/TSDAE</a></p>
</div></blockquote>
</div></blockquote>
<p>idea: timse series into “text token”</p>
</dd>
</dl>
<p>##Text Version:      bs, seq_len            [int] -&gt; EmbeddingLayer                 –&gt;  bs sl d_model [float]
## Time version :    bs, seq_len, n_feat [float]  -&gt; 1D Conv (to create token-like) –&gt;  bs sl2 d_model [float]</p>
<blockquote>
<div><dl>
<dt>#### Implementation</dt><dd><p>(Conv, TFEnc-decode, DeConv)   –&gt; get back time series.</p>
<dl>
<dt>We only have interest in the pooling “embedding”</dt><dd><blockquote>
<div><p>timeseries –&gt;   Can we have Embedding Vector  –&gt; we compare vectors…</p>
<p>Embedding Vector ==  Output of the pooling  (which is here: 1st item , special token):</p>
<p>Dimension of Output of the Pooling (for example):  bs, 1, d_model</p>
<p>Pooling = Compression/Flattening of the list of TFEncode vectors  into  1D vector.</p>
<p>Suppose you have used the output of the pooling for debugging/ check/ actual neighbor search ?
by Cos. similarity,  “Easy Check if we have access to the pooling output”.</p>
<p>ConV/ Deconv        : TSeries –&gt; Token…
Poooling in the middle:   for</p>
<p>and Plug into TFLayer (Contentn Encoder, Content Decoder)
TFLayer parameters NAMES, they are normalizd : d_in, d_h</p>
<blockquote>
<div><p>d_model (both input and output) num_heads (d_model//num_heads == 0)
d_ff = 4* d_model
growth like exponential with seq_len - but there linear attention models</p>
</div></blockquote>
<p>++ cost of training ++ longer.  3X-10X longer than LSTM ???
In the end, LSTM is a good baselines (cost/perf/debugging )</p>
<blockquote>
<div><p><a class="reference external" href="https://medium.com/ai%C2%B3-theory-practice-business/awd-lstm-6b2744e809c5">https://medium.com/ai%C2%B3-theory-practice-business/awd-lstm-6b2744e809c5</a>
FastAI, default one.
standard one,
Dropout.</p>
</div></blockquote>
<p>Sure, Are you faimilait with Image model like EfficienttNet ?
Big model…. no worries,</p>
</div></blockquote>
<p>AWD-LSTM by Merity
SHA-RNN by Merity</p>
</dd>
</dl>
</dd>
</dl>
</div></blockquote>
<p>from linear_attention_transformer import LinearAttentionTransformerLM, LinformerContextSettings
settings = LinformerContextSettings(  seq_len = 2048,  k = 256)</p>
<dl>
<dt>dec = LinearAttentionTransformerLM(</dt><dd><p>num_tokens = 20000,
dim = 512,
heads = 8,
depth = 6,</p>
<p>max_seq_len = 4096,
causal = True,
context_linformer_settings = settings,
receives_context = True</p>
</dd>
</dl>
<p>).cuda()</p>
<blockquote>
<div><blockquote>
<div><dl>
<dt>Cannot use Pre-trained model (== hard to get results fast….).</dt><dd><p>Google Mutlti horizon transformer time series.
TSAI
<a class="reference external" href="https://github.com/timeseriesAI/tsai">https://github.com/timeseriesAI/tsai</a>    nice API</p>
<p>DeepAR  by Amaazon ,  RNN + Auto-rerefressive ,…. It
<a class="reference external" href="https://ts.gluon.ai/">https://ts.gluon.ai/</a>
<a class="reference external" href="https://ts.gluon.ai/api/gluonts/gluonts.model.deepar.html">https://ts.gluon.ai/api/gluonts/gluonts.model.deepar.html</a></p>
<blockquote>
<div><dl>
<dt>Business focus API</dt><dd><p>Yes and NO :  it takes time to train….
( simpler model == les time to train and perf woithin reasonable )</p>
<p>TiemSeries :  we need more 4-5  years before anything liek NLP onees./.
Deep Learning is NOT YET enough flexible to include business rules/ many Context things…</p>
<dl class="simple">
<dt>BUT, for Outlier its ok, because of embedding.</dt><dd><p>very easy after/standard.</p>
</dd>
</dl>
<p>70% of DL application to get the embedding in some ways.</p>
</dd>
</dl>
</div></blockquote>
</dd>
</dl>
</div></blockquote>
<p>ts-gluon  is good, generic, different models…. more simple ones too.
torch-forecast :</p>
<blockquote>
<div><blockquote>
<div><p><a class="reference external" href="https://pytorch-forecasting.readthedocs.io/en/stable/">https://pytorch-forecasting.readthedocs.io/en/stable/</a></p>
</div></blockquote>
<dl>
<dt>into the embedding part  —&gt; we can combine with Text, …. more easily.</dt><dd><p>we can store somewhere, forever….
Some input.</p>
<dl>
<dt>DeepCTRL by Google ?</dt><dd><p>To encore rules + data
<a class="reference external" href="https://ai.googleblog.com/2022/01/controlling-neural-networks-with-rule.html">https://ai.googleblog.com/2022/01/controlling-neural-networks-with-rule.html</a></p>
<p>### Some clean re-implementation here too:
utilmy/deeplearning/ttorch/rule_encoder4.py  (not perfect…)</p>
<dl>
<dt>Idea :</dt><dd><p>modelA to encode “MANUAL RULES”  (ex:   if weight &gt; 70% –&gt; Always ypred=1, HARD constraints )</p>
<p>modelB to encode  “data”  (ie auto-encoder, MLP, …)</p>
<p>modelMerge(modelA, modelB)  –&gt; Prediction.</p>
</dd>
</dl>
<p>Their code is very “hard-coded everywhere”….</p>
<p>Encode Data (encoder)</p>
</dd>
</dl>
</dd>
</dl>
<p>library - api for time-series</p>
</div></blockquote>
</div></blockquote>
<p>#### from SentenceEmb Code,
class Pooling(nn.Module):</p>
<blockquote>
<div><p>Performs pooling (max or mean) on the token embeddings.
Using pooling, it generates from a variable sized sentence a fixed sized sentence embedding. This layer also allows to use the CLS token if it is returned by the underlying word embedding model.
You can concatenate multiple poolings together.
word_embedding_dimension: Dimensions for the word embeddings
pooling_mode: Can be a string: mean/max/cls. If set, overwrites the other pooling_mode_* settings</p>
<p>pooling_mode_cls_token: Use the first token (CLS token) as text representations</p>
<p>pooling_mode_max_tokens: Use max in each dimension over all tokens.</p>
<p>pooling_mode_mean_tokens: Perform mean-pooling
pooling_mode_mean_sqrt_len_tokens: Perform mean-pooling, but devide by sqrt(input_length).</p>
</div></blockquote>
<p># Defining our sentence transformer model
word_embedding_model = models.Transformer(model_name, max_seq_length=max_seq_length)
pooling_model = models.Pooling(word_embedding_model.get_word_embedding_dimension(), ‘cls’)
model = SentenceTransformer(modules=[word_embedding_model, pooling_model</p>
<blockquote>
<div><dl>
<dt>###</dt><dd><p>self_attn.in_proj_weight     torch.Size([96, 32])
self_attn.in_proj_bias       torch.Size([96])
self_attn.out_proj.weight    torch.Size([32, 32])
self_attn.out_proj.bias      torch.Size([32])</p>
<p>##### Position wise feedforward. &lt;&gt; MLP
### mini-compression  32 –&gt; 128 –&gt; 32   (mutiplication is done….)bs, sq 32 -&gt; bs sq 128 -&gt; bs sl 32</p>
<blockquote>
<div><p>Input / outpit</p>
</div></blockquote>
<p>linear1.weight               torch.Size([128, 32])  +  linear1.bias                torch.Size([128])
linear2.weight               torch.Size([32, 128])</p>
<p>linear2.bias                 torch.Size([32])
norm1.weight                 torch.Size([32])
norm1.bias                   torch.Size([32])
norm2.weight                 torch.Size([32])
norm2.bias                   torch.Size([32])</p>
</dd>
</dl>
</div></blockquote>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="utilmy.tseries.torch_outlier.modelEncoder3.forward">
<span class="sig-name descname"><span class="pre">forward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">x</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#utilmy.tseries.torch_outlier.modelEncoder3.forward" title="Permalink to this definition"></a></dt>
<dd><p>Defines the computation performed at every call.</p>
<p>Should be overridden by all subclasses.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Although the recipe for forward pass needs to be defined within
this function, one should call the <code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code> instance afterwards
instead of this since the former takes care of running the
registered hooks while the latter silently ignores them.</p>
</div>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="utilmy.tseries.torch_outlier.modelEncoder3.hidden_dim">
<span class="sig-name descname"><span class="pre">hidden_dim</span></span><a class="headerlink" href="#utilmy.tseries.torch_outlier.modelEncoder3.hidden_dim" title="Permalink to this definition"></a></dt>
<dd><dl>
<dt>its more an EXERCICE to try to plug in the Transformer Encoder</dt><dd><blockquote>
<div><blockquote>
<div><p>and match the dimensions.
(  therey are many transformer things (ie sentence tranfsormer… ), next time
we can try another one more adapted. )</p>
<blockquote>
<div><p><a class="reference external" href="https://github.com/lucidrains/linear-attention-transformer">https://github.com/lucidrains/linear-attention-transformer</a></p>
<p>TransformerEncoderLayer(d_model,
nhead, dim_feedforward=2048,</p>
<blockquote>
<div><p>dropout=0.1,</p>
<blockquote>
<div><p>layer_norm_eps=1e-05,
batch_first=False,</p>
<blockquote>
<div><p>norm_first=False,
device=None, dtype=None)</p>
</div></blockquote>
</div></blockquote>
</div></blockquote>
</div></blockquote>
</div></blockquote>
<p>bs = 5  ### 5 odd number for easy debug…
seq_len =  10   ## seq lenth, nb of tokens.
d_in = 32   #####  In features size   //    d_in = 16 -&gt; Linear(16, 32) -&gt; d_model 32 tor march</p>
<p>#### LSTM       batch,      seq_len,      n_features
x = torch.randn(bs, seq_len, d_in)
d_h = 32   ### 32 of features…. for ONE token   ===  d_model ===  self.hidden_dim</p>
<p>### dim_feedforward=2048   ### Internal MLP
Linear(d_model, d_ff) Linear(d_ff, d_model)
enc = nn.TransformerEncoderLayer(d_h, 4, dim_feedforward=128,</p>
<blockquote>
<div><blockquote>
<div><blockquote>
<div><p>batch_first=True   #### Ordering the input (batch size,  ….  )</p>
</div></blockquote>
<p>)</p>
</div></blockquote>
<p>Xencode = enc(X)</p>
<p>### torch.Size([5, 10, 32])  ####   out = enc(x)  print(out.shape)
#### same dimension, in float ???
# ###  cannot use as “embedding” …</p>
<p>TimSeries= bs, seq_len n_features -&gt;ConveLayer–&gt; (bs new_seq_len d_model)</p>
<p>(bs new_seq_len d_model)   —&gt; TFLayer  —&gt;  (bs new_seq_len d_model)</p>
<blockquote>
<div><blockquote>
<div><p>TLayerEncoder –&gt;   (bs, seq_len_v2 d_model)
Pooling –&gt; (bs, 0, d_model)    ### Role of middle “compressor””</p>
</div></blockquote>
<p>(bs, seq_len_v3 d_model), (bs, 0, d_model)–&gt; TFLayerDecode –&gt;  (bs, seq_len_v3 d_model)</p>
</div></blockquote>
<p>### this is what they are using:  Pooling in the middle: here this one below:
<a class="reference external" href="https://github.com/UKPLab/sentence-transformers/tree/master/examples/unsupervised_learning/TSDAE">https://github.com/UKPLab/sentence-transformers/tree/master/examples/unsupervised_learning/TSDAE</a></p>
</div></blockquote>
</div></blockquote>
<p>idea: timse series into “text token”</p>
</dd>
</dl>
<p>##Text Version:      bs, seq_len            [int] -&gt; EmbeddingLayer                 –&gt;  bs sl d_model [float]
## Time version :    bs, seq_len, n_feat [float]  -&gt; 1D Conv (to create token-like) –&gt;  bs sl2 d_model [float]</p>
<blockquote>
<div><dl>
<dt>#### Implementation</dt><dd><p>(Conv, TFEnc-decode, DeConv)   –&gt; get back time series.</p>
<dl>
<dt>We only have interest in the pooling “embedding”</dt><dd><blockquote>
<div><p>timeseries –&gt;   Can we have Embedding Vector  –&gt; we compare vectors…</p>
<p>Embedding Vector ==  Output of the pooling  (which is here: 1st item , special token):</p>
<p>Dimension of Output of the Pooling (for example):  bs, 1, d_model</p>
<p>Pooling = Compression/Flattening of the list of TFEncode vectors  into  1D vector.</p>
<p>Suppose you have used the output of the pooling for debugging/ check/ actual neighbor search ?
by Cos. similarity,  “Easy Check if we have access to the pooling output”.</p>
<p>ConV/ Deconv        : TSeries –&gt; Token…
Poooling in the middle:   for</p>
<p>and Plug into TFLayer (Contentn Encoder, Content Decoder)
TFLayer parameters NAMES, they are normalizd : d_in, d_h</p>
<blockquote>
<div><p>d_model (both input and output) num_heads (d_model//num_heads == 0)
d_ff = 4* d_model
growth like exponential with seq_len - but there linear attention models</p>
</div></blockquote>
<p>++ cost of training ++ longer.  3X-10X longer than LSTM ???
In the end, LSTM is a good baselines (cost/perf/debugging )</p>
<blockquote>
<div><p><a class="reference external" href="https://medium.com/ai%C2%B3-theory-practice-business/awd-lstm-6b2744e809c5">https://medium.com/ai%C2%B3-theory-practice-business/awd-lstm-6b2744e809c5</a>
FastAI, default one.
standard one,
Dropout.</p>
</div></blockquote>
<p>Sure, Are you faimilait with Image model like EfficienttNet ?
Big model…. no worries,</p>
</div></blockquote>
<p>AWD-LSTM by Merity
SHA-RNN by Merity</p>
</dd>
</dl>
</dd>
</dl>
</div></blockquote>
<p>from linear_attention_transformer import LinearAttentionTransformerLM, LinformerContextSettings
settings = LinformerContextSettings(  seq_len = 2048,  k = 256)</p>
<dl>
<dt>dec = LinearAttentionTransformerLM(</dt><dd><p>num_tokens = 20000,
dim = 512,
heads = 8,
depth = 6,</p>
<p>max_seq_len = 4096,
causal = True,
context_linformer_settings = settings,
receives_context = True</p>
</dd>
</dl>
<p>).cuda()</p>
<blockquote>
<div><blockquote>
<div><dl>
<dt>Cannot use Pre-trained model (== hard to get results fast….).</dt><dd><p>Google Mutlti horizon transformer time series.
TSAI
<a class="reference external" href="https://github.com/timeseriesAI/tsai">https://github.com/timeseriesAI/tsai</a>    nice API</p>
<p>DeepAR  by Amaazon ,  RNN + Auto-rerefressive ,…. It
<a class="reference external" href="https://ts.gluon.ai/">https://ts.gluon.ai/</a>
<a class="reference external" href="https://ts.gluon.ai/api/gluonts/gluonts.model.deepar.html">https://ts.gluon.ai/api/gluonts/gluonts.model.deepar.html</a></p>
<blockquote>
<div><dl>
<dt>Business focus API</dt><dd><p>Yes and NO :  it takes time to train….
( simpler model == les time to train and perf woithin reasonable )</p>
<p>TiemSeries :  we need more 4-5  years before anything liek NLP onees./.
Deep Learning is NOT YET enough flexible to include business rules/ many Context things…</p>
<dl class="simple">
<dt>BUT, for Outlier its ok, because of embedding.</dt><dd><p>very easy after/standard.</p>
</dd>
</dl>
<p>70% of DL application to get the embedding in some ways.</p>
</dd>
</dl>
</div></blockquote>
</dd>
</dl>
</div></blockquote>
<p>ts-gluon  is good, generic, different models…. more simple ones too.
torch-forecast :</p>
<blockquote>
<div><blockquote>
<div><p><a class="reference external" href="https://pytorch-forecasting.readthedocs.io/en/stable/">https://pytorch-forecasting.readthedocs.io/en/stable/</a></p>
</div></blockquote>
<dl>
<dt>into the embedding part  —&gt; we can combine with Text, …. more easily.</dt><dd><p>we can store somewhere, forever….
Some input.</p>
<dl>
<dt>DeepCTRL by Google ?</dt><dd><p>To encore rules + data
<a class="reference external" href="https://ai.googleblog.com/2022/01/controlling-neural-networks-with-rule.html">https://ai.googleblog.com/2022/01/controlling-neural-networks-with-rule.html</a></p>
<p>### Some clean re-implementation here too:
utilmy/deeplearning/ttorch/rule_encoder4.py  (not perfect…)</p>
<dl>
<dt>Idea :</dt><dd><p>modelA to encode “MANUAL RULES”  (ex:   if weight &gt; 70% –&gt; Always ypred=1, HARD constraints )</p>
<p>modelB to encode  “data”  (ie auto-encoder, MLP, …)</p>
<p>modelMerge(modelA, modelB)  –&gt; Prediction.</p>
</dd>
</dl>
<p>Their code is very “hard-coded everywhere”….</p>
<p>Encode Data (encoder)</p>
</dd>
</dl>
</dd>
</dl>
<p>library - api for time-series</p>
</div></blockquote>
</div></blockquote>
<p>#### from SentenceEmb Code,
class Pooling(nn.Module):</p>
<blockquote>
<div><p>Performs pooling (max or mean) on the token embeddings.
Using pooling, it generates from a variable sized sentence a fixed sized sentence embedding. This layer also allows to use the CLS token if it is returned by the underlying word embedding model.
You can concatenate multiple poolings together.
word_embedding_dimension: Dimensions for the word embeddings
pooling_mode: Can be a string: mean/max/cls. If set, overwrites the other pooling_mode_* settings</p>
<p>pooling_mode_cls_token: Use the first token (CLS token) as text representations</p>
<p>pooling_mode_max_tokens: Use max in each dimension over all tokens.</p>
<p>pooling_mode_mean_tokens: Perform mean-pooling
pooling_mode_mean_sqrt_len_tokens: Perform mean-pooling, but devide by sqrt(input_length).</p>
</div></blockquote>
<p># Defining our sentence transformer model
word_embedding_model = models.Transformer(model_name, max_seq_length=max_seq_length)
pooling_model = models.Pooling(word_embedding_model.get_word_embedding_dimension(), ‘cls’)
model = SentenceTransformer(modules=[word_embedding_model, pooling_model</p>
<blockquote>
<div><dl>
<dt>###</dt><dd><p>self_attn.in_proj_weight     torch.Size([96, 32])
self_attn.in_proj_bias       torch.Size([96])
self_attn.out_proj.weight    torch.Size([32, 32])
self_attn.out_proj.bias      torch.Size([32])</p>
<p>##### Position wise feedforward. &lt;&gt; MLP
### mini-compression  32 –&gt; 128 –&gt; 32   (mutiplication is done….)bs, sq 32 -&gt; bs sq 128 -&gt; bs sl 32</p>
<blockquote>
<div><p>Input / outpit</p>
</div></blockquote>
<p>linear1.weight               torch.Size([128, 32])  +  linear1.bias                torch.Size([128])
linear2.weight               torch.Size([32, 128])</p>
<p>linear2.bias                 torch.Size([32])
norm1.weight                 torch.Size([32])
norm1.bias                   torch.Size([32])
norm2.weight                 torch.Size([32])
norm2.bias                   torch.Size([32])</p>
</dd>
</dl>
</div></blockquote>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="utilmy.tseries.torch_outlier.modelEncoder3.training">
<span class="sig-name descname"><span class="pre">training</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">bool</span></em><a class="headerlink" href="#utilmy.tseries.torch_outlier.modelEncoder3.training" title="Permalink to this definition"></a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="utilmy.tseries.torch_outlier.modelRecurrentAutoencoder">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">utilmy.tseries.torch_outlier.</span></span><span class="sig-name descname"><span class="pre">modelRecurrentAutoencoder</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">seq_len</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">n_features</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">embedding_dim</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">64</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">device</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'cpu'</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#utilmy.tseries.torch_outlier.modelRecurrentAutoencoder" title="Permalink to this definition"></a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code></p>
<dl class="simple">
<dt>### LSTM Autoencoder</dt><dd><p>[Autoencoder’s](<a class="reference external" href="https://en.wikipedia.org/wiki/Autoencoder">https://en.wikipedia.org/wiki/Autoencoder</a>) job is to get some input data, pass it through  model, and obtain a reconstruction of  input.  reconstruction should match  input as much as possible.  trick is to use a small number of parameters, so your model learns a compressed representation of  data.</p>
</dd>
</dl>
<p>In a sense, Autoencoders try to learn only  most important features (compressed version) of  data. Here, have a look at how to feed Time Series data to an Autoencoder. use a couple of LSTM layers (hence  LSTM Autoencoder) to capture  temporal dependencies of  data.
To classify a sequence as normal or an anomaly, pick a cc.THRESHOLD above which a heartbeat is considered abnormal.</p>
<p>### Reconstruction Loss
When training an Autoencoder,  objective is to reconstruct  input as best as possible.
This is done by minimizing a loss function (just like in supervised learning).
This function is known as <em>reconstruction loss</em>. Cross-entropy loss and Mean squared error are common examples.</p>
<p>![Autoencoder](<a class="reference external" href="https://lilianweng.github.io/lil-log/assets/images/autoencoder-architecture.png">https://lilianweng.github.io/lil-log/assets/images/autoencoder-architecture.png</a>)
<em>Sample Autoencoder Architecture [Image Source](https://lilianweng.github.io/lil-log/2018/08/12/from-autoencoder-to-beta-vae.html)</em></p>
<blockquote>
<div><dl class="simple">
<dt>general Autoencoder architecture consists of two components.</dt><dd><p>An <em>Encoder</em> that compresses  input and a <em>Decoder</em> that tries to reconstruct it.</p>
</dd>
</dl>
</div></blockquote>
<p>use  LSTM Autoencoder from this [GitHub repo](<a class="reference external" href="https://github.com/shobrook/sequitur">https://github.com/shobrook/sequitur</a>)
with some small tweaks. Our model’s job is to reconstruct Time Series data.</p>
<dl class="py method">
<dt class="sig sig-object py" id="utilmy.tseries.torch_outlier.modelRecurrentAutoencoder.forward">
<span class="sig-name descname"><span class="pre">forward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">x</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#utilmy.tseries.torch_outlier.modelRecurrentAutoencoder.forward" title="Permalink to this definition"></a></dt>
<dd><p>Defines the computation performed at every call.</p>
<p>Should be overridden by all subclasses.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Although the recipe for forward pass needs to be defined within
this function, one should call the <code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code> instance afterwards
instead of this since the former takes care of running the
registered hooks while the latter silently ignores them.</p>
</div>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="utilmy.tseries.torch_outlier.modelRecurrentAutoencoder.training">
<span class="sig-name descname"><span class="pre">training</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">bool</span></em><a class="headerlink" href="#utilmy.tseries.torch_outlier.modelRecurrentAutoencoder.training" title="Permalink to this definition"></a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="utilmy.tseries.torch_outlier.modelRecurrentAutoencoder3">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">utilmy.tseries.torch_outlier.</span></span><span class="sig-name descname"><span class="pre">modelRecurrentAutoencoder3</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">seq_len</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">n_features</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">embedding_dim</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">64</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">device</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'cpu'</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#utilmy.tseries.torch_outlier.modelRecurrentAutoencoder3" title="Permalink to this definition"></a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code></p>
<dl class="simple">
<dt>### LSTM Autoencoder</dt><dd><p>[Autoencoder’s](<a class="reference external" href="https://en.wikipedia.org/wiki/Autoencoder">https://en.wikipedia.org/wiki/Autoencoder</a>) job is to get some input data, pass it through  model, and obtain a reconstruction of  input.  reconstruction should match  input as much as possible.  trick is to use a small number of parameters, so your model learns a compressed representation of  data.</p>
</dd>
</dl>
<p>In a sense, Autoencoders try to learn only  most important features (compressed version) of  data. Here, have a look at how to feed Time Series data to an Autoencoder. use a couple of LSTM layers (hence  LSTM Autoencoder) to capture  temporal dependencies of  data.
To classify a sequence as normal or an anomaly, pick a cc.THRESHOLD above which a heartbeat is considered abnormal.</p>
<p>### Reconstruction Loss
When training an Autoencoder,  objective is to reconstruct  input as best as possible.
This is done by minimizing a loss function (just like in supervised learning).
This function is known as <em>reconstruction loss</em>. Cross-entropy loss and Mean squared error are common examples.</p>
<p>![Autoencoder](<a class="reference external" href="https://lilianweng.github.io/lil-log/assets/images/autoencoder-architecture.png">https://lilianweng.github.io/lil-log/assets/images/autoencoder-architecture.png</a>)
<em>Sample Autoencoder Architecture [Image Source](https://lilianweng.github.io/lil-log/2018/08/12/from-autoencoder-to-beta-vae.html)</em></p>
<blockquote>
<div><dl class="simple">
<dt>general Autoencoder architecture consists of two components.</dt><dd><p>An <em>Encoder</em> that compresses  input and a <em>Decoder</em> that tries to reconstruct it.</p>
</dd>
</dl>
</div></blockquote>
<p>use  LSTM Autoencoder from this [GitHub repo](<a class="reference external" href="https://github.com/shobrook/sequitur">https://github.com/shobrook/sequitur</a>)
with some small tweaks. Our model’s job is to reconstruct Time Series data.</p>
<dl class="py method">
<dt class="sig sig-object py" id="utilmy.tseries.torch_outlier.modelRecurrentAutoencoder3.forward">
<span class="sig-name descname"><span class="pre">forward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">x</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#utilmy.tseries.torch_outlier.modelRecurrentAutoencoder3.forward" title="Permalink to this definition"></a></dt>
<dd><p>Defines the computation performed at every call.</p>
<p>Should be overridden by all subclasses.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Although the recipe for forward pass needs to be defined within
this function, one should call the <code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code> instance afterwards
instead of this since the former takes care of running the
registered hooks while the latter silently ignores them.</p>
</div>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="utilmy.tseries.torch_outlier.modelRecurrentAutoencoder3.training">
<span class="sig-name descname"><span class="pre">training</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">bool</span></em><a class="headerlink" href="#utilmy.tseries.torch_outlier.modelRecurrentAutoencoder3.training" title="Permalink to this definition"></a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="utilmy.tseries.torch_outlier.model_evaluate">
<span class="sig-prename descclassname"><span class="pre">utilmy.tseries.torch_outlier.</span></span><span class="sig-name descname"><span class="pre">model_evaluate</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">model</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">test_normal_dataset</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">device</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'cpu'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">THRESHOLD</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.2</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#utilmy.tseries.torch_outlier.model_evaluate" title="Permalink to this definition"></a></dt>
<dd></dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="utilmy.tseries.torch_outlier.model_plotLoss">
<span class="sig-prename descclassname"><span class="pre">utilmy.tseries.torch_outlier.</span></span><span class="sig-name descname"><span class="pre">model_plotLoss</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">history</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">dict</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#utilmy.tseries.torch_outlier.model_plotLoss" title="Permalink to this definition"></a></dt>
<dd></dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="utilmy.tseries.torch_outlier.model_predict">
<span class="sig-prename descclassname"><span class="pre">utilmy.tseries.torch_outlier.</span></span><span class="sig-name descname"><span class="pre">model_predict</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">model</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dataset</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">device</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'cpu'</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#utilmy.tseries.torch_outlier.model_predict" title="Permalink to this definition"></a></dt>
<dd></dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="utilmy.tseries.torch_outlier.model_save">
<span class="sig-prename descclassname"><span class="pre">utilmy.tseries.torch_outlier.</span></span><span class="sig-name descname"><span class="pre">model_save</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">model</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">path</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#utilmy.tseries.torch_outlier.model_save" title="Permalink to this definition"></a></dt>
<dd></dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="utilmy.tseries.torch_outlier.model_train">
<span class="sig-prename descclassname"><span class="pre">utilmy.tseries.torch_outlier.</span></span><span class="sig-name descname"><span class="pre">model_train</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">model</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">train_dataset</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">val_dataset</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">n_epochs</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">device</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'cpu'</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#utilmy.tseries.torch_outlier.model_train" title="Permalink to this definition"></a></dt>
<dd><p>using a batch size of 1 (our model sees only 1 sequence at a time).
minimizing  [L1Loss](<a class="reference external" href="https://pytorch.org/docs/stable/nn.html#l1loss">https://pytorch.org/docs/stable/nn.html#l1loss</a>),
which measures  MAE (mean absolute error). Why?  reconstructions seem to be better than with MSE (mean squared error).</p>
<p>n_epochs = cc.epochs</p>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="utilmy.tseries.torch_outlier.plot_prediction">
<span class="sig-prename descclassname"><span class="pre">utilmy.tseries.torch_outlier.</span></span><span class="sig-name descname"><span class="pre">plot_prediction</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">data</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">model</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">title</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">ax</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#utilmy.tseries.torch_outlier.plot_prediction" title="Permalink to this definition"></a></dt>
<dd></dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="utilmy.tseries.torch_outlier.plot_time_series_class">
<span class="sig-prename descclassname"><span class="pre">utilmy.tseries.torch_outlier.</span></span><span class="sig-name descname"><span class="pre">plot_time_series_class</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">data</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">class_name</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">ax</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">n_steps</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">10</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#utilmy.tseries.torch_outlier.plot_time_series_class" title="Permalink to this definition"></a></dt>
<dd></dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="utilmy.tseries.torch_outlier.test1">
<span class="sig-prename descclassname"><span class="pre">utilmy.tseries.torch_outlier.</span></span><span class="sig-name descname"><span class="pre">test1</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#utilmy.tseries.torch_outlier.test1" title="Permalink to this definition"></a></dt>
<dd><p>lSTM Auto-encoder
.. rubric:: Example</p>
<p>In this tutorial, you learned how to create an LSTM Autoencoder with PyTorch and use it to detect heartbeat anomalies in ECG data.</p>
<ul class="simple">
<li><p>[Read  tutorial](<a class="reference external" href="https://www.curiousily.com/posts/time-series-anomaly-detection-using-lstm-autoencoder-with-pytorch-in-python/">https://www.curiousily.com/posts/time-series-anomaly-detection-using-lstm-autoencoder-with-pytorch-in-python/</a>)</p></li>
<li><p>[Run  notebook in your browser (Google Colab)](<a class="reference external" href="https://colab.research.google.com/drive/1_J2MrBSvsJfOcVmYAN2-WSp36BtsFZCa">https://colab.research.google.com/drive/1_J2MrBSvsJfOcVmYAN2-WSp36BtsFZCa</a>)</p></li>
<li><p>[Read  Getting Things Done with Pytorch book](<a class="reference external" href="https://github.com/curiousily/Getting-Things-Done-with-Pytorch">https://github.com/curiousily/Getting-Things-Done-with-Pytorch</a>)</p></li>
</ul>
<p>You learned how to:</p>
<ul class="simple">
<li><p>Prepare a dataset for Anomaly Detection from Time Series Data</p></li>
<li><p>Build an LSTM Autoencoder with PyTorch</p></li>
<li><p>Train and evaluate your model</p></li>
<li><p>Choose a cc.THRESHOLD for anomaly detection</p></li>
<li><p>Classify unseen examples as normal or anomaly</p></li>
</ul>
<p>While our Time Series data is univariate (have only 1 feature),  code should work for multivariate datasets (multiple features) with little or no modification. Feel free to try it!</p>
<p>## References
- [Sequitur - Recurrent Autoencoder (RAE)](<a class="reference external" href="https://github.com/shobrook/sequitur">https://github.com/shobrook/sequitur</a>)
- [Towards Never-Ending Learning from Time Series Streams](<a class="reference external" href="https://www.cs.ucr.edu/~eamonn/neverending.pdf">https://www.cs.ucr.edu/~eamonn/neverending.pdf</a>)
- [LSTM Autoencoder for Anomaly Detection](<a class="reference external" href="https://towardsdatascience.com/lstm-autoencoder-for-anomaly-detection-e1f4f2ee7ccf">https://towardsdatascience.com/lstm-autoencoder-for-anomaly-detection-e1f4f2ee7ccf</a>)</p>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="utilmy.tseries.torch_outlier.test_all">
<span class="sig-prename descclassname"><span class="pre">utilmy.tseries.torch_outlier.</span></span><span class="sig-name descname"><span class="pre">test_all</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">None</span></span></span><a class="headerlink" href="#utilmy.tseries.torch_outlier.test_all" title="Permalink to this definition"></a></dt>
<dd><p>function test_all   to be used in test.py</p>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="utilmy.tseries.torch_outlier.test_trans">
<span class="sig-prename descclassname"><span class="pre">utilmy.tseries.torch_outlier.</span></span><span class="sig-name descname"><span class="pre">test_trans</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#utilmy.tseries.torch_outlier.test_trans" title="Permalink to this definition"></a></dt>
<dd><p>are you here ?</p>
</dd></dl>

</section>
<section id="module-utilmy.tseries.torch_outlier_comment">
<span id="utilmy-tseries-torch-outlier-comment-module"></span><h2>utilmy.tseries.torch_outlier_comment module<a class="headerlink" href="#module-utilmy.tseries.torch_outlier_comment" title="Permalink to this heading"></a></h2>
<dl class="py class">
<dt class="sig sig-object py" id="utilmy.tseries.torch_outlier_comment.DynamicLSTM">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">utilmy.tseries.torch_outlier_comment.</span></span><span class="sig-name descname"><span class="pre">DynamicLSTM</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">input_size</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">hidden_size</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">100</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">num_layers</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dropout</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">bidirectional</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#utilmy.tseries.torch_outlier_comment.DynamicLSTM" title="Permalink to this definition"></a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code></p>
<p>Dynamic LSTM module, which can handle variable length input sequence.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>input_size</strong> (<em>input size</em>) – </p></li>
<li><p><strong>hidden_size</strong> (<em>hidden size</em>) – </p></li>
<li><p><strong>num_layers</strong> (<em>number of hidden layers. Default: 1</em>) – </p></li>
<li><p><strong>dropout</strong> (<em>dropout rate. Default: 0.5</em>) – </p></li>
<li><p><strong>bidirectional</strong> (<em>If True</em><em>, </em><em>becomes a bidirectional RNN. Default: False.</em>) – </p></li>
<li><p><strong>Inputs</strong> – </p></li>
<li><p><strong>------</strong> – </p></li>
<li><p><strong>input</strong> (<em>tensor</em><em>, </em><em>shaped</em><em> [</em><em>batch</em><em>, </em><em>max_step</em><em>, </em><em>input_size</em><em>]</em>) – </p></li>
<li><p><strong>seq_lens</strong> (<em>tensor</em><em>, </em><em>shaped</em><em> [</em><em>batch</em><em>]</em><em>, </em><em>sequence lengths of batch</em>) – </p></li>
<li><p><strong>Outputs</strong> – </p></li>
<li><p><strong>-------</strong> – </p></li>
<li><p><strong>output</strong> (<em>tensor</em><em>, </em><em>shaped</em><em> [</em><em>batch</em><em>, </em><em>max_step</em><em>, </em><em>num_directions * hidden_size</em><em>]</em><em>,</em>) – tensor containing the output features (h_t) from the last layer
of the LSTM, for each t.</p></li>
</ul>
</dd>
</dl>
<dl class="py method">
<dt class="sig sig-object py" id="utilmy.tseries.torch_outlier_comment.DynamicLSTM.forward">
<span class="sig-name descname"><span class="pre">forward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">x</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">seq_lens</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#utilmy.tseries.torch_outlier_comment.DynamicLSTM.forward" title="Permalink to this definition"></a></dt>
<dd><p>Defines the computation performed at every call.</p>
<p>Should be overridden by all subclasses.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Although the recipe for forward pass needs to be defined within
this function, one should call the <code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code> instance afterwards
instead of this since the former takes care of running the
registered hooks while the latter silently ignores them.</p>
</div>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="utilmy.tseries.torch_outlier_comment.DynamicLSTM.training">
<span class="sig-name descname"><span class="pre">training</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">bool</span></em><a class="headerlink" href="#utilmy.tseries.torch_outlier_comment.DynamicLSTM.training" title="Permalink to this definition"></a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="py data">
<dt class="sig sig-object py" id="utilmy.tseries.torch_outlier_comment.MNAME">
<span class="sig-prename descclassname"><span class="pre">utilmy.tseries.torch_outlier_comment.</span></span><span class="sig-name descname"><span class="pre">MNAME</span></span><em class="property"><span class="w"> </span><span class="p"><span class="pre">=</span></span><span class="w"> </span><span class="pre">''</span></em><a class="headerlink" href="#utilmy.tseries.torch_outlier_comment.MNAME" title="Permalink to this definition"></a></dt>
<dd><p>#
Doc:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">Time</span> <span class="n">Series</span> <span class="n">Anomaly</span> <span class="n">Detection</span> <span class="n">using</span> <span class="n">LSTM</span> <span class="n">Autoencoders</span> <span class="k">with</span> <span class="n">PyTorch</span> <span class="ow">in</span> <span class="n">Python</span>
</pre></div>
</div>
<p>pip install -qq arff2pandas watermark</p>
<p># %reload_ext watermark
# %watermark -v -p numpy,pandas,torch,arff2pandas</p>
<blockquote>
<div><p><a class="reference external" href="https://prod.liveshare.vsengsaas.visualstudio.com/join?B79A51FE5D82309D32D1283340E14D9C411C">https://prod.liveshare.vsengsaas.visualstudio.com/join?B79A51FE5D82309D32D1283340E14D9C411C</a></p>
</div></blockquote>
<p>Join Zoom Meeting
<a class="reference external" href="https://us05web.zoom.us/j/2933746463?pwd=WUhRWkx0NWNZRVBFVjZ4enV6Y1R2QT09">https://us05web.zoom.us/j/2933746463?pwd=WUhRWkx0NWNZRVBFVjZ4enV6Y1R2QT09</a></p>
<p>Meeting ID: 293 374 6463
Passcode: J50Muh</p>
</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="utilmy.tseries.torch_outlier_comment.QuoraModel">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">utilmy.tseries.torch_outlier_comment.</span></span><span class="sig-name descname"><span class="pre">QuoraModel</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">args</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#utilmy.tseries.torch_outlier_comment.QuoraModel" title="Permalink to this definition"></a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code></p>
<p>Model for quora insincere question classification.</p>
<dl class="py method">
<dt class="sig sig-object py" id="utilmy.tseries.torch_outlier_comment.QuoraModel.forward">
<span class="sig-name descname"><span class="pre">forward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">word_seq</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">seq_len</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#utilmy.tseries.torch_outlier_comment.QuoraModel.forward" title="Permalink to this definition"></a></dt>
<dd><p>Defines the computation performed at every call.</p>
<p>Should be overridden by all subclasses.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Although the recipe for forward pass needs to be defined within
this function, one should call the <code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code> instance afterwards
instead of this since the former takes care of running the
registered hooks while the latter silently ignores them.</p>
</div>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="utilmy.tseries.torch_outlier_comment.QuoraModel.training">
<span class="sig-name descname"><span class="pre">training</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">bool</span></em><a class="headerlink" href="#utilmy.tseries.torch_outlier_comment.QuoraModel.training" title="Permalink to this definition"></a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="utilmy.tseries.torch_outlier_comment.dataset_ECG5000_fetch_pandas">
<span class="sig-prename descclassname"><span class="pre">utilmy.tseries.torch_outlier_comment.</span></span><span class="sig-name descname"><span class="pre">dataset_ECG5000_fetch_pandas</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">nrows</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">100</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dirout</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'./ztmp/'</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#utilmy.tseries.torch_outlier_comment.dataset_ECG5000_fetch_pandas" title="Permalink to this definition"></a></dt>
<dd><p>combine  training and test data into a single data frame.
This will give us more data to train our Autoencoder. also shuffle it:</p>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="utilmy.tseries.torch_outlier_comment.dataset_ECG5000_prep">
<span class="sig-prename descclassname"><span class="pre">utilmy.tseries.torch_outlier_comment.</span></span><span class="sig-name descname"><span class="pre">dataset_ECG5000_prep</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">df</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#utilmy.tseries.torch_outlier_comment.dataset_ECG5000_prep" title="Permalink to this definition"></a></dt>
<dd><p>have 5,000 examples. Each row represents a single heartbeat record. Let’s name  possible classes:</p>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="utilmy.tseries.torch_outlier_comment.dataset_create">
<span class="sig-prename descclassname"><span class="pre">utilmy.tseries.torch_outlier_comment.</span></span><span class="sig-name descname"><span class="pre">dataset_create</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">df</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#utilmy.tseries.torch_outlier_comment.dataset_create" title="Permalink to this definition"></a></dt>
<dd></dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="utilmy.tseries.torch_outlier_comment.help">
<span class="sig-prename descclassname"><span class="pre">utilmy.tseries.torch_outlier_comment.</span></span><span class="sig-name descname"><span class="pre">help</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#utilmy.tseries.torch_outlier_comment.help" title="Permalink to this definition"></a></dt>
<dd><p>function help</p>
</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="utilmy.tseries.torch_outlier_comment.modelDecoder">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">utilmy.tseries.torch_outlier_comment.</span></span><span class="sig-name descname"><span class="pre">modelDecoder</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">seq_len</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">input_dim</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">64</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">n_features</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#utilmy.tseries.torch_outlier_comment.modelDecoder" title="Permalink to this definition"></a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code></p>
<p>Our Decoder contains two LSTM layers and an output layer that gives  final reconstruction.
#
# Time to wrap everything into an easy to use module:</p>
<dl class="py method">
<dt class="sig sig-object py" id="utilmy.tseries.torch_outlier_comment.modelDecoder.forward">
<span class="sig-name descname"><span class="pre">forward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">x</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#utilmy.tseries.torch_outlier_comment.modelDecoder.forward" title="Permalink to this definition"></a></dt>
<dd><p>Defines the computation performed at every call.</p>
<p>Should be overridden by all subclasses.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Although the recipe for forward pass needs to be defined within
this function, one should call the <code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code> instance afterwards
instead of this since the former takes care of running the
registered hooks while the latter silently ignores them.</p>
</div>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="utilmy.tseries.torch_outlier_comment.modelDecoder.training">
<span class="sig-name descname"><span class="pre">training</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">bool</span></em><a class="headerlink" href="#utilmy.tseries.torch_outlier_comment.modelDecoder.training" title="Permalink to this definition"></a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="utilmy.tseries.torch_outlier_comment.modelDecoder3">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">utilmy.tseries.torch_outlier_comment.</span></span><span class="sig-name descname"><span class="pre">modelDecoder3</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">seq_len</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">input_dim</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">64</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">n_features</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#utilmy.tseries.torch_outlier_comment.modelDecoder3" title="Permalink to this definition"></a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code></p>
<p>Our Decoder contains two LSTM layers and an output layer that gives  final reconstruction.
#
# Time to wrap everything into an easy to use module:</p>
<dl class="py method">
<dt class="sig sig-object py" id="utilmy.tseries.torch_outlier_comment.modelDecoder3.forward">
<span class="sig-name descname"><span class="pre">forward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">x</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#utilmy.tseries.torch_outlier_comment.modelDecoder3.forward" title="Permalink to this definition"></a></dt>
<dd><p>Defines the computation performed at every call.</p>
<p>Should be overridden by all subclasses.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Although the recipe for forward pass needs to be defined within
this function, one should call the <code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code> instance afterwards
instead of this since the former takes care of running the
registered hooks while the latter silently ignores them.</p>
</div>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="utilmy.tseries.torch_outlier_comment.modelDecoder3.training">
<span class="sig-name descname"><span class="pre">training</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">bool</span></em><a class="headerlink" href="#utilmy.tseries.torch_outlier_comment.modelDecoder3.training" title="Permalink to this definition"></a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="utilmy.tseries.torch_outlier_comment.modelEncoder">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">utilmy.tseries.torch_outlier_comment.</span></span><span class="sig-name descname"><span class="pre">modelEncoder</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">seq_len</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">n_features</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">embedding_dim</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">64</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#utilmy.tseries.torch_outlier_comment.modelEncoder" title="Permalink to this definition"></a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code></p>
<p><em>Encoder</em> uses two LSTM layers to compress  Time Series data input.</p>
<p>Next, decode  compressed representation using a <em>Decoder</em>:</p>
<dl class="py method">
<dt class="sig sig-object py" id="utilmy.tseries.torch_outlier_comment.modelEncoder.forward">
<span class="sig-name descname"><span class="pre">forward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">x</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#utilmy.tseries.torch_outlier_comment.modelEncoder.forward" title="Permalink to this definition"></a></dt>
<dd><p>Defines the computation performed at every call.</p>
<p>Should be overridden by all subclasses.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Although the recipe for forward pass needs to be defined within
this function, one should call the <code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code> instance afterwards
instead of this since the former takes care of running the
registered hooks while the latter silently ignores them.</p>
</div>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="utilmy.tseries.torch_outlier_comment.modelEncoder.training">
<span class="sig-name descname"><span class="pre">training</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">bool</span></em><a class="headerlink" href="#utilmy.tseries.torch_outlier_comment.modelEncoder.training" title="Permalink to this definition"></a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="utilmy.tseries.torch_outlier_comment.modelEncoder2">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">utilmy.tseries.torch_outlier_comment.</span></span><span class="sig-name descname"><span class="pre">modelEncoder2</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">seq_len</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">n_features</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">embedding_dim</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">64</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dropout</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.3</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#utilmy.tseries.torch_outlier_comment.modelEncoder2" title="Permalink to this definition"></a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code></p>
<dl>
<dt><em>Encoder</em> uses two LSTM layers to compress  Time Series data input.</dt><dd><blockquote>
<div><dl>
<dt>Args:</dt><dd><p>input_size : The number of expected features in the input <cite>x</cite>
hidden_size: The number of features in the hidden state <cite>h</cite>
num_layers : Number of recurrent layers. E.g., setting <code class="docutils literal notranslate"><span class="pre">num_layers=2</span></code></p>
<blockquote>
<div><p>would mean stacking two LSTMs together to form a <cite>stacked LSTM</cite>,
with the second LSTM taking in outputs of the first LSTM and
computing the final results. Default: 1</p>
</div></blockquote>
<p>bias:        If <code class="docutils literal notranslate"><span class="pre">False</span></code>, then the layer does not use bias weights <cite>b_ih</cite> and <cite>b_hh</cite>. Default: <code class="docutils literal notranslate"><span class="pre">True</span></code>
batch_first: If <code class="docutils literal notranslate"><span class="pre">True</span></code>, then the input and output tensors are provided as (batch, seq, feature). Default: <code class="docutils literal notranslate"><span class="pre">False</span></code>
dropout    : If non-zero, introduces a <cite>Dropout</cite> layer on the outputs of each</p>
<blockquote>
<div><p>LSTM layer except the last layer, with dropout probability equal to <code class="xref py py-attr docutils literal notranslate"><span class="pre">dropout</span></code>. Default: 0</p>
</div></blockquote>
<p>bidirectional: If <code class="docutils literal notranslate"><span class="pre">True</span></code>, becomes a bidirectional LSTM. Default: <code class="docutils literal notranslate"><span class="pre">False</span></code></p>
</dd>
<dt>Inpu Dimmension :</dt><dd><p>embedding_dim= 64
hidden_dim =  2 * 64</p>
</dd>
</dl>
</div></blockquote>
<p>encoder is 2 LSTM stacked as below:</p>
<dl>
<dt>Let’s continiue on the input like this :</dt><dd><blockquote>
<div><p>Xinput dim :    torch.Size([140, 1])
Ypred_dim  :    torch.Size([140, 1])</p>
</div></blockquote>
<dl class="simple">
<dt>self.rnn1 = nn.LSTM(</dt><dd><p>input_size=n_features,
hidden_size=self.hidden_dim,
num_layers=1,</p>
</dd>
</dl>
<p>)</p>
<dl class="simple">
<dt>self.rnn2 = nn.LSTM(</dt><dd><p>input_size=self.hidden_dim,
hidden_size=embedding_dim,
num_layers=1,</p>
</dd>
</dl>
<p>)</p>
</dd>
</dl>
<p>what does it mean the n_features ???</p>
<blockquote>
<div><p>seq_len : lenght of the sequence (time series ??)</p>
<dl class="simple">
<dt>Example<span class="classifier">multi-variate time seeries , stacked together ?</span></dt><dd><p>time sereis 1
time series 2
time series 3</p>
</dd>
</dl>
<p>=  n_features = 3  ????</p>
<p>ok,   What about the stakced LSTM parts ??/</p>
<blockquote>
<div><blockquote>
<div><p>x, (_, _)        = self.rnn1(x)
x, (hidden_n, _) = self.rnn2(x)</p>
</div></blockquote>
<dl>
<dt>Sure, why using</dt><dd><dl>
<dt>self.rnn1 = nn.LSTM(</dt><dd><blockquote>
<div><p>…</p>
</div></blockquote>
<p>num_layers=  2</p>
</dd>
</dl>
</dd>
</dl>
<p>because the hidden layer is not passed</p>
<dl class="simple">
<dt>How to find dimension of the hidden layer ?</dt><dd><dl class="simple">
<dt>ok, we reshape 1 embedding per channel (time series)  ??</dt><dd><p>return hidden_n.reshape((self.n_features, self.embedding_dim))</p>
</dd>
</dl>
</dd>
</dl>
<p>Other question, the forward pass only return the hidden layer…
Thhough it will return the x too ???</p>
</div></blockquote>
<p>I would do it with 2 layers in one module
that is for 2 layers
allows more representation power for the model</p>
</div></blockquote>
</dd>
<dt>correct</dt><dd><p>seq_len - is obvi ous
n_features is number of channels in input  (==nb of time series)</p>
</dd>
</dl>
<dl class="py method">
<dt class="sig sig-object py" id="utilmy.tseries.torch_outlier_comment.modelEncoder2.forward">
<span class="sig-name descname"><span class="pre">forward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">x</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#utilmy.tseries.torch_outlier_comment.modelEncoder2.forward" title="Permalink to this definition"></a></dt>
<dd><p>Defines the computation performed at every call.</p>
<p>Should be overridden by all subclasses.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Although the recipe for forward pass needs to be defined within
this function, one should call the <code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code> instance afterwards
instead of this since the former takes care of running the
registered hooks while the latter silently ignores them.</p>
</div>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="utilmy.tseries.torch_outlier_comment.modelEncoder2.training">
<span class="sig-name descname"><span class="pre">training</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">bool</span></em><a class="headerlink" href="#utilmy.tseries.torch_outlier_comment.modelEncoder2.training" title="Permalink to this definition"></a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="utilmy.tseries.torch_outlier_comment.modelEncoder3">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">utilmy.tseries.torch_outlier_comment.</span></span><span class="sig-name descname"><span class="pre">modelEncoder3</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">seq_len</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">n_features</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">embedding_dim</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">64</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#utilmy.tseries.torch_outlier_comment.modelEncoder3" title="Permalink to this definition"></a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code></p>
<p><em>Encoder</em> uses two LSTM layers to compress  Time Series data input.</p>
<p>Next, decode  compressed representation using a <em>Decoder</em>:</p>
<dl class="py attribute">
<dt class="sig sig-object py" id="utilmy.tseries.torch_outlier_comment.modelEncoder3.embedding_dim">
<span class="sig-name descname"><span class="pre">embedding_dim</span></span><a class="headerlink" href="#utilmy.tseries.torch_outlier_comment.modelEncoder3.embedding_dim" title="Permalink to this definition"></a></dt>
<dd><dl>
<dt>its more an EXERCICE to try to plug in the Transformer Encoder</dt><dd><blockquote>
<div><blockquote>
<div><p>and match the dimensions.
(  therey are many transformer things (ie sentence tranfsormer… ), next time
we can try another one more adapted. )</p>
<blockquote>
<div><p><a class="reference external" href="https://github.com/lucidrains/linear-attention-transformer">https://github.com/lucidrains/linear-attention-transformer</a></p>
<p>TransformerEncoderLayer(d_model,
nhead, dim_feedforward=2048,</p>
<blockquote>
<div><p>dropout=0.1,</p>
<blockquote>
<div><p>layer_norm_eps=1e-05,
batch_first=False,</p>
<blockquote>
<div><p>norm_first=False,
device=None, dtype=None)</p>
</div></blockquote>
</div></blockquote>
</div></blockquote>
</div></blockquote>
</div></blockquote>
<p>bs = 5  ### 5 odd number for easy debug…
seq_len =  10   ## seq lenth, nb of tokens.
d_in = 32   #####  In features size   //    d_in = 16 -&gt; Linear(16, 32) -&gt; d_model 32 tor march</p>
<p>#### LSTM       batch,      seq_len,      n_features
x = torch.randn(bs, seq_len, d_in)
d_h = 32   ### 32 of features…. for ONE token   ===  d_model ===  self.hidden_dim</p>
<p>### dim_feedforward=2048   ### Internal MLP
Linear(d_model, d_ff) Linear(d_ff, d_model)
enc = nn.TransformerEncoderLayer(d_h, 4, dim_feedforward=128,</p>
<blockquote>
<div><blockquote>
<div><blockquote>
<div><p>batch_first=True   #### Ordering the input (batch size,  ….  )</p>
</div></blockquote>
<p>)</p>
</div></blockquote>
<p>Xencode = enc(X)</p>
<p>### torch.Size([5, 10, 32])  ####   out = enc(x)  print(out.shape)
#### same dimension, in float ???
# ###  cannot use as “embedding” …</p>
<p>TimSeries= bs, seq_len n_features -&gt;ConveLayer–&gt; (bs new_seq_len d_model)</p>
<p>(bs new_seq_len d_model)   —&gt; TFLayer  —&gt;  (bs new_seq_len d_model)</p>
<blockquote>
<div><blockquote>
<div><p>TLayerEncoder –&gt;   (bs, seq_len_v2 d_model)
Pooling –&gt; (bs, 0, d_model)    ### Role of middle “compressor””</p>
</div></blockquote>
<p>(bs, seq_len_v3 d_model), (bs, 0, d_model)–&gt; TFLayerDecode –&gt;  (bs, seq_len_v3 d_model)</p>
</div></blockquote>
<p>### this is what they are using:  Pooling in the middle: here this one below:
<a class="reference external" href="https://github.com/UKPLab/sentence-transformers/tree/master/examples/unsupervised_learning/TSDAE">https://github.com/UKPLab/sentence-transformers/tree/master/examples/unsupervised_learning/TSDAE</a></p>
</div></blockquote>
</div></blockquote>
<p>idea: timse series into “text token”</p>
</dd>
</dl>
<p>##Text Version:      bs, seq_len            [int] -&gt; EmbeddingLayer                 –&gt;  bs sl d_model [float]
## Time version :    bs, seq_len, n_feat [float]  -&gt; 1D Conv (to create token-like) –&gt;  bs sl2 d_model [float]</p>
<blockquote>
<div><dl>
<dt>#### Implementation</dt><dd><p>(Conv, TFEnc-decode, DeConv)   –&gt; get back time series.</p>
<dl>
<dt>We only have interest in the pooling “embedding”</dt><dd><blockquote>
<div><p>timeseries –&gt;   Can we have Embedding Vector  –&gt; we compare vectors…</p>
<p>Embedding Vector ==  Output of the pooling  (which is here: 1st item , special token):</p>
<p>Dimension of Output of the Pooling (for example):  bs, 1, d_model</p>
<p>Pooling = Compression/Flattening of the list of TFEncode vectors  into  1D vector.</p>
<p>Suppose you have used the output of the pooling for debugging/ check/ actual neighbor search ?
by Cos. similarity,  “Easy Check if we have access to the pooling output”.</p>
<p>ConV/ Deconv        : TSeries –&gt; Token…
Poooling in the middle:   for</p>
<p>and Plug into TFLayer (Contentn Encoder, Content Decoder)
TFLayer parameters NAMES, they are normalizd : d_in, d_h</p>
<blockquote>
<div><p>d_model (both input and output) num_heads (d_model//num_heads == 0)
d_ff = 4* d_model
growth like exponential with seq_len - but there linear attention models</p>
</div></blockquote>
<p>++ cost of training ++ longer.  3X-10X longer than LSTM ???
In the end, LSTM is a good baselines (cost/perf/debugging )</p>
<blockquote>
<div><p><a class="reference external" href="https://medium.com/ai%C2%B3-theory-practice-business/awd-lstm-6b2744e809c5">https://medium.com/ai%C2%B3-theory-practice-business/awd-lstm-6b2744e809c5</a>
FastAI, default one.
standard one,
Dropout.</p>
</div></blockquote>
<p>Sure, Are you faimilait with Image model like EfficienttNet ?
Big model…. no worries,</p>
</div></blockquote>
<p>AWD-LSTM by Merity
SHA-RNN by Merity</p>
</dd>
</dl>
</dd>
</dl>
</div></blockquote>
<p>from linear_attention_transformer import LinearAttentionTransformerLM, LinformerContextSettings
settings = LinformerContextSettings(  seq_len = 2048,  k = 256)</p>
<dl>
<dt>dec = LinearAttentionTransformerLM(</dt><dd><p>num_tokens = 20000,
dim = 512,
heads = 8,
depth = 6,</p>
<p>max_seq_len = 4096,
causal = True,
context_linformer_settings = settings,
receives_context = True</p>
</dd>
</dl>
<p>).cuda()</p>
<blockquote>
<div><blockquote>
<div><dl>
<dt>Cannot use Pre-trained model (== hard to get results fast….).</dt><dd><p>Google Mutlti horizon transformer time series.
TSAI
<a class="reference external" href="https://github.com/timeseriesAI/tsai">https://github.com/timeseriesAI/tsai</a>    nice API</p>
<p>DeepAR  by Amaazon ,  RNN + Auto-rerefressive ,…. It
<a class="reference external" href="https://ts.gluon.ai/">https://ts.gluon.ai/</a>
<a class="reference external" href="https://ts.gluon.ai/api/gluonts/gluonts.model.deepar.html">https://ts.gluon.ai/api/gluonts/gluonts.model.deepar.html</a></p>
<blockquote>
<div><dl>
<dt>Business focus API</dt><dd><p>Yes and NO :  it takes time to train….
( simpler model == les time to train and perf woithin reasonable )</p>
<p>TiemSeries :  we need more 4-5  years before anything liek NLP onees./.
Deep Learning is NOT YET enough flexible to include business rules/ many Context things…</p>
<dl class="simple">
<dt>BUT, for Outlier its ok, because of embedding.</dt><dd><p>very easy after/standard.</p>
</dd>
</dl>
<p>70% of DL application to get the embedding in some ways.</p>
</dd>
</dl>
</div></blockquote>
</dd>
</dl>
</div></blockquote>
<p>ts-gluon  is good, generic, different models…. more simple ones too.
torch-forecast :</p>
<blockquote>
<div><blockquote>
<div><p><a class="reference external" href="https://pytorch-forecasting.readthedocs.io/en/stable/">https://pytorch-forecasting.readthedocs.io/en/stable/</a></p>
</div></blockquote>
<dl>
<dt>into the embedding part  —&gt; we can combine with Text, …. more easily.</dt><dd><p>we can store somewhere, forever….
Some input.</p>
<dl>
<dt>DeepCTRL by Google ?</dt><dd><p>To encore rules + data
<a class="reference external" href="https://ai.googleblog.com/2022/01/controlling-neural-networks-with-rule.html">https://ai.googleblog.com/2022/01/controlling-neural-networks-with-rule.html</a></p>
<p>### Some clean re-implementation here too:
utilmy/deeplearning/ttorch/rule_encoder4.py  (not perfect…)</p>
<dl>
<dt>Idea :</dt><dd><p>modelA to encode “MANUAL RULES”  (ex:   if weight &gt; 70% –&gt; Always ypred=1, HARD constraints )</p>
<p>modelB to encode  “data”  (ie auto-encoder, MLP, …)</p>
<p>modelMerge(modelA, modelB)  –&gt; Prediction.</p>
</dd>
</dl>
<p>Their code is very “hard-coded everywhere”….</p>
<p>Encode Data (encoder)</p>
</dd>
</dl>
</dd>
</dl>
<p>library - api for time-series</p>
</div></blockquote>
</div></blockquote>
<p>#### from SentenceEmb Code,
class Pooling(nn.Module):</p>
<blockquote>
<div><p>Performs pooling (max or mean) on the token embeddings.
Using pooling, it generates from a variable sized sentence a fixed sized sentence embedding. This layer also allows to use the CLS token if it is returned by the underlying word embedding model.
You can concatenate multiple poolings together.
word_embedding_dimension: Dimensions for the word embeddings
pooling_mode: Can be a string: mean/max/cls. If set, overwrites the other pooling_mode_* settings</p>
<p>pooling_mode_cls_token: Use the first token (CLS token) as text representations</p>
<p>pooling_mode_max_tokens: Use max in each dimension over all tokens.</p>
<p>pooling_mode_mean_tokens: Perform mean-pooling
pooling_mode_mean_sqrt_len_tokens: Perform mean-pooling, but devide by sqrt(input_length).</p>
</div></blockquote>
<p># Defining our sentence transformer model
word_embedding_model = models.Transformer(model_name, max_seq_length=max_seq_length)
pooling_model = models.Pooling(word_embedding_model.get_word_embedding_dimension(), ‘cls’)
model = SentenceTransformer(modules=[word_embedding_model, pooling_model</p>
<blockquote>
<div><dl>
<dt>###</dt><dd><p>self_attn.in_proj_weight     torch.Size([96, 32])
self_attn.in_proj_bias       torch.Size([96])
self_attn.out_proj.weight    torch.Size([32, 32])
self_attn.out_proj.bias      torch.Size([32])</p>
<p>##### Position wise feedforward. &lt;&gt; MLP
### mini-compression  32 –&gt; 128 –&gt; 32   (mutiplication is done….)bs, sq 32 -&gt; bs sq 128 -&gt; bs sl 32</p>
<blockquote>
<div><p>Input / outpit</p>
</div></blockquote>
<p>linear1.weight               torch.Size([128, 32])  +  linear1.bias                torch.Size([128])
linear2.weight               torch.Size([32, 128])</p>
<p>linear2.bias                 torch.Size([32])
norm1.weight                 torch.Size([32])
norm1.bias                   torch.Size([32])
norm2.weight                 torch.Size([32])
norm2.bias                   torch.Size([32])</p>
</dd>
</dl>
</div></blockquote>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="utilmy.tseries.torch_outlier_comment.modelEncoder3.forward">
<span class="sig-name descname"><span class="pre">forward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">x</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#utilmy.tseries.torch_outlier_comment.modelEncoder3.forward" title="Permalink to this definition"></a></dt>
<dd><p>Defines the computation performed at every call.</p>
<p>Should be overridden by all subclasses.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Although the recipe for forward pass needs to be defined within
this function, one should call the <code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code> instance afterwards
instead of this since the former takes care of running the
registered hooks while the latter silently ignores them.</p>
</div>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="utilmy.tseries.torch_outlier_comment.modelEncoder3.hidden_dim">
<span class="sig-name descname"><span class="pre">hidden_dim</span></span><a class="headerlink" href="#utilmy.tseries.torch_outlier_comment.modelEncoder3.hidden_dim" title="Permalink to this definition"></a></dt>
<dd><dl>
<dt>its more an EXERCICE to try to plug in the Transformer Encoder</dt><dd><blockquote>
<div><blockquote>
<div><p>and match the dimensions.
(  therey are many transformer things (ie sentence tranfsormer… ), next time
we can try another one more adapted. )</p>
<blockquote>
<div><p><a class="reference external" href="https://github.com/lucidrains/linear-attention-transformer">https://github.com/lucidrains/linear-attention-transformer</a></p>
<p>TransformerEncoderLayer(d_model,
nhead, dim_feedforward=2048,</p>
<blockquote>
<div><p>dropout=0.1,</p>
<blockquote>
<div><p>layer_norm_eps=1e-05,
batch_first=False,</p>
<blockquote>
<div><p>norm_first=False,
device=None, dtype=None)</p>
</div></blockquote>
</div></blockquote>
</div></blockquote>
</div></blockquote>
</div></blockquote>
<p>bs = 5  ### 5 odd number for easy debug…
seq_len =  10   ## seq lenth, nb of tokens.
d_in = 32   #####  In features size   //    d_in = 16 -&gt; Linear(16, 32) -&gt; d_model 32 tor march</p>
<p>#### LSTM       batch,      seq_len,      n_features
x = torch.randn(bs, seq_len, d_in)
d_h = 32   ### 32 of features…. for ONE token   ===  d_model ===  self.hidden_dim</p>
<p>### dim_feedforward=2048   ### Internal MLP
Linear(d_model, d_ff) Linear(d_ff, d_model)
enc = nn.TransformerEncoderLayer(d_h, 4, dim_feedforward=128,</p>
<blockquote>
<div><blockquote>
<div><blockquote>
<div><p>batch_first=True   #### Ordering the input (batch size,  ….  )</p>
</div></blockquote>
<p>)</p>
</div></blockquote>
<p>Xencode = enc(X)</p>
<p>### torch.Size([5, 10, 32])  ####   out = enc(x)  print(out.shape)
#### same dimension, in float ???
# ###  cannot use as “embedding” …</p>
<p>TimSeries= bs, seq_len n_features -&gt;ConveLayer–&gt; (bs new_seq_len d_model)</p>
<p>(bs new_seq_len d_model)   —&gt; TFLayer  —&gt;  (bs new_seq_len d_model)</p>
<blockquote>
<div><blockquote>
<div><p>TLayerEncoder –&gt;   (bs, seq_len_v2 d_model)
Pooling –&gt; (bs, 0, d_model)    ### Role of middle “compressor””</p>
</div></blockquote>
<p>(bs, seq_len_v3 d_model), (bs, 0, d_model)–&gt; TFLayerDecode –&gt;  (bs, seq_len_v3 d_model)</p>
</div></blockquote>
<p>### this is what they are using:  Pooling in the middle: here this one below:
<a class="reference external" href="https://github.com/UKPLab/sentence-transformers/tree/master/examples/unsupervised_learning/TSDAE">https://github.com/UKPLab/sentence-transformers/tree/master/examples/unsupervised_learning/TSDAE</a></p>
</div></blockquote>
</div></blockquote>
<p>idea: timse series into “text token”</p>
</dd>
</dl>
<p>##Text Version:      bs, seq_len            [int] -&gt; EmbeddingLayer                 –&gt;  bs sl d_model [float]
## Time version :    bs, seq_len, n_feat [float]  -&gt; 1D Conv (to create token-like) –&gt;  bs sl2 d_model [float]</p>
<blockquote>
<div><dl>
<dt>#### Implementation</dt><dd><p>(Conv, TFEnc-decode, DeConv)   –&gt; get back time series.</p>
<dl>
<dt>We only have interest in the pooling “embedding”</dt><dd><blockquote>
<div><p>timeseries –&gt;   Can we have Embedding Vector  –&gt; we compare vectors…</p>
<p>Embedding Vector ==  Output of the pooling  (which is here: 1st item , special token):</p>
<p>Dimension of Output of the Pooling (for example):  bs, 1, d_model</p>
<p>Pooling = Compression/Flattening of the list of TFEncode vectors  into  1D vector.</p>
<p>Suppose you have used the output of the pooling for debugging/ check/ actual neighbor search ?
by Cos. similarity,  “Easy Check if we have access to the pooling output”.</p>
<p>ConV/ Deconv        : TSeries –&gt; Token…
Poooling in the middle:   for</p>
<p>and Plug into TFLayer (Contentn Encoder, Content Decoder)
TFLayer parameters NAMES, they are normalizd : d_in, d_h</p>
<blockquote>
<div><p>d_model (both input and output) num_heads (d_model//num_heads == 0)
d_ff = 4* d_model
growth like exponential with seq_len - but there linear attention models</p>
</div></blockquote>
<p>++ cost of training ++ longer.  3X-10X longer than LSTM ???
In the end, LSTM is a good baselines (cost/perf/debugging )</p>
<blockquote>
<div><p><a class="reference external" href="https://medium.com/ai%C2%B3-theory-practice-business/awd-lstm-6b2744e809c5">https://medium.com/ai%C2%B3-theory-practice-business/awd-lstm-6b2744e809c5</a>
FastAI, default one.
standard one,
Dropout.</p>
</div></blockquote>
<p>Sure, Are you faimilait with Image model like EfficienttNet ?
Big model…. no worries,</p>
</div></blockquote>
<p>AWD-LSTM by Merity
SHA-RNN by Merity</p>
</dd>
</dl>
</dd>
</dl>
</div></blockquote>
<p>from linear_attention_transformer import LinearAttentionTransformerLM, LinformerContextSettings
settings = LinformerContextSettings(  seq_len = 2048,  k = 256)</p>
<dl>
<dt>dec = LinearAttentionTransformerLM(</dt><dd><p>num_tokens = 20000,
dim = 512,
heads = 8,
depth = 6,</p>
<p>max_seq_len = 4096,
causal = True,
context_linformer_settings = settings,
receives_context = True</p>
</dd>
</dl>
<p>).cuda()</p>
<blockquote>
<div><blockquote>
<div><dl>
<dt>Cannot use Pre-trained model (== hard to get results fast….).</dt><dd><p>Google Mutlti horizon transformer time series.
TSAI
<a class="reference external" href="https://github.com/timeseriesAI/tsai">https://github.com/timeseriesAI/tsai</a>    nice API</p>
<p>DeepAR  by Amaazon ,  RNN + Auto-rerefressive ,…. It
<a class="reference external" href="https://ts.gluon.ai/">https://ts.gluon.ai/</a>
<a class="reference external" href="https://ts.gluon.ai/api/gluonts/gluonts.model.deepar.html">https://ts.gluon.ai/api/gluonts/gluonts.model.deepar.html</a></p>
<blockquote>
<div><dl>
<dt>Business focus API</dt><dd><p>Yes and NO :  it takes time to train….
( simpler model == les time to train and perf woithin reasonable )</p>
<p>TiemSeries :  we need more 4-5  years before anything liek NLP onees./.
Deep Learning is NOT YET enough flexible to include business rules/ many Context things…</p>
<dl class="simple">
<dt>BUT, for Outlier its ok, because of embedding.</dt><dd><p>very easy after/standard.</p>
</dd>
</dl>
<p>70% of DL application to get the embedding in some ways.</p>
</dd>
</dl>
</div></blockquote>
</dd>
</dl>
</div></blockquote>
<p>ts-gluon  is good, generic, different models…. more simple ones too.
torch-forecast :</p>
<blockquote>
<div><blockquote>
<div><p><a class="reference external" href="https://pytorch-forecasting.readthedocs.io/en/stable/">https://pytorch-forecasting.readthedocs.io/en/stable/</a></p>
</div></blockquote>
<dl>
<dt>into the embedding part  —&gt; we can combine with Text, …. more easily.</dt><dd><p>we can store somewhere, forever….
Some input.</p>
<dl>
<dt>DeepCTRL by Google ?</dt><dd><p>To encore rules + data
<a class="reference external" href="https://ai.googleblog.com/2022/01/controlling-neural-networks-with-rule.html">https://ai.googleblog.com/2022/01/controlling-neural-networks-with-rule.html</a></p>
<p>### Some clean re-implementation here too:
utilmy/deeplearning/ttorch/rule_encoder4.py  (not perfect…)</p>
<dl>
<dt>Idea :</dt><dd><p>modelA to encode “MANUAL RULES”  (ex:   if weight &gt; 70% –&gt; Always ypred=1, HARD constraints )</p>
<p>modelB to encode  “data”  (ie auto-encoder, MLP, …)</p>
<p>modelMerge(modelA, modelB)  –&gt; Prediction.</p>
</dd>
</dl>
<p>Their code is very “hard-coded everywhere”….</p>
<p>Encode Data (encoder)</p>
</dd>
</dl>
</dd>
</dl>
<p>library - api for time-series</p>
</div></blockquote>
</div></blockquote>
<p>#### from SentenceEmb Code,
class Pooling(nn.Module):</p>
<blockquote>
<div><p>Performs pooling (max or mean) on the token embeddings.
Using pooling, it generates from a variable sized sentence a fixed sized sentence embedding. This layer also allows to use the CLS token if it is returned by the underlying word embedding model.
You can concatenate multiple poolings together.
word_embedding_dimension: Dimensions for the word embeddings
pooling_mode: Can be a string: mean/max/cls. If set, overwrites the other pooling_mode_* settings</p>
<p>pooling_mode_cls_token: Use the first token (CLS token) as text representations</p>
<p>pooling_mode_max_tokens: Use max in each dimension over all tokens.</p>
<p>pooling_mode_mean_tokens: Perform mean-pooling
pooling_mode_mean_sqrt_len_tokens: Perform mean-pooling, but devide by sqrt(input_length).</p>
</div></blockquote>
<p># Defining our sentence transformer model
word_embedding_model = models.Transformer(model_name, max_seq_length=max_seq_length)
pooling_model = models.Pooling(word_embedding_model.get_word_embedding_dimension(), ‘cls’)
model = SentenceTransformer(modules=[word_embedding_model, pooling_model</p>
<blockquote>
<div><dl>
<dt>###</dt><dd><p>self_attn.in_proj_weight     torch.Size([96, 32])
self_attn.in_proj_bias       torch.Size([96])
self_attn.out_proj.weight    torch.Size([32, 32])
self_attn.out_proj.bias      torch.Size([32])</p>
<p>##### Position wise feedforward. &lt;&gt; MLP
### mini-compression  32 –&gt; 128 –&gt; 32   (mutiplication is done….)bs, sq 32 -&gt; bs sq 128 -&gt; bs sl 32</p>
<blockquote>
<div><p>Input / outpit</p>
</div></blockquote>
<p>linear1.weight               torch.Size([128, 32])  +  linear1.bias                torch.Size([128])
linear2.weight               torch.Size([32, 128])</p>
<p>linear2.bias                 torch.Size([32])
norm1.weight                 torch.Size([32])
norm1.bias                   torch.Size([32])
norm2.weight                 torch.Size([32])
norm2.bias                   torch.Size([32])</p>
</dd>
</dl>
</div></blockquote>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="utilmy.tseries.torch_outlier_comment.modelEncoder3.training">
<span class="sig-name descname"><span class="pre">training</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">bool</span></em><a class="headerlink" href="#utilmy.tseries.torch_outlier_comment.modelEncoder3.training" title="Permalink to this definition"></a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="utilmy.tseries.torch_outlier_comment.modelRecurrentAutoencoder">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">utilmy.tseries.torch_outlier_comment.</span></span><span class="sig-name descname"><span class="pre">modelRecurrentAutoencoder</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">seq_len</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">n_features</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">embedding_dim</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">64</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">device</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'cpu'</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#utilmy.tseries.torch_outlier_comment.modelRecurrentAutoencoder" title="Permalink to this definition"></a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code></p>
<dl class="simple">
<dt>### LSTM Autoencoder</dt><dd><p>[Autoencoder’s](<a class="reference external" href="https://en.wikipedia.org/wiki/Autoencoder">https://en.wikipedia.org/wiki/Autoencoder</a>) job is to get some input data, pass it through  model, and obtain a reconstruction of  input.  reconstruction should match  input as much as possible.  trick is to use a small number of parameters, so your model learns a compressed representation of  data.</p>
</dd>
</dl>
<p>In a sense, Autoencoders try to learn only  most important features (compressed version) of  data. Here, have a look at how to feed Time Series data to an Autoencoder. use a couple of LSTM layers (hence  LSTM Autoencoder) to capture  temporal dependencies of  data.
To classify a sequence as normal or an anomaly, pick a cc.THRESHOLD above which a heartbeat is considered abnormal.</p>
<p>### Reconstruction Loss
When training an Autoencoder,  objective is to reconstruct  input as best as possible.
This is done by minimizing a loss function (just like in supervised learning).
This function is known as <em>reconstruction loss</em>. Cross-entropy loss and Mean squared error are common examples.</p>
<p>![Autoencoder](<a class="reference external" href="https://lilianweng.github.io/lil-log/assets/images/autoencoder-architecture.png">https://lilianweng.github.io/lil-log/assets/images/autoencoder-architecture.png</a>)
<em>Sample Autoencoder Architecture [Image Source](https://lilianweng.github.io/lil-log/2018/08/12/from-autoencoder-to-beta-vae.html)</em></p>
<blockquote>
<div><dl class="simple">
<dt>general Autoencoder architecture consists of two components.</dt><dd><p>An <em>Encoder</em> that compresses  input and a <em>Decoder</em> that tries to reconstruct it.</p>
</dd>
</dl>
</div></blockquote>
<p>use  LSTM Autoencoder from this [GitHub repo](<a class="reference external" href="https://github.com/shobrook/sequitur">https://github.com/shobrook/sequitur</a>)
with some small tweaks. Our model’s job is to reconstruct Time Series data.</p>
<dl class="py method">
<dt class="sig sig-object py" id="utilmy.tseries.torch_outlier_comment.modelRecurrentAutoencoder.forward">
<span class="sig-name descname"><span class="pre">forward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">x</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#utilmy.tseries.torch_outlier_comment.modelRecurrentAutoencoder.forward" title="Permalink to this definition"></a></dt>
<dd><p>Defines the computation performed at every call.</p>
<p>Should be overridden by all subclasses.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Although the recipe for forward pass needs to be defined within
this function, one should call the <code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code> instance afterwards
instead of this since the former takes care of running the
registered hooks while the latter silently ignores them.</p>
</div>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="utilmy.tseries.torch_outlier_comment.modelRecurrentAutoencoder.training">
<span class="sig-name descname"><span class="pre">training</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">bool</span></em><a class="headerlink" href="#utilmy.tseries.torch_outlier_comment.modelRecurrentAutoencoder.training" title="Permalink to this definition"></a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="utilmy.tseries.torch_outlier_comment.modelRecurrentAutoencoder3">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">utilmy.tseries.torch_outlier_comment.</span></span><span class="sig-name descname"><span class="pre">modelRecurrentAutoencoder3</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">seq_len</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">n_features</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">embedding_dim</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">64</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">device</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'cpu'</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#utilmy.tseries.torch_outlier_comment.modelRecurrentAutoencoder3" title="Permalink to this definition"></a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code></p>
<dl class="simple">
<dt>### LSTM Autoencoder</dt><dd><p>[Autoencoder’s](<a class="reference external" href="https://en.wikipedia.org/wiki/Autoencoder">https://en.wikipedia.org/wiki/Autoencoder</a>) job is to get some input data, pass it through  model, and obtain a reconstruction of  input.  reconstruction should match  input as much as possible.  trick is to use a small number of parameters, so your model learns a compressed representation of  data.</p>
</dd>
</dl>
<p>In a sense, Autoencoders try to learn only  most important features (compressed version) of  data. Here, have a look at how to feed Time Series data to an Autoencoder. use a couple of LSTM layers (hence  LSTM Autoencoder) to capture  temporal dependencies of  data.
To classify a sequence as normal or an anomaly, pick a cc.THRESHOLD above which a heartbeat is considered abnormal.</p>
<p>### Reconstruction Loss
When training an Autoencoder,  objective is to reconstruct  input as best as possible.
This is done by minimizing a loss function (just like in supervised learning).
This function is known as <em>reconstruction loss</em>. Cross-entropy loss and Mean squared error are common examples.</p>
<p>![Autoencoder](<a class="reference external" href="https://lilianweng.github.io/lil-log/assets/images/autoencoder-architecture.png">https://lilianweng.github.io/lil-log/assets/images/autoencoder-architecture.png</a>)
<em>Sample Autoencoder Architecture [Image Source](https://lilianweng.github.io/lil-log/2018/08/12/from-autoencoder-to-beta-vae.html)</em></p>
<blockquote>
<div><dl class="simple">
<dt>general Autoencoder architecture consists of two components.</dt><dd><p>An <em>Encoder</em> that compresses  input and a <em>Decoder</em> that tries to reconstruct it.</p>
</dd>
</dl>
</div></blockquote>
<p>use  LSTM Autoencoder from this [GitHub repo](<a class="reference external" href="https://github.com/shobrook/sequitur">https://github.com/shobrook/sequitur</a>)
with some small tweaks. Our model’s job is to reconstruct Time Series data.</p>
<dl class="py method">
<dt class="sig sig-object py" id="utilmy.tseries.torch_outlier_comment.modelRecurrentAutoencoder3.forward">
<span class="sig-name descname"><span class="pre">forward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">x</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#utilmy.tseries.torch_outlier_comment.modelRecurrentAutoencoder3.forward" title="Permalink to this definition"></a></dt>
<dd><p>Defines the computation performed at every call.</p>
<p>Should be overridden by all subclasses.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Although the recipe for forward pass needs to be defined within
this function, one should call the <code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code> instance afterwards
instead of this since the former takes care of running the
registered hooks while the latter silently ignores them.</p>
</div>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="utilmy.tseries.torch_outlier_comment.modelRecurrentAutoencoder3.training">
<span class="sig-name descname"><span class="pre">training</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">bool</span></em><a class="headerlink" href="#utilmy.tseries.torch_outlier_comment.modelRecurrentAutoencoder3.training" title="Permalink to this definition"></a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="utilmy.tseries.torch_outlier_comment.model_evaluate">
<span class="sig-prename descclassname"><span class="pre">utilmy.tseries.torch_outlier_comment.</span></span><span class="sig-name descname"><span class="pre">model_evaluate</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">model</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">test_normal_dataset</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">device</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'cpu'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">THRESHOLD</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.2</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#utilmy.tseries.torch_outlier_comment.model_evaluate" title="Permalink to this definition"></a></dt>
<dd></dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="utilmy.tseries.torch_outlier_comment.model_plotLoss">
<span class="sig-prename descclassname"><span class="pre">utilmy.tseries.torch_outlier_comment.</span></span><span class="sig-name descname"><span class="pre">model_plotLoss</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">history</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">dict</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#utilmy.tseries.torch_outlier_comment.model_plotLoss" title="Permalink to this definition"></a></dt>
<dd></dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="utilmy.tseries.torch_outlier_comment.model_predict">
<span class="sig-prename descclassname"><span class="pre">utilmy.tseries.torch_outlier_comment.</span></span><span class="sig-name descname"><span class="pre">model_predict</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">model</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dataset</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">device</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'cpu'</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#utilmy.tseries.torch_outlier_comment.model_predict" title="Permalink to this definition"></a></dt>
<dd></dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="utilmy.tseries.torch_outlier_comment.model_save">
<span class="sig-prename descclassname"><span class="pre">utilmy.tseries.torch_outlier_comment.</span></span><span class="sig-name descname"><span class="pre">model_save</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">model</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">path</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#utilmy.tseries.torch_outlier_comment.model_save" title="Permalink to this definition"></a></dt>
<dd></dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="utilmy.tseries.torch_outlier_comment.model_train">
<span class="sig-prename descclassname"><span class="pre">utilmy.tseries.torch_outlier_comment.</span></span><span class="sig-name descname"><span class="pre">model_train</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">model</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">train_dataset</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">val_dataset</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">n_epochs</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">device</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'cpu'</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#utilmy.tseries.torch_outlier_comment.model_train" title="Permalink to this definition"></a></dt>
<dd><p>using a batch size of 1 (our model sees only 1 sequence at a time).
minimizing  [L1Loss](<a class="reference external" href="https://pytorch.org/docs/stable/nn.html#l1loss">https://pytorch.org/docs/stable/nn.html#l1loss</a>),
which measures  MAE (mean absolute error). Why?  reconstructions seem to be better than with MSE (mean squared error).</p>
<p>n_epochs = cc.epochs</p>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="utilmy.tseries.torch_outlier_comment.plot_prediction">
<span class="sig-prename descclassname"><span class="pre">utilmy.tseries.torch_outlier_comment.</span></span><span class="sig-name descname"><span class="pre">plot_prediction</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">data</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">model</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">title</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">ax</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#utilmy.tseries.torch_outlier_comment.plot_prediction" title="Permalink to this definition"></a></dt>
<dd></dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="utilmy.tseries.torch_outlier_comment.plot_time_series_class">
<span class="sig-prename descclassname"><span class="pre">utilmy.tseries.torch_outlier_comment.</span></span><span class="sig-name descname"><span class="pre">plot_time_series_class</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">data</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">class_name</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">ax</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">n_steps</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">10</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#utilmy.tseries.torch_outlier_comment.plot_time_series_class" title="Permalink to this definition"></a></dt>
<dd></dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="utilmy.tseries.torch_outlier_comment.test1">
<span class="sig-prename descclassname"><span class="pre">utilmy.tseries.torch_outlier_comment.</span></span><span class="sig-name descname"><span class="pre">test1</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#utilmy.tseries.torch_outlier_comment.test1" title="Permalink to this definition"></a></dt>
<dd><p>lSTM Auto-encoder
.. rubric:: Example</p>
<p>In this tutorial, you learned how to create an LSTM Autoencoder with PyTorch and use it to detect heartbeat anomalies in ECG data.</p>
<ul class="simple">
<li><p>[Read  tutorial](<a class="reference external" href="https://www.curiousily.com/posts/time-series-anomaly-detection-using-lstm-autoencoder-with-pytorch-in-python/">https://www.curiousily.com/posts/time-series-anomaly-detection-using-lstm-autoencoder-with-pytorch-in-python/</a>)</p></li>
<li><p>[Run  notebook in your browser (Google Colab)](<a class="reference external" href="https://colab.research.google.com/drive/1_J2MrBSvsJfOcVmYAN2-WSp36BtsFZCa">https://colab.research.google.com/drive/1_J2MrBSvsJfOcVmYAN2-WSp36BtsFZCa</a>)</p></li>
<li><p>[Read  Getting Things Done with Pytorch book](<a class="reference external" href="https://github.com/curiousily/Getting-Things-Done-with-Pytorch">https://github.com/curiousily/Getting-Things-Done-with-Pytorch</a>)</p></li>
</ul>
<p>You learned how to:</p>
<ul class="simple">
<li><p>Prepare a dataset for Anomaly Detection from Time Series Data</p></li>
<li><p>Build an LSTM Autoencoder with PyTorch</p></li>
<li><p>Train and evaluate your model</p></li>
<li><p>Choose a cc.THRESHOLD for anomaly detection</p></li>
<li><p>Classify unseen examples as normal or anomaly</p></li>
</ul>
<p>While our Time Series data is univariate (have only 1 feature),  code should work for multivariate datasets (multiple features) with little or no modification. Feel free to try it!</p>
<p>## References
- [Sequitur - Recurrent Autoencoder (RAE)](<a class="reference external" href="https://github.com/shobrook/sequitur">https://github.com/shobrook/sequitur</a>)
- [Towards Never-Ending Learning from Time Series Streams](<a class="reference external" href="https://www.cs.ucr.edu/~eamonn/neverending.pdf">https://www.cs.ucr.edu/~eamonn/neverending.pdf</a>)
- [LSTM Autoencoder for Anomaly Detection](<a class="reference external" href="https://towardsdatascience.com/lstm-autoencoder-for-anomaly-detection-e1f4f2ee7ccf">https://towardsdatascience.com/lstm-autoencoder-for-anomaly-detection-e1f4f2ee7ccf</a>)</p>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="utilmy.tseries.torch_outlier_comment.test_all">
<span class="sig-prename descclassname"><span class="pre">utilmy.tseries.torch_outlier_comment.</span></span><span class="sig-name descname"><span class="pre">test_all</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">None</span></span></span><a class="headerlink" href="#utilmy.tseries.torch_outlier_comment.test_all" title="Permalink to this definition"></a></dt>
<dd><p>function test_all   to be used in test.py</p>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="utilmy.tseries.torch_outlier_comment.test_trans">
<span class="sig-prename descclassname"><span class="pre">utilmy.tseries.torch_outlier_comment.</span></span><span class="sig-name descname"><span class="pre">test_trans</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#utilmy.tseries.torch_outlier_comment.test_trans" title="Permalink to this definition"></a></dt>
<dd><p>are you here ?</p>
</dd></dl>

</section>
<section id="module-utilmy.tseries.util_tseries">
<span id="utilmy-tseries-util-tseries-module"></span><h2>utilmy.tseries.util_tseries module<a class="headerlink" href="#module-utilmy.tseries.util_tseries" title="Permalink to this heading"></a></h2>
<p>#
Doc:</p>
<p>Sequential bootstraping
<a class="reference external" href="https://nbviewer.ipython.org/github/bashtage/arch/blob/main/examples/bootstrap_examples.ipynb">https://nbviewer.ipython.org/github/bashtage/arch/blob/main/examples/bootstrap_examples.ipynb</a></p>
<p><a class="reference external" href="https://quantdare.com/bootstrapping-time-series-data/">https://quantdare.com/bootstrapping-time-series-data/</a></p>
<p>Block Bootstraping</p>
<p><a class="reference external" href="https://lbelzile.github.io/timeseRies/boostrap-methods-for-time-series.html">https://lbelzile.github.io/timeseRies/boostrap-methods-for-time-series.html</a></p>
<p># Import data
import datetime as dt
import pandas as pd
import numpy as np
import pandas_datareader.data as web
start = dt.datetime(1951,1,1)
end = dt.datetime(2014,1,1)
sp500 = web.get_data_yahoo(‘^GSPC’, start=start, end=end)
start = sp500.index.min()
end = sp500.index.max()
monthly_dates = pd.date_range(start, end, freq=’M’)
monthly = sp500.reindex(monthly_dates, method=’ffill’)
returns = 100 * monthly[‘Adj Close’].pct_change().dropna()</p>
<p># Function to compute parameters
def sharpe_ratio(x):</p>
<blockquote>
<div><p>mu, sigma = 12 * x.mean(), np.sqrt(12 * x.var())
return np.array([mu, sigma, mu / sigma])</p>
</div></blockquote>
<p># Bootstrap confidence intervals
from arch.bootstrap import IIDBootstrap
bs = IIDBootstrap(returns)
ci = bs.conf_int(sharpe_ratio, 1000, method=’percentile’)</p>
<dl class="py function">
<dt class="sig sig-object py" id="utilmy.tseries.util_tseries.bootstrap_sequential">
<span class="sig-prename descclassname"><span class="pre">utilmy.tseries.util_tseries.</span></span><span class="sig-name descname"><span class="pre">bootstrap_sequential</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#utilmy.tseries.util_tseries.bootstrap_sequential" title="Permalink to this definition"></a></dt>
<dd><p>Sequential.
Doc:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span> <span class="c1"># Import data</span>
 <span class="c1"># Function to compute parameters</span>
 <span class="k">def</span> <span class="nf">sharpe_ratio</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
     <span class="n">mu</span><span class="p">,</span> <span class="n">sigma</span> <span class="o">=</span> <span class="mi">12</span> <span class="o">*</span> <span class="n">x</span><span class="o">.</span><span class="n">mean</span><span class="p">(),</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="mi">12</span> <span class="o">*</span> <span class="n">x</span><span class="o">.</span><span class="n">var</span><span class="p">())</span>
     <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="n">mu</span><span class="p">,</span> <span class="n">sigma</span><span class="p">,</span> <span class="n">mu</span> <span class="o">/</span> <span class="n">sigma</span><span class="p">])</span>

 <span class="c1"># Bootstrap confidence intervals</span>
 <span class="kn">from</span> <span class="nn">arch.bootstrap</span> <span class="kn">import</span> <span class="n">IIDBootstrap</span>
 <span class="n">bs</span> <span class="o">=</span> <span class="n">IIDBootstrap</span><span class="p">(</span><span class="n">returns</span><span class="p">)</span>
 <span class="n">ci</span> <span class="o">=</span> <span class="n">bs</span><span class="o">.</span><span class="n">conf_int</span><span class="p">(</span><span class="n">sharpe_ratio</span><span class="p">,</span> <span class="mi">1000</span><span class="p">,</span> <span class="n">method</span><span class="o">=</span><span class="s1">&#39;percentile&#39;</span><span class="p">)</span>

<span class="n">Bootstrap</span>
   <span class="n">Bootstraps</span>
   <span class="n">IID</span> <span class="n">Bootstrap</span>
   <span class="n">Stationary</span> <span class="n">Bootstrap</span>
   <span class="n">Circular</span> <span class="n">Block</span> <span class="n">Bootstrap</span>
   <span class="n">Moving</span> <span class="n">Block</span> <span class="n">Bootstrap</span>
   <span class="n">Methods</span>
   <span class="n">Confidence</span> <span class="n">interval</span> <span class="n">construction</span>
   <span class="n">Covariance</span> <span class="n">estimation</span>
   <span class="n">Apply</span> <span class="n">method</span> <span class="n">to</span> <span class="n">estimate</span> <span class="n">model</span> <span class="n">across</span> <span class="n">bootstraps</span>
   <span class="n">Generic</span> <span class="n">Bootstrap</span> <span class="n">iterator</span>
</pre></div>
</div>
</dd></dl>

</section>
<section id="module-utilmy.tseries">
<span id="module-contents"></span><h2>Module contents<a class="headerlink" href="#module-utilmy.tseries" title="Permalink to this heading"></a></h2>
</section>
</section>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="utilmy.tools.html" class="btn btn-neutral float-left" title="utilmy.tools package" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
        <a href="utilmy.viz.html" class="btn btn-neutral float-right" title="utilmy.viz package" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright .</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  

  <div class="rst-versions" data-toggle="rst-versions" role="note" aria-label="versions">
    <span class="rst-current-version" data-toggle="rst-current-version">
      <span class="fa fa-book"> Read the Docs</span>
      v: zdocs_y23487teg65f6
      <span class="fa fa-caret-down"></span>
    </span>
    <div class="rst-other-versions">
      
      <dl>
        <dt>Languages</dt>
        
           <strong> 
          <dd><a href="/myutil/en/zdocs_y23487teg65f6/">en</a></dd>
           </strong> 
        
          
          <dd><a href="/myutil/es/zdocs_y23487teg65f6/">es</a></dd>
          
        
      </dl>
      
      
      <dl>
        <dt>Versions</dt>
        
           <strong> 
          <dd><a href="/myutil/en/zdocs_y23487teg65f6/">zdocs_y23487teg65f6</a></dd>
           </strong> 
        
      </dl>
      
      
      <dl>
        <dt>Downloads</dt>
        
          <dd><a href="/myutil/en/zdocs_y23487teg65f6/utilmy-docs_en_zdocs_y23487teg65f6.pdf">pdf</a></dd>
        
          <dd><a href="/myutil/en/zdocs_y23487teg65f6/utilmy-docs_en_zdocs_y23487teg65f6.epub">epub</a></dd>
        
      </dl>
      
      
      <hr/>
      Free document hosting provided by <a href="http://www.readthedocs.org">Read the Docs</a>.
 
    </div>
  </div>

 <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>
    <!-- Theme Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-XXXXXXXXXX"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());

      gtag('config', 'G-XXXXXXXXXX', {
          'anonymize_ip': false,
      });
    </script> 

</body>
</html>